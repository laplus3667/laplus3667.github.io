[{"categories":["寫作"],"content":"工作溝通寫作流程四階段  紫式晦澀每日一篇文章第40天\n 前言   今天是2022年第38天, 全年第6週, 二月的第1個週一. 今天來思考寫作流程的四階段:準備, 研究, 草稿, 編輯.\n  今天的素材主要來自文章:\n   Communication at Work   過程一: 準備 (Preparing)  準備: 等同備料, 把文本切成好處理的格式.\nPrepare: to make something or somebody ready to be used or to do something; to make yourself ready to do something or for something that you expect to happen.\n 工序一: 認知寫作目的(Knowing purpose for writing) 溝通: 一般目的\u0026amp;特定目的:\n 所有的溝通，包括寫文件，都涉及關於反饋信息的一般目的和特定目的。  一般目的(General purpose):\n 一般目的: 溝通的結果目標(the end-goal of communication). 例如: 告知、說服、激勵、娛樂或將這些和其他效果結合起來。  特定目的(Specific purpose):\n 具體目的: 取決於手頭的情況( depends on the situation at hand)。 例如: 留下書面紀錄.  工序二: 分析受眾 (Analyzing Audience) 受眾大小:\n二級三級受眾:\n個人與對方位階的關係:\n受眾的知識背景:\n工序三: 選擇恰當溝通渠道-媒體形式 (Selecting Appropriate Channels) 渠道選擇:\n 1.個人交談與團體會議(In-Personal Conversation; Group Meeting)  信件(Email)    訊息(Instant/Text Message)    推特(Tweet)    IG(Instagram)    文章, 論文, 部落格(Article, Essay, Blog)    信件 (Letter)    備忘錄 (Memo)    報告/簡報 (Report/Presentation)    傳真(Fax)    電話, 語音, 會議電話 (Phone, Voicemail, Conference Calls)    視訊會議 (Video Chat \u0026amp; Web Conference)    過程二: 研究 (Researching)  研究: 為了找到新事實與資訊仔細研讀主題.\nResearch: a careful study of a subject, especially in order to discover new facts or information about it; to study something carefully and try to discover new facts about it\n 工序一: 選擇研究方法 (Choosing a Research Methodology)  正式非正式, 一手創造二手應用\n 非正式研究(Informal Research):\n 找資料, 用郵件回答問題, 無需正式引用. The research methodology where you look up information and deliver the goods in an email answering someone’s question without needing to formally cite your sources is informal research.  正式研究(Formal Research):\n 正式研究, 以系統性方法, 文檔化文獻, 使讀者可以自行驗證資訊的可信度. Formal research, on the other hand, takes a more systematic approach and documents the sources of information compiled using a conventional citation and reference system designed to make it easy for the audience to check out your sources themselves to verify their credibility.  一手研究(Primary research):\n 一手研究「產生新知識」,二手研究「應用新知識」. primary research generates new knowledge and secondary research applies it. Secondary research is what most people—especially students—do when they have academic or professional tasks because it involves finding and using primary research.  非正式二手研究 (Informal secondary research):\n The easiest, most common, and most expedient research, the kind that the vast majority of informative workplace communication involves, is informal secondary research.  工序二: 找尋可信資訊來源 (Locating Credible Sources) 衡量離線資源可信度:\n 作者可性度 (Author credentials): If the author is identified by name and credentials, you can verify whether they are expert enough on the topic to be a credible authority. 潮流(Currency): Depending on the topic, how recently the source was published can be a key indicator of credibility. 作者意圖(Author objectivity): If the author argues entirely on one side of a debate on which experts disagree, be suspicious of the source’s credibility. 同行審核 (Peer review): Any source that undergoes the peer-review process requires the author to make changes suggested by credentialed experts in the field called upon by the publisher. This process ensures that author errors are corrected before the text is published and hence improves both its quality and credibility. 寫作品質(Writing quality): The quality of the writing is another indicator of credibility because it also suggests that the source underwent an editorial process to ensure quality and respectability. 文獻(References): If a source identifies its sources and all of them meet the credibility standards outlined above, then you can be reasonably certain that the effort the source author made towards formal secondary research ensures their credibility.  衡量線上資源可信度:\n 網站設計品質: A final indicator of credibility for online sources, similar to the writing-quality check discussed above, is the overall design quality of the website.  工序三: 收集來源以帶目的閱讀 (Collecting Sources by Reading with a Purpose) ****: ****: ****: ****:\n工序四: 文本處理三味-引用(Quoting), 轉譯(Paraphrasing), 總結(Summarizing) ****: ****: ****: ****:\n工序五: 文檔化文本來源 (Documenting Sources) ****: ****: ****: ****:\n過程三: 草稿 (Drafting)  草稿: 某事物的大概版本, 還為達到最終形式.\nDraft: a rough written version of something that is not yet in its final form; to write the first rough version of something such as a letter, speech, book or law\n 工序一: 選擇組織模式 (Choosing an Organizational Pattern) ****: ****: ****: ****:\n工序二: 概述主要訊息 (Outline main Message) ****: ****: ****: ****:\n工序三: 形朔有效句子 (Forming Effective Sentences) ****: ****: ****: ****:\n工序四: 形朔有效段落 (Forming Effective Paragraphs) ****: ****: ****: ****:\n工序五: 標準商業模式 (Standard Business Style) ****: ****: ****: ****:\n工序六: 有效文件設計 (Effective Document Design) ****: ****: ****: ****:\n過程四: 編輯 (Editing)  編輯: 對文本修正錯誤, 改良, 使其能出版.\nEdit: an act of making changes to text or data; to prepare a piece of writing, a book, etc. to be published by correcting the mistakes, making improvements to it, etc.\n 工序一: 實質修訂(Substantial Revisions)\u0026ndash;評估(Evaluate), 重組(Reorganize), 增(Add), 剪(Trim) ****: ****: ****: ****:\n工序二: 語法校對 (Proofreading for Grammar) ****: ****: ****: ****:\n工序三: 標點符號校對 (Proofreading for Punctuation) ****: ****: ****: ****:\n工序四: 拼寫校對 (Proofreading for Spelling) ****: ****: ****: ****:\n工序五: 風格校對 (Proofreading for mechanics) ****: ****: ****: ****:\n後記 本文章梳理寫作的四階段:準備, 研究, 草稿, 編輯. 每個階段分別有3,5,6,5共19道工序.\n2022.02.07. 紫蕊 於 西拉法葉, 印第安納, 美國.\n    Version Date Summary     0.1 2022-02-07 把架構架起來, 之後慢慢補內容    ","date":"2022-02-07","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur040-%E5%B7%A5%E4%BD%9C%E6%BA%9D%E9%80%9A%E5%AF%AB%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%9B%9B%E9%9A%8E%E6%AE%B5/","series":["每日文章","學術工作流"],"tags":[],"title":"MUR040 工作溝通寫作流程四階段"},{"categories":["語言學"],"content":"思考句子的卡片筆記系統  紫式晦澀每日一篇文章第39天\n 前言   今天是2022年第37天, 全年第6週, 二月的第1個週日. 昨日對Obsidian筆記系統有新的理解, 今天來重新思考以「句子」為最小單位的卡片筆記系統.\n  今天的素材主要來自文章:\n   FORMING EFFECTIVE SENTENCES : 句子種類理論基礎 主、述二部來認識句子的組成 :主述部分析理論基礎 高校英語文法　1　語・句・節・文と品詞  高校英語文法　2　文の要素、主部・述部、目的語、補語、修飾語  高校英語文法　3　英語の文の形、平叙文、疑問文、命令文、感嘆文  高校英語文法　4　基本の5文型、第1文型、第2文型、第3文型、第4文型、第5文型　  主部結構:主語+修飾語 : 主部分析理論基礎 五種述部結構 : 述部分析理論基礎 Verb Classes  What Is a Phrase? Definition and Examples in Grammar Glossary of Grammatical and Rhetorical Terms    輸入處理輸出IPO思考\n 三法分析:語用, 語意, 語法, 詞法, 音韻 三法分析:\n   用法Use (Pragmatics 語用學): 學術文章; 雜誌報刊 ; 一般交談    心法Content (Semantic 語意學): 句子的種類, 意思, 修辭    技法Form (Syntax 語法學; Morpholohy 詞法學; Phonology 音韻學)       三法分析 語言層級 具體實例     用法     Pragmatics 語用學 文本到段落 學術文章; 雜誌報刊 ; 一般交談   心法     Semantic 語意學 段落到句子 句子的種類, 意思(字面意義), 修辭(非字面意義)   技法三味     Syntax 語法學 句子到主述部 主部分析(Subject); 述部分析(Predicate)   Morpholohy 詞法學 主述部到詞彙 句子成分分析(Part of Speech)   Phonology 音韻學 詞彙到單字 單字唸法, 與其他單字組合成片語時發音轉變    Input: 輸入來源 輸入一: 語言家教課使用的文本 輸入一:語言家教課使用的文本:\n 先學習家教課使用的文本有很多好處, 包含上課時可以更投入, 而老師可以提供即時feedback, 可以馬上答疑. 俄: 上課Siqi選的動詞的例句. 德: 上課Sofie準備的講義內容 韓: 上課Ling準備的投影片內容 法: 上課Lyn準備的上課內容 波: 上課Vera的課本或想討論的文本  輸入二: 網路文章 輸入二:網路文章:\n 俄: Russian podcast, Audible電子書 德: Audible電子書 韓: Talk to me in Korean, Audible電子書 法: Learn French with Podcast, Audible電子書 波: RealPolish, Audible電子書  輸入三: 帶逐字稿的Youtube影片 輸入三:帶逐字稿的Youtube影片:\n 俄: EasyRussian, Hack your Russian, Russian with Max 德: EasyGerman 韓: EasyKorean 法: French morning, A piece of French, EasyFrench 波: EasyPolish  輸入四: Glossika-口語單句 輸入四:Glossika-口語單句:\n 俄: A2, 10句一文本 德: A2, 10句一文本 韓: A1, 10句一文本 法: A2, 10句一文本 波: A1, 10句一文本  Process: 處理文本的四道濾鏡 濾鏡一: 句子種類-簡單(Simple), 複合(Compound), 複雜(Complex), 複合複雜(Compound-Complex) (Pragmatics 語用學)  共有四種句子種類(Sentence Variety): 簡單, 複合, 複雜, 複合複雜.\n 1. 簡單句(Simple):\n 一個獨立子句(One independent clause): 主部(Subject)+述部(Predicate) 表達一個完整意思   2. 複合句(Compound):\n 至少兩個獨立子句 利用(1) comma, 對等連接詞(Coordinating conjunction)(2)分號連結起兩個獨立子句 表達「兩個」「完整意思」之間的關係.   3. 複雜句(Complex):\n 從子句(Subordinate Clause)+主子句(Main clause) 利用「從屬連接詞(Subordinating conjunction)」連結從子句與主子句.   4. 複合複雜(Compound-Complex):\n 二獨立子句+一相依子句. 盡量少用, 因為是長句子.   濾鏡二: 主述部切割-主部(Subject), 述部(Predicate) (Semantic 語義學)  共有兩大成分: 主部(Subject), 述部(Predicate)\n 1. 主部(Subject):\n 與「動詞」無關 包含主詞（名詞、代名詞、片語）和修飾語（形容詞、或與形容詞相等的語詞）。【中文文法把主部稱做「主語」】 主部是句子的主角 主詞是主部的中心 物件屬性(Class attribute) 2. 述部(Predicate): 與「動詞」有關 包含述語動詞（動詞、助動詞），受詞（名詞、代名詞、片語），補語（名詞、代名詞、形容詞、片語、子句），修飾語（形容詞、副詞）等。【中文文法把述部稱做「謂語」】 述部則說明主部的性質或狀態。 述語動詞是述部的中心 物件方法(Class method).  濾鏡三之一: 主部分析 (Syntax 句法學)  主部: 主詞（名詞、代名詞、片語）+ 修飾語（形容詞、或與形容詞相等的語詞）\n 濾鏡三之二: 述部分析 (Syntax 句法學)  述部: 述語動詞（動詞、助動詞）+ 受詞（名詞、代名詞、片語）+ 補語（名詞、代名詞、形容詞、片語、子句）+ 修飾語（形容詞、副詞）\n 受詞:直接與間接:\n 在述部中，用來接受動詞之動作的對象，稱為受詞【中文文法稱做「賓語」】。 其中直接接受動作的，稱為直接受詞； 間接接受動作的，稱為間接受詞。  補語: 主詞與受詞:\n 在述部中，用來補充說明主詞或受詞之情況的語詞，稱為補語。 其中補充說明主詞的，稱為主詞補語；補充說明受詞的，稱為受詞補語。   五大句型\n **3. **: ****:\n濾鏡四: 片語分析: 名詞片語(Noun Phrase) (詞法學 Morphology)  八大片語: 名詞片語, 動詞片語, 形容詞片語, 副詞片語, 介系詞片語, 分詞片語, 動名詞片語, 不定詞片語\n 1. 名詞片語(Noun Phrase):\n Noun Phrase: Noun + Article + Adjective + Prepositional Phrase + Determiner\n 2. 動詞片語(Verb Phrase):\n 用helping verb改變語氣 用tense改變relative time 用voice改變主動被動  3. 形容詞片語(Adjective Phrase):\n 形容主語(補語功能) 形容賓語(補語功能) 數詞是一種特殊的主語, 也可以用形容詞片語修飾  4. 副詞片語(Adverb Phrase):\n 修飾動詞. 在英語中, 形式上很像介系詞片語. 功能多樣, 可以回答when, where, why, how. 不能回答who,what. 就近修飾原則: 不同位置有不同修飾功能.   5. 介系詞片語(Prepositional Phrase):\n 格退化後的產物: by(五格), at(六格), for(三格), in(六格), on(六格). 根據修飾功能, 可能是形容詞片語(修飾名詞), 也可能是副詞片語(修飾動詞).   6. 分詞片語(Participle Phrase):\n 關係代名詞縮減版: 修辭目的, 把關係代名詞省略掉. 現在分詞修飾「主動語氣」中的主語:主動動詞轉分詞片語, 做形容詞功能修飾主語 過去分詞修飾「被動語氣」中的主語：被動動詞轉分詞片語, 做形容詞功能修飾主語  7. 動名詞片語(Gerund Phrase):\n 動詞轉品, 成為主語: 當主語要是「動作」, 需要將動詞轉為「動名詞片語」來當作主語. 名詞行為: 動名詞片語的行為同名詞, 可以做「主詞」, 「直接受詞」, 「間接受詞」三者作用. 更進階也可當作「謂語名詞」, 「同位語」使用.  8. 不定詞片語(Infinitive Phrase):\n 補充意思不完整的動詞. 動詞可以加的賓語, 修飾成分, 不定詞片語都可以繼承 可以做形容詞, 修飾名詞(表作用, 動作) 也可以做副詞, 修飾動詞(表目的, 潛台詞)   Output: 輸出可重用備忘- Source-模式1-Clause-模式2-Expression-模式3-Case Phrase-模式4-Collocation.  文本\u0026ndash;\u0026gt;去掉子句轉接\u0026ndash;\u0026gt;子句群 子句\u0026ndash;\u0026gt;主部(Subject);述部(Predicate)\u0026ndash;\u0026gt;兩種表達 主部, 述部\u0026ndash;\u0026gt;格理論\u0026ndash;\u0026gt;六格片語 六格片語\u0026ndash;\u0026gt; 格還原\u0026ndash;\u0026gt; 搭配詞(Collocation)    製作Metadata, 與dataview配合.\n 輸出模式一(濾鏡一):\n 句子種類:Sentence Variety:{Sentence}\u0026ndash;\u0026gt;{Simple, Compound, Complex, Compund-Complex}. Clause:{Clause1, Clause2, Clause3}   關鍵: 讓「連接詞(Conjunction)」留在Source文檔.\n 輸出模式一: Source \u0026ndash;「句子類型, 連接詞」\u0026ndash; Clause  4種tags: #Simple;#Compound;#Complex; #CompoundComplex\n  連接詞應該就不用tags了.\n Metadata 1: Sentence Variety: 四種句子類型. Metadata 2: Coordination: {If not simple sentence}, 對等連接詞, 從屬連結詞. Metadata 3: Sentence Modifier: 也有可能有修飾整個子句的成分, 在這個模式要抓出來去掉. Output: Clause list: 可以有1,2,3個Clause\n輸出模式二: Clause \u0026ndash; 「主述部切割」\u0026ndash; {Subject Expression, Predicate Expression}  兩種Tags: #SubjectExpression; #PredicateExpression\n Metadata 1: Subject Expression: 主部, 包含主語, 主語修飾成分\nMetadata 2: Predicate Expression: 述部, 包含動詞需要的搭配, 修飾動詞的副詞.\n輸出模式三: {Subject Expression, Predicate Expression} \u0026ndash; 「格理論」\u0026ndash;{Case Phrase}  六種tags: #Num; #Gen; #Dat; #Acc; #Ins; #Prep\n  主部都是主格, 直接格標記Nominative\n  述部的成分可能有五格, 需要做五格標記\n Metadata 1: Nom: {Subject Expression}\u0026ndash;\u0026gt;{Nominative Case}\nMetadata 2: Gen: {Predicate Expression}\u0026ndash;\u0026gt;{Genitive Case}\nMetadata 3: Dat: {Predicate Expression}\u0026ndash;\u0026gt;{Dative Case}\nMetadata 4: Acc: {Predicate Expression}\u0026ndash;\u0026gt;{Accusitive Case}\nMetadata 5: Ins: {Predicate Expression}\u0026ndash;\u0026gt;{Instrumental Case}\n 方法手段  Metadata 6: Prep: {Predicate Expression}\u0026ndash;\u0026gt;{ Prepositional Case}\n可否用六格理論(Case Theory)來拆解主述部?:\n 主格(Nominative;被給的名稱) 屬格(Genitive;與生俱來的) 與格(Dative;給與) 賓格(Accusitive;被影響的) 工具格(Instrumental;完成行動所憑藉的工具方法) 前置格(Prepositional;時空關係)  可否用句法形態配列(Morphosyntactic alignment)來拆解主述部?:\n 施事論元(A;Agent) 受事論元(P;Patient) 變元(S;Subject)  分解語言的句法型態配列:\n   句法型態配列 定義 實例     主賓型 (Nominative–accusative alignment) 主格=A,S; 賓格=P 英語, 日語, 世界上大多數語言   中立型 (Neutral) A,P,S 三者無格的區分 漢語    輸出模式四: {Case Phrase}\u0026ndash;「文化底蘊」\u0026ndash;{Collocation} Metadata 1: Nom成分分析:\n 做動作的  Metadata 2: Gen成分分析:\n 與生俱來的  Metadata 3: Dat成分分析:\n 被給予的  Metadata 4: Acc成分分析:\n 被影響的  Metadata 5: Ins成分分析:\n 方法手段 工具  Metadata 6: Prep成分分析:\n 空間關係 時間關係  後記 寫了150分鐘, 感受到體系的龐大！之後還需要花些精力研究具體的phrase構造, 主部述部的各種細節. 走通後可以建立一套metadata系統, 就可以在Obsidian建立有用的template來標準化文件輸出.\n繼續寫了100分鐘, 感受到格理論的好用之處, 在主述部切割以後, 進行格標記, 來把句子拆做六大格片語; 利用格片語, 再來與單字連結, 實踐最小意義單位. 感覺系統已經workable了！實踐到句子分析上玩玩看.\n實踐在Obsidian上面感覺不錯！持續改善系統, 可以scaling the language learning progress!\n2022.02.06. 紫蕊 於 西拉法葉, 印第安納, 美國.\n    Version Date Summary     0.1 2022-02-06 初次組織, 思考語言學, 思考片語, 思考格理論.    ","date":"2022-02-06","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur039-%E6%80%9D%E8%80%83%E5%8F%A5%E5%AD%90%E7%9A%84%E5%8D%A1%E7%89%87%E7%AD%86%E8%A8%98%E7%B3%BB%E7%B5%B1/","series":["每日文章","學術工作流"],"tags":[],"title":"MUR039 思考句子的卡片筆記系統"},{"categories":["貝氏思維"],"content":"由後驗取樣學習最優化  紫式晦澀每日一篇文章第38天\n 前言   今天是2022年第36天, 全年第5週, 二月的第1個週六. 今天來思考後驗取樣來學習最優化.\n  今天的素材主要來自文章:\n   2014: Learning to Optimize via Posterior Sampling   摘要:後驗取樣的極大優勢 摘要:後驗取樣的極大優勢:\n 學習行動最優化(Learning to optimize actions): 平衡探索與剝削(Balance between exploration and exploitation) Thompson Sampling (機率匹配 Probability Matching): 比UCB更優, 可以申請有限或無限行動. 理論貢獻一: 轉換UCB後悔界成為後驗取樣的貝氏後悔界. 理論貢獻二: 後驗取樣的貝氏後悔界, 對許多模型族. 逃避者維度(Eluder dimension): 測量「行動獎勵(action reward)」相依程度. 在線性模型下, 此方法的廣義bound 可以match最好的可能, 也比最好廣義線性模型的結果好. 分析展現出後驗取樣的表現優勢, 實驗展現出比UCB還要好很多的表現   This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multiarmed bandit problems. The algorithm, also known as Thompson Sampling and as probability matching, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms.\n 文章組成:\n   Introduction    Related literature.   2.1. Measures of performance. 2.2. Related results.  Problem formulation.   3.1. On regret and Bayesian regret. 3.2. On changing action sets.  Algorithms.   4.1. UCB algorithms. 4.2. Posterior sampling. 4.3. Potential advantage of posterior sampling.  Confidence bounds and regret decompositions.   5.1. UCB regret decomposition. 5.2. Posterior sampling regret decomposition.  From UCB to posterior sampling regret bounds.   6.1. Finitely many actions. 6.2. Linear and generalized linear models. 6.2.1. Linear models. 6.2.2. Generalized linear models. 6.3. Gaussian processes.  Bounds for general function classes.   7.1. Confidence bounds. 7.2. Bayesian regret bounds. 7.3. Relation to the VC dimension.  Simulation results.    Conclusion.   Appendix A. Details regarding Lemma 1. Appendix B. Proof of confidence bound. B.1. Preliminaries: Martingale exponential inequalities. B.2. Proof of Lemma 3. B.3. Least squares bound—Proof of Proposition 6. B.4. Discretization error. Appendix C. Bounds on eluder dimension for common function classes. C.1. Finite action spaces. C.2. Linear case. C.3. Generalized linear models.  訣竅一. 鞅指數不等式 (Martingale Exponential Inequality)  Lemma 6 \u0026ndash;\u0026gt; Lemma 7 \u0026ndash;\u0026gt; Lemma 3 \u0026ndash;\u0026gt; Section 7.1. Confidence bounds. \u0026ndash;\u0026gt; Section 7: Bounds for general function classes.\n 構造: 過濾隨機變數序列, 中心化隨機變數, 條件累積生成函數, 指數鞅 過濾隨機變數序列, 中心化隨機變數, 條件累積生成函數, 指數鞅:\n 一組隨機變數序列, 適應一組過濾(filtration) 假設此組隨機變數序列的「累計生成函數」是有限的 定義「條件均值(Conditional mean)」 定義「中心化隨機變數」的「條件累積生成函數」 定義「指數鞅 (Exponential martingale)」   引理6:指數鞅是鞅, 期望是1. 引理6:指數鞅是鞅, 期望是1.:\n 利用「條件累積生成函數」的定義, 在時間1的時候, 「指數鞅」的期望是1 利用「過濾」, 可以推導出時間n的指數鞅條件期望,是時間n-1的指數鞅. 於是, 滿足的「鞅(Martingale)」的定義.   這其實是在操作某種勢能函數(potential function)?\n 引理7: 累積和的高機率上邊界 引理7: 累積和的高機率上邊界:\n 對任何一個「溫度lambda」, 這都是一個鞅 因此, 對於任何的「停時stopping time」, 指數鞅的期望一直都是1. 定義特殊的停時: 當「指數鞅」大過「截距(Intercept) $x$」 利用「馬可夫不等式」, 「指數鞅超過截距」的機率, 不會大過「1/截距」. 同樣的事件, 表示在時間1,2,\u0026hellip;,n中「過至少一次門檻」的機率, 不會大過「1/截距」. 使用「單調收斂定理(Monotone convergence theorem)」, 得到「在任何時間中過至少一次門檻」的機率, 不會大過「1/截距」. 同樣的道理, 大過「exp(截距)」的機率低於「exp(-截距)」 帶回文脈: 「取樣誤差」* 「溫度」會很靠近條件累積生成函數」(該溫度) 於是, 這個「取樣誤差偏差累積過程」過「門檻」的機率可以被控制住.   「取樣誤差」* 「溫度」會很靠近條件累積生成函數」(該溫度)  ****: ****:\n ****: ****: ****: ****: ****:\n ****: ****: ****: ****: ****:\n ****: ****: ****: ****: ****:\n ****: ****: ****: ****: ****:\n後記 2022.02.05. 紫蕊 於 西拉法葉, 印第安納, 美國.\n    Version Date Summary     0.1 2022-02-05 初次閱讀, 摘要, 文章結構    ","date":"2022-02-05","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur038-%E7%94%B1%E5%BE%8C%E9%A9%97%E5%8F%96%E6%A8%A3%E5%AD%B8%E7%BF%92%E6%9C%80%E5%84%AA%E5%8C%96/","series":["每日文章"],"tags":[],"title":"MUR038 由後驗取樣學習最優化"},{"categories":["寫作","學術工作流"],"content":"讀山口拓郎的摘要力  紫式晦澀每日一篇文章第37天\n 前言   今天是2022年第35天, 全年第5週, 二月的第1個週五. 今天來磨練「摘要」的技術, 標準化相關的知識系統.\n  今天的素材主要來自文章:\n   摘要力：刪掉9成重點，比別人強10倍的表達力   摘要力:向「誰」, 「如何」表達「什麼」  工作 = 不斷摘要\n 摘要三步驟:\n   收集資訊: Input    整理資訊: Process    表達資訊: Output    資訊的種類(收集資訊):\n 1.他人所說的話語（包含閒聊） 2.開會討論時聽到的內容 3.親身經歷之事 4.感官接收到的感受 5.研習或研討會學到的知識 6.文件、資料、信件等記載的資訊 7.報章雜誌或書籍的媒體資訊 8.網站或社群軟體中的資訊 9.以各式資訊做為根柢, 自己另外衍生出的「想法」或「意見」 10.以資訊為基礎建立的「預測」或「假設」  資訊分類三件事(整理資訊):\n   判斷資訊必要與否    將資訊分門別類    為資訊制定優先順序    精簡資訊(表達資訊):\n 將資訊轉為「有連貫性」的內容 表達時捨割九成內容  向對方傳達的資訊優先順序:\n   滿足對方需求的資訊    感覺會讓對方開心的資訊    自己想表達的資訊    步驟一: 收集資訊\u0026ndash;收集充分必要的資訊 訣竅一: 收集充分必要資訊 訣竅一: 收集充分必要資訊:\n 討論的前提, 是擁有該話題的相關資訊.  訣竅二: 從信任的出處收集資訊 訣竅二:從信任的出處收集資訊:\n 可信度高的情報源或熱線. 高品質資訊  訣竅三: 設置資訊收集天線 訣竅三:設置資訊收集天線:\n 設置天線, 就是「命令大腦」, 收集特定方面的資訊. 關注\u0026ndash;\u0026gt;收集.  訣竅四: 利用「自我提問」提高資訊品質-5W3H, If. 訣竅四:利用「自我提問」提高資訊品質:\n 以自我提問, 確認資訊的「真實性」和「價值」. 5W3H: Who, what, when, where, why; how, how many, how much. If: 如果, 預想還沒成為現實的事物 Why: 理由, 原因, 根據, 動機 How: 引導出某事物的方法, 今後將來行動, 進展, 策略.  訣竅五: 分別使用封閉性問題與開放性問題 訣竅五: 分別使用封閉性問題與開放性問題:\n 封閉式-得到對方意見態度: 是或否; A或B, 讓對方只能選擇其一. 開放式-得到對方意見想法: 回答範圍不受限, 引導出對方意見或想法.  訣竅六: 消除資訊中的「主觀想法」 訣竅六: 消除資訊中的「主觀想法」:\n 收幾資訊時, 應注意「認知偏差」, 「刻板印象」,「偏見」 沒有自己體驗, 就先因為評分與意見對其假設好印象壞印象. 保持自覺, 意識到人類是有「認知偏差」的生物.  訣竅七: 提升「後設認知」, 客觀審視自我思考及行為的能力 訣竅七:提升「後設認知」, 客觀審視自我思考及行為的能力:\n 客觀審視自我思考與行為. 寫下自己的想法和情緖（客觀看待自己的想法和情緖） 寫下發生在自己身上的事（客觀看待事件） 讓他人對自己的言行做出評價（客觀看待自我） 思考並寫下「對方可能的想法」（客觀看待對方情緖） 閱讀小說 / 看電影 (感受不同人的各種想法和心情)   貼標籤?\n 訣竅八: 收集資訊, 先看透「本質」-觀察力+洞察力 訣竅八: 收集資訊, 先看透「本質」:\n 本質: 提高摘要的精確度 觀察力: 肉眼可見資訊 洞察力: 內眼不可見, 書沒寫出來的部分 透過觀察, 增加自己所認知的事物, 提高看透本質的能力  訣竅九: 類推法利用資訊擴展思維 訣竅九:類推法利用資訊擴展思維:\n 類推: 將已知情報和經驗, 應用在未知的領域上  訣竅十: 養成轉換「抽象資訊具體資訊」的習慣-關鍵字 訣竅十: 養成轉換「抽象資訊具體資訊」的習慣:\n 收集情報, 掌握主題關鍵字, 是重要習慣. 「詞彙」等同於「資訊網」; 增加「理解的詞彙量」與「詞彙間的連結」 詞彙: 具體資訊, 抽象資訊. 多個具體資訊\u0026ndash;\u0026gt;一個抽象資訊 一個抽象資訊\u0026ndash;\u0026gt;多個具體資訊  步驟二: 整理資訊\u0026ndash;將資訊分類 訣竅一:從理想目標回推以整理資訊 訣竅一:從理想目標回推以整理資訊:\n 摘要的最終目標: 藉由輸出, 將對對方有價值的資訊傳達給對方. 回推: 找到適合整理資訊的方式.  訣竅二: 利用「具體分類思考」徹底整理資訊 訣竅二: 利用「具體分類思考」徹底整理資訊:\n 替資訊貼「標籤」 先引導出對方的需求  訣竅三: 訓練「優先順序思考」 訣竅三: 訓練「優先順序思考」:\n 從分好類的資訊, 挑出所需資訊, 制定先後順序. 選擇：具體分類思考能力 決策: 優先順序思考能力  訣竅四: 摘要的資訊是活的, 需不斷更新-假設, 實踐, 驗證, 修正. 訣竅四: 摘要的資訊是活的, 需不斷更新:\n 意識到「資訊是活著的」 步驟(1)\u0026hellip;..建立一個能檢側該資訊是否有效的假設 步驟(2)\u0026hellip;\u0026hellip;實踐(1)的假設 步驟(3)\u0026hellip;..驗證(2)的實驗結果 步驟(4)\u0026hellip;..根據(3)的結果修正自己的資訊  步驟三: 表達資訊\u0026ndash;簡潔的傳達給對方 訣竅一: 避免「說太多」或「說不夠」 訣竅一: 避免「說太多」或「說不夠」:\n 說太多: 需提高資訊整理的品質 說太少: 注意前提大綱主詞受詞  訣竅二:按照「主幹\u0026ndash;樹枝\u0026ndash;樹葉」的順序說話 訣竅二:按照「主幹\u0026ndash;\u0026gt;樹枝\u0026ndash;\u0026gt;樹葉」的順序說話:\n 主幹: 整體架構 樹枝: 更具體 樹葉: 更更具體  訣竅三: 「一句話」描述整體及結論的技巧 訣竅三: 「一句話」描述整體及結論的技巧:\n 一句話說重點: 抽象化思考, 與之前的具體分類思考相反.  訣竅四: 用「主題＋結論優先型」做說明-主題, 結論, 理由, 細項 訣竅四: 用「主題＋結論優先型」做說明:\n (1)主題（現在要說的內容整體大綱） (2)結論（你想要表達的重點「臨死前最想說的話！」） (3)理由（得到此結論的理由） (4)細項（跟結論有關的細項）  訣竅五: 用「列舉型」做說明-整體架構, 列舉1, 列舉2, 列舉3, 總結 訣竅五: 用「列舉型」做說明-整體架構, 列舉1, 列舉2, 列舉3, 總結:\n (1)整體架構（有幾個重點？) (2)列舉1（第一項重點） (3)列舉2（第二項重點） (4)列舉3（第三項重點） (5)總結  訣竅六: 增加具體性關鍵字, 使用「數字」或「確切名詞」 訣竅六: 增加具體性關鍵字, 使用「數字」或「確切名詞」:\n 確認對方懂不懂「數字」或「確切名詞」.  訣竅七: 回答問題所需的「瞬間摘要力」 訣竅七: 回答問題所需的「瞬間摘要力」:\n 回答問題流程: 收集資訊(掌握對方提問的目的)\u0026ndash;\u0026gt;整理手中資訊\u0026ndash;\u0026gt;表達資訊. (1)正確理解對方提問目的的能力 (2)確實回答對方提問的應答能力  訣竅八: 將摘要「以圖表呈現」 訣竅八: 將摘要「以圖表呈現」:\n 一目瞭然, 瞬間看懂 收集資訊\u0026ndash;\u0026gt; 整理資訊   訣竅九: 提高抽象度表達, 譬喻-本質與喻依 訣竅九: 提高抽象度表達:\n 具體令人感覺攏長 (1)先找出跟原本事件「本質」相通的「喻依」 (譬喻內容) (2)這個「喻依」必須是對方知道的東西 (3)這個「喻依」最好是容易被想像成具體圖片或形象的內容  後記 今天花了90分鐘把「摘要力」這本書走了一趟. 其中也利用了IPO模型, 來從輸入, 處理, 輸出三個方面給內容. 在輸入的過程中, 「關鍵字」是摘要力的根本; 在處理的過程, 根據需求做摘要的迭代; 在輸出的過程, 以模板範本, 從主幹樹枝樹葉的順序給出有結構的報告. 整體非常實用, 而且現在實戰經驗充分, 幾乎所有的策略都親身實踐過了. 但仍然可以欣賞其組織這些材料成系統的功力, 非常棒. 天天向上, 共勉之!\n2022.02.04. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-02-04","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur037-%E8%AE%80%E5%B1%B1%E5%8F%A3%E6%8B%93%E9%83%8E%E7%9A%84%E6%91%98%E8%A6%81%E5%8A%9B/","series":["每日文章"],"tags":["山口拓郎"],"title":"MUR037 讀山口拓郎的摘要力"},{"categories":["合成數據"],"content":"合成數據的六種技法  紫式晦澀每日一篇文章第36天\n 前言   今天是2022年第34天, 全年第5週, 二月的第1個週四. 今天對「合成數據方法(Methods for Synthesizing Data)」做系列學習思考.\n  今天的素材主要來自文章:\n   2021: Practical Synthetic Data Generation 的第五單元 PSDF引用  Google talk  How do you generate synthetic data?   方法一: 從理論合成數據 現象模型: 多變量高斯, 共變異, 耦合 現象模型(Model of the phenomenon):\n 分析師沒有實際數據, 但有「對現象的模型」, 因此可以建立模型去產生數據. 例子1: Gaussian, bell-shaped分佈 \u0026ndash;\u0026gt; 多變量常態分佈. 例子2: 引入「取樣過程的共變異(Correlation during the sampling process)」. 例子3: 使用「耦合(Copulas)」方法.  多變量常態分佈:\n 用各種統計package可以很容易產生.  帶有特定邊緣分佈的共變異(Correlations with Specified Marginal Distributions):\n 例子: 「病人體重」與「住院長度(Length of Stay;LOS). 此例子明顯不是常態分佈. 他的目的是想保留著「共變異(Correlations)」 指定「共變異矩陣(Correlation Matrix)」後來產生樣本. 見Chapter 3 對這些參數化分佈的生成技巧.  具已知邊緣分佈的耦合(Copulas with Known Marginal Distributions):\n 耦合: 建模「邊緣分佈」, 且保留「共變結構」. 耦合的優點：分離「邊緣分佈定義」與「共變結構」.   對Copula相關的理論理解的不多, 之後可以寫寫來學習.\n 高斯耦合, 機率積分轉換 高斯耦合(Gaussian copula):\n 兩個邊緣分佈: 常態分佈, 指數分佈. 高斯耦合: Step 1: 產生標準多變數常態分佈, 帶有共變0.1. Step 2: 利用「機率積分轉換(Probability Integral Transform)」藉由CDF來產生「常態分佈」與「指數分佈」. 先利用標準多維常態分佈計算CDF, 再計算分位數回到「住院時間與病患體重」的常態分佈與指數分佈.  方法二: 從真實資料合成數據 (Generating Realistic Synthetic Data) 無法分辨度量: 高斯, 共變耦合, 決策樹 基於真實數據的合成數據:\n 當「真實數據」存在, 那可以基於真實數據來合成數據, 擺脫基於理論關係產生的合成數據.  出院資料集(Hospital Discharge Dataset):\n 當一個病人出現, 所有「病患醫院體驗」會聚合進入「邊準出現資料庫(Standardized Discharge Databases)」 這些資料庫是分析「保健(Healthcare)」系統的表現,花費, 品質改良, 公共健康中藥的資源. 這些資料成為「出院總結(Discharge abstracts or summaries)」. 考慮三種變數: (1) 出院年紀 (0表達出生) (2) 上次住院間隔 (0表示第一次住院) (3) 住院長度 (0表達沒有住院) 三種變數明顯不是常態分佈\u0026hellip;    作法: 其實就是統計的做法?:\n 擬合「資料邊緣分佈」到一些經典分佈. 我們從「最靠近真實資料」的「經典分佈」來合成數據. 共變矩陣, 從實際數據上取得. 再去算合成數據的共變結構.   合成數據看中原本的「共變結構(Correlation Structure)」\n 使用機器學習fit分佈:\n 利用機器學習, 可以更faithfully的反應「資料真實分佈(Real distributions in the data)」. 有了ML fitted的分佈, 再去執行「共變」與「耦合」的方法.  衡量法:無法分辨度量(distinguishability metric):\n 在utility章節, 使用了「無法分辨度量(distinguishability metric)」 具體有三種度量: 「共變」, 「高斯耦合」, 「決策樹」    機器學習方法, 會比經典分佈方法, 得到好非常多的合成數據.\n 方法三: 混血合成數據 (Hybrid Synthetic Data) 混血合成 = 真實資訊 + 假說資訊 混血合成使用背景:\n 混血合成(Hybrid Synthetic): 一部分使用真實數據, 一部分使用合成數據. 可「在數據增加訊號(add signal to the data)  例子：醫院數據:\n 增加「抽雪茄數目」來表達「抽菸(Smoking)」的程度 增加後用高斯耦合, 產生86%的不抽菸個體的指數分佈合成數據. 加入「共變(Correlations)」到原始數據中. 共變結構要靠自己假設.  混血合成好處:\n 一部分是真實數據. 有額外增加的訊號. 維持原本的共變結構.   混血合成數據集: 真實資訊(Real Information)+假說資訊(Hypothetical Information)\n  這種的確蠻有道理, 藉由「假說(Hypothesis)」去補missing的資料, 產生的合成數據, 來看實際表現.\n 序貫機器學習合成(Sequential ML Synthesis):\n 使用迴歸與分類算法(regression, classification) 實例1: CARTs: classification and regression treees. 實例2: 支撐向量機: Support vector machines. 五個變數要合成: 可以靠各種關係持續去做出來.    這些與Gibbs sampling相關的方法很有關係\u0026hellip;不知道誰是這方面的專家.\n 方法四: 機器學習合成數據 (Machine Learning Methods) 序貫合成變數(Sequentially synthesize variables) 序貫合成變數(Sequentially synthesize variables):\n 使用決策樹 原則: 使用分類與迴歸模型, 序貫合成變數(sequentially synthesize variables)  方法五: 深度學習合成數據 (Deep Learning Methods) 變分自動編碼器 : 非線性的主成分分析 變分自動編碼器(Variational Autoencoder; VAE):\n 變分自動編碼器 : 非線性的主成分分析\n  無監督方法, 從多維數據集學習「表徵」 步驟一 (Encoder): 壓縮資料集, 到低維度更緊緻的表現, 以多維度高斯分佈 步驟二 (Decoder): 將壓縮的表現, 重新造出原始輸入數據. 訓練VAE: 優化「解碼數據(Decoded data)」與「輸入數據(Input data)」之間的「相似性(Similarity)」. 線性版本: 主成分分析  對抗生成網路: 生成與鑑別的輪迴 對抗生成網路 (Generative Adversarial Network; GAN):\n 步驟一: 生成網路(Generator networks): 輸入隨機數據(從高斯分佈或均勻分布取樣), 生成「合成數據」 步驟二: 鑑別器 (Discriminator): 比較「合成數據」與「真實數據」, 計算「傾向分數(Propensity score)」 步驟三: 回傳傾向分數來訓練生成網路.  方法六: 序列合成數據 (Synthesizing Sequences) 服務是事件的序列, 需合成轉移矩陣 事件串:\n 許多資料集, 都有「事件串(Sequences of events)」需要建模. 例子: 離散事件序列(Series of discrete events). 保健使用者: 看醫生, 做診斷檢查, 拿藥. 每個階段可能都還有多重子事件.   合成轉移矩陣(Synthesize transition matrix):\n 需要計算「所有事件之間的轉移矩陣」 可以用排隊理論等等的東西加進來, 很OR風格的研究.   用成熟的Tabular RL算法可以試著解決這些事情? 感覺這邊的確很多東西可以玩, 而且用Deep RL做計算可以搞些研究, 但要商用可能還要很多engineering efforts.\n 後記 大概花了90分鐘, 把合成數據方法章節寫過了一遍. 仔細比較一下, 這個章節寫得蠻好的! 從最「心智」的模型到最「計算」的模型, 展現人與機器的結合程度由淺至深.\n   方法編號 研究範式 實踐技巧     方法一 數理機率範式 多變量高斯, 共變異,耦合   方法二 應用機率範式 多變量高斯, 共變耦合, 決策樹   方法三 統計科學範式 混血合成=真實資訊+假說資訊   方法四 機器學習範式 決策樹, 序貫合成   方法五 深度學習範式 變分自動編碼器, 對抗生成網路   方法六 強化學習範式 服務是事件的序列, 合成轉移矩陣    過去的經驗都還能支撐這六種做法, 非常棒! 我想所謂的工程師, 就是要能很快速實踐各種研究範式裡面的方法, 來認識複雜的世界. 非常有趣！期待之後經驗累積! 天天向上, 共勉之!\n2022.02.03. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-02-03","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur036-%E5%90%88%E6%88%90%E6%95%B8%E6%93%9A%E7%9A%84%E5%85%AD%E7%A8%AE%E6%96%B9%E6%B3%95/","series":["每日文章"],"tags":[],"title":"MUR036 合成數據的六種技法"},{"categories":[],"content":"讀大前研一所敘事的金融科技  紫式晦澀每日一篇文章第35天\n 前言   今天是2022年第33天, 全年第5週, 二月的第1個週三. 今天對「金融科技(Fintech)」來做科普書程度的學習. 早上剛好逛到大前研一的書, 可以對其中金融科技的章節做思考.\n  今天的素材主要來自文章:\n   2017: 科技4.0 網路串聯時代的新商業模式 的第二單元. 沒想到已經是五年前的書了, 時間過得真快.   金融科技改變信用概念\n 金融科技改變通貨與信用 技術革新創造通貨歷史 金融科技(FinTech = Finance + Technology):   信息技術: 智能手機, 大數據. 金融服務, 金融產品. 新技術與存在的經濟模式大不同.   金融服務列表:   金融科技的世界將提供眾多服務, 比如支付、融資、存款、銀行基礎設施、比特幣等虛擬貨幣、財務會計、匯款、投 資等。 換言之, 在運用金融科技的支付、貨幣、財務會計等領 域會產生不同於傳統的全新服務形態。  手機支付與匯款降低成本:   智能手機: 快捷支付, 小額匯款, PayPal. 數據分析: 分析客戶訊息, 提供消費者金融(貸款, 放款)  金融科技的本質-再建構傳統金融領域:   「再建構」傳統金融領域機制: 匯款, 投資, 支付, 貸款, 存款, 財務, 會計. 舊制度: 紙上蓋章.  機器人理財-更少的手續費:   資產管理公司 機器人理財部門 低利率時代, 機器人理財只需更少的手續費  金融科技改變信用擔保物 商業模式：債務結算:   浮動式結算: checking account不夠, 先凍結saving account. 借記卡結算 信用卡結算  個人信用, 應由個人持有, 而非信用卡公司持有:   傳統: 銀行擔保信用, 與商店交易\u0026ndash;\u0026gt;信用由信用卡公司持有 金融科技: 確認個人資金, 就讓交易過關\u0026ndash;\u0026gt;信用由個人持有   如果個人違約, 那會有什麼罰則? 不再能使用信用服務? 何時從黑名單變成白名單?\n 區塊鏈技術:擔保消費, 過程不產生手續費:   傳統: 商家要查詢並確認使用者的信用信息. 金融科技: 區塊鏈技術擔保此消費   但誰來提供區塊鏈技術的算力? 比特幣就是這樣兌換算力的嗎?\n 金融科技四大原理 四大原理:     任何「有價值的東西」都可以替代「貨幣」    「價值」是「時間」的函數    智能手機生態系統, 讓人們, 「隨時」, 「隨地」與「任何人」進行交易    在網路空間中, 出現擔保信用的金融科技企業. 這些企業將取代國家或金融機構, 掌控貨幣流向.     國家不會就這樣放任金融科技做大.\n 原理一: 任何「有價值的東西」都可以替代「貨幣」 擔保物:   任何「人持有或能提供」的資產與服務, 都可以成為擔保物. 東西都會有個價錢.  原理二: 「價值」是「時間」的函數 擔保物的時間價值:   結算時須考慮時間價值, 支付金額隨時間推移而增加.  原理三: 不依賴貨幣的智能手機經濟 行動支付:   只要有手機電話, 不用銀行, 也可以完成交易.   原理四: 金融科技替代國家或金融機構提供信用 信用的新建模:   個人持有擔保品: 個人的IC卡, 電子錢包, 積分點數, 人壽保險, 退休金 金融行為: 與現有貨幣實現換算 需求: 評估換算對象風險, 並決定交換比率. 避免: 應對風險的套利措施.  金融科技如何擴大商機 理解原理和本質:   擴展商業: 運用智能手機. 傳統:以土地做抵押 金融科技: 以業績作為擔保獲取融資  金融科技帶來貿易額增長:   與貨幣等價的服務或物品, 都能「交易」. 交易與貨幣發行量無關.   後記 今天讀這個算是商業科普書, 對於其中講到金融科技四大原理, 講到任何個人能當作擔保品的東西, 在時間下都有其價值, 可以與其他個人交易另外的擔保品. 這個模式讓人之間的資源交換, 提高個人的「效度(Utility)」, 讓大家更高興.  機器學習任務: 「個人擔保品估值」, 「交易行為違規風險」, 「雙邊市場買賣配對」, 「交易效度最大化」 這樣的問題牽扯很多任務, 至少包含「個人擔保品估值」, 「交易行為違規風險」, 「雙邊市場買賣配對」, 「交易效度最大化」等等, 四個任務. 其實這些都有相對應得統計建模, 是我們之後做事情的根據.天天向上, 共勉之!  2022.02.02. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-02-02","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur035-%E8%AE%80%E5%A4%A7%E5%89%8D%E7%A0%94%E4%B8%80%E6%89%80%E6%95%98%E4%BA%8B%E7%9A%84%E9%87%91%E8%9E%8D%E7%A7%91%E6%8A%80/","series":["每日文章"],"tags":["金融科技"],"title":"MUR035 讀大前研一所敘事的金融科技"},{"categories":["物件導向編程"],"content":"作為可重用區塊的Python函數  紫式晦澀每日一篇文章第34天\n 前言   今天是2022年第32天, 全年第5週, 一月的第5個週二. 今天思考「可重用性(Reusability)」. 在Python裡面「可重用性」的最基礎, 就是「寫好函數」的技能. 今天利用文章來組織這項軟技能.\n  今天的素材主要來自文章:\n   2018: Get Programming Learn to code with Python 的第五單元(P.181-232). 此單元完整講解了如何執行模組化與抽象化,   收集「三」的表達.\n 心法: 模組化大事化小, 抽象化以簡馭繁, 形式參數開發者, 實際參數使用者 三課內容:\n 20 建築能持續的程序 21 利用函數達到模組化與抽象 22 函數的進階操作  三課小節: 共11小節:\n 20.1 Breaking a big task into smaller tasks: 20.2 Introducing black boxes of code in programming 20.3 Subtasks exist in their own environments 21.1 Writing a function 21.2 Using functions 21.3 Documenting your functions 22.1 Thinking about functions with two hats 22.2 Function scope 22.3 Nesting functions 22.4 Passing functions as parameters 22.5 Returning a function  三課小小節: 共11小節:\n 20.1 Breaking a big task into smaller tasks:   20.1.1.任務相關/獨立性(Task dependence/independence).     20.1.2.黑盒子(black box)-模組(Module).   20.2 Introducing black boxes of code in programming   20.2.1. Using code modules: 模組化(Modularity)     20.2.2. Abstracting code: 字符串(docstring)     20.2.3. Reusing code: 代碼包裝(code wrapper)   20.3 Subtasks exist in their own environments: 切任務成子任務(dividing a larger task into subtasks), 細節抽象化(abstraction of details). 21.1 Writing a function   21.1.1. 輸入 Function basics: what the function takes in: 參數有兩個英文, parameters, arguments. 在設計函數的層面, 是「形式參數(formal parameters, formal arguments)」     21.1.2. 處理 Function basics: what the function does     21.1.3 輸出 Function basics: what the function returns   21.2 Using functions: 真實參數(Actual parameters). 21.2.1 Returning more than one value: 雖然函數只會回傳「一個物件(a object)」, 但我們可以把需要的東西都包裝成物件.   21.2.2 Functions without a return statement: 沒寫return的話, Python automatically returns the value None in the function.   21.3 Documenting your functions:函數指定(function specifications)或字符串(docstrings)來實踐抽象化. 22.1 Thinking about functions with two hats   22.1.1 Writer hat: 輸入為「形式參數(Formal parameters)」     22.1.2 User hat: 輸入為「實際參數(Actual parameters)」   22.2 Function scope   22.2.1 Simple scoping example 作用域影響函數回傳     22.2.2 Scoping rules 作用域規則   22.3 Nesting functions 22.4 Passing functions as parameters 22.5 Returning a function  Lesson 20: 建築能持續的程序 Lesson 20:建築能持續的程序:\n 如何: 大任務成小模組(how a bigger task is divided into modules) 為何: 隱藏複雜任務的細節( why you should hide away details of complicated tasks) 是什麼: 任務之間「獨立」與「相依」的意涵 ( what it means for tasks to be dependent on or independent ofother tasks)  Lesson 21: 利用函數達到模組化與抽象 Lesson 21 利用函數達到模組化與抽象:\n 寫使用「函數(Functions)」的代碼 寫有「參數(Parameters)」的函數 寫會「回傳特定職(return a specified value)」的函數 理解「變數值(Variable)」是如何在不同函數環境下變化  模組化大事化小;抽象化以簡馭繁:\n 模組化: 把問題拆小, 一個個解決. Modularity is having smaller (more or less independent) problems to tackle, one by one. 抽象化: 以模組為單位思考,不擔心模組內細節. Abstraction is being able to think about the modules themselves at a higher level, without worrying about the details of implementing each.   模組化大事化小;抽象化以簡馭繁\n Lesson 22: 函數的進階操作 Lesson 22 函數的進階操作:\n 將「函數」作為「另一函數的參數」(Pass functions (as an object) as a parameter to another function) 從「函數」回傳「作為物件的函數」(Return a function (as an object) from another function) 利用特定規則, 理解變數所在的「作用域(Scope)」(Understand which variables belong to which scope based on certain rules)  ****: ****:\n技法 字符串(Docstring):\n A docstring contains the following information:   All inputs to the module—Represented by variables and their types.    What the module is supposed to do—Its function.    What output the module gives you—This might be an object (variable) or it might be something that the module prints.    ****: ****: ****: ****: ****:\n用法 ****: ****: ****: ****: ****: ****:\n文法 相關任務(Dependent Task):\n 一個任務要依賴於另一個任務結束才能開始, 就是相關任務. 獨立任務: 兩個任務獨立, 他們就可以同時做.   DEFINITION A task depends on another one if it can\u0026rsquo;t start before the other one completes. Two tasks are independent if they can be performed at the same time.\n 任務抽象(Abstraction of a task):\n 用最少資訊就可以理解任務內容; 隱藏不必要的細節. Definition Abstraction of a task is a way to simplify the task such that you understand it by using the least amount of information; you hide all unnecessary details.  可重用子任務(Reusable subtasks):\n 其步驟, 可以用不同的輸入, 來得到不同的輸出 DEFINITION Reusable subtasks are tasks whose steps can be reused with different inputs to produce different output.  黑盒子(Black box):\n 可視化做某件任務的系統. DEFINITION A black box is a way to visualize a system that does a certain task. A black box on top of the system reminds you that you don\u0026rsquo;t get to [or need to] see inside the box in order to understand what the system does.  模塊(Module): IPO-輸入, 任務, 輸出:\n 代碼模塊: 達成特定任務的代碼 模塊: 輸入-任務-輸出 A code module is a piece of code that achieves a certain task. A module is associated with input, a task, and output.   函數(Function):簡單任務的代碼模組:\n 函數輸入; 函數處理; 函數輸出 In many programing languages, a function is used to stand for a module of code that achieves a simple task. When you’re writing a function, you have to think about three things:   What input the function takes in    What operations/calculations the function does    What the function returns    作用域(Scope):\n 同name的變數, 要靠作用域來區分.  ****: ****:\n後記 Ver0.1 (2/1): 今天對文章中各種圖, 文章結構想強調的點, 進行了80分鐘的思考.本文章的結構非常好, 從「連環圖」的角度講了很棒的科學研究故事. 多欣賞好文章, 來讓自己的文章的readability上升. 又追加30分鐘, 把一些專有名詞與配圖補上. 要把函數寫好, 心中要常有「可複用」的原則, 才可以抽象鍛鍊出好的「函數」. 作為函數的文本, 要如何在不同的paper中重複被使用呢?\n 對於證明類的文本, 如何做圖與可視化呢? 收集好的文獻?\n 2022.02.01. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-02-01","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur034-%E4%BD%9C%E7%82%BA%E5%8F%AF%E9%87%8D%E7%94%A8%E5%8D%80%E5%A1%8A%E7%9A%84python%E5%87%BD%E6%95%B8/","series":["每日文章","編程思維"],"tags":["Python"],"title":"MUR034 作為可重用區塊的Python函數"},{"categories":[],"content":"如何衡量合成數據的真實性與準確性?  紫式晦澀每日一篇文章第33天\n 前言   今天是2022年第31天, 全年第5週, 一月的第5個週一. 今天對合成數據的真實性與準確性相關文章來做反思.\n  今天的素材主要來自文章:\n   2021: CHow Faithful is your Synthetic Data?Sample-level Metrics for Evaluating and Auditing Generative Models    收集「三」的表達.\n 心法: 保真度, 多樣性, 泛化力 度量三大品質(Metric quantify 3 synthetic data qualities):\n alpha準確-保真度 (alpha-Precision-fidelity) beta召回-多樣性(beta-Recall-diversity) 真實性-泛化力 (Authenticity-Generalization)    評估途徑(Evaluation Pipeline):\n   評估嵌入 (Evaluation embedding)    樣本層級度量 (Sample-level metrics)    評估度量 (Evaluation metric)    監管途徑(Auditing Pipeline):\n   「生成模型(Generative model)」進「評估途徑」    樣本過濾器(Sample filter): 因為是「sample-level」, 對每個樣本問「是否是高品質樣本(High-quality sample?)」    品質管控: 決定要不要刪掉特定sample     準確曲線\u0026amp;召回曲線(alpha-precision curve \u0026amp; beta-recall curve):\n   模式崩塌 (Mode Collapse)    模式發明 (Mode Invention)    密度遷移 (Density Shift)    基於合成數據的預測建模(Predictive Modeling using Synthetic Data):\n   排名生成模型(Ranking generative models)    超參數優化 (Hyper-parameter optimization)    審計改良合成數據 (Synthetic data improvement via auditing)     診斷模式崩潰(Diagnose mode collapse):\n衡量躲貓貓挑戰的參賽者:\nㄅㄅ\n技法-理想世界 Alpha準確性:合成數據分佈能生成「正常真實數據」的機率 alpla準確性度量($\\alpha$-precision metric):\n   源頭: 合成分佈抓住真實數據的程度 (Sajjadi et al., 2018)    目的: 合成分佈抓住「$1-\\alpha$真實數據」的機率    白話: 把真實數據的「離群值(Outlier)」去掉, 以抓住「正常的真實數據」. 檢查合成分佈是否有很多「沒合成到的真實數據」    Beta召回度: 真實數據分佈能生成「正常合成數據」的機率 beta召回度度量($\\beta$-recall metric):\n   源頭: ??? Recall metric    目的: 真實數據分佈抓住「$1-\\beta$合成數據」的機率    白話: 檢查合成數據是否造出很多「真實不存在的數據」    真實性:造出「訓練數據」以外數據的能力  泛化是造出「訓練數據」以外數據的能力.\n 需要本真資料來確保「泛化能力」:\n 漏洞: 只要重抽樣訓練資料, 就能得到很好的fidelity and diversity; 但壞處就是無法「泛化(Generalization)」 本真指標(Authenticity score): 生成模型是否有能力產生「訓練資料以外」的樣本. 加Noise可能會出現新的樣本. 以A的機率「創新數據(innovate new sample)」  準確性-召回度分析 小支撐是典型;大支撐有離群 理解alpha支撐與beta支撐:\n alpla-支撐: 那些看起來「真實(Realistic)」且「典型(Typical)」的「合成數據(Synthetic sample)」 beta-支撐: 在「合成數據分佈(Synthetic data distribution)」下, 「典型(Typical)」的「真實數據(Real sample)」   扎實質量: 最扎實集中的alpha質量(the most densely packed probability mass α in a distribution,)    集中峰值: alpla-支撐總是集中於「背後分佈的峰值 (concentrate around the modes)」    不計算離群值: 在計算「忠貞度(fidelity)」與「廣度(diversity)」只計算「典型樣本(Typical sample)」, 不計算離群值.      從典型(0)到離群(1):\n alpha, beta = 0: 典型 alpha, beta = 1: 離群 alpha=0.1支撐: 最緊實, 靠近眾數的那些10%群體 alpha=0.9支撐: 蠻鬆散, 涵蓋90%的群體 $P_{\\alpha}$: 合成數據分佈, 產生「典型(百分之alpha)的真實數據」,的機率 $R_{\\beta}$: 真實數據分佈, 產生「典型(百分之beta)的真實數據」, 的機率.    三種失敗: 太像某個, 沒有一個像, 像了但比例不對 準確召回分析(a):眾數塌陷(Mode Collapse):\n 太像真實數據中的某一組了\n  (a) alpha:0.25\u0026ndash;\u0026gt;最典型（百分之25的真實數據)\u0026ndash;\u0026gt; (a)可以以超過一半的機率成功產生 (a) alpha:0.50\u0026ndash;\u0026gt;中典型 (百分之50的真實數據)\u0026ndash;\u0026gt;(a)也可以以超過一半的機率成功產生 (a) alpha:0.75\u0026ndash;\u0026gt;全典型 (百分之75)的真實數據\u0026ndash;\u0026gt;(a) 增加的成功率就不多了\u0026ndash;\u0026gt; 新的典型無法產生\u0026ndash;\u0026gt;模式崩塌 (a) beta:0.25 \u0026ndash;\u0026gt; 最典型(百分之25的合成數據)\u0026ndash;\u0026gt;(a)真實數據出現「最典型合成數據」的機率不高 (a) beta:0.50 \u0026ndash;\u0026gt; 中典型(百分之50的合成數據)\u0026ndash;\u0026gt;(a)真實數據出現「中典型合成數據」的機率不高 (a) beta:0.75 \u0026ndash;\u0026gt; 全典型(百分之75的合成數據)\u0026ndash;\u0026gt;(a)真實數據出現「全典型合成數據」的機率不高   準確召回分析(b):眾數發明(Mode Invention):\n 完全不像真實數據裡的組\n  (b) alpha:0.25\u0026ndash;\u0026gt;最典型（百分之25的真實數據)\u0026ndash;\u0026gt; (b)不到一半的機率成功產生 (b) alpha:0.50\u0026ndash;\u0026gt;中典型 (百分之50的真實數據)\u0026ndash;\u0026gt;ba)不到一半的機率成功產生, 成功機率也增加不多 (b) alpha:0.75\u0026ndash;\u0026gt;全典型 (百分之75)的真實數據\u0026ndash;\u0026gt;(b) 不到一半的機率成功產生, 但成功機率增加很快\u0026ndash;\u0026gt; 有涵蓋到真實數據了\u0026ndash;\u0026gt;發展出新的不存在於真實數據的模式 (b) beta:0.25 \u0026ndash;\u0026gt; 最典型(百分之25的合成數據)\u0026ndash;\u0026gt;(b)真實數據出現「最典型合成數據」的機率不高\u0026ndash;\u0026gt; 真實不太存在 (b) beta:0.50 \u0026ndash;\u0026gt; 中典型(百分之50的合成數據)\u0026ndash;\u0026gt;(b)真實數據出現「中典型合成數據」的機率不高\u0026ndash;\u0026gt;真實不太存在 (b) beta:0.75 \u0026ndash;\u0026gt; 全典型(百分之75的合成數據)\u0026ndash;\u0026gt;(b)真實數據出現「全典型合成數據」的機率不高\u0026ndash;\u0026gt; 開始涵蓋到一些真實存在的數據了   準確召回分析(c):密度遷移(Density shift):\n 真實數據裡的組對了, 但比例不對\n  (c) alpha:0.25\u0026ndash;\u0026gt;最典型（百分之25的真實數據)\u0026ndash;\u0026gt; (c)不到一半的機率成功產生 (c) alpha:0.50\u0026ndash;\u0026gt;中典型 (百分之50的真實數據)\u0026ndash;\u0026gt;(c)不到一半的機率成功產生, 帶成功率增加了, 代表有能力合成真實數據了 (c) alpha:0.75\u0026ndash;\u0026gt;全典型 (百分之75)的真實數據\u0026ndash;\u0026gt;(c) 不到一半的機率成功產生\u0026ndash;\u0026gt; 成功率也持續增加\u0026ndash;\u0026gt;可以合成常見的真實數據了. (c) beta:0.25 \u0026ndash;\u0026gt; 最典型(百分之25的合成數據)\u0026ndash;\u0026gt;(c)真實數據出現「最典型合成數據」的機率不高\u0026ndash;\u0026gt;合成的都不是真實會有的 (c) beta:0.50 \u0026ndash;\u0026gt; 中典型(百分之50的合成數據)\u0026ndash;\u0026gt;(c)真實數據出現「中典型合成數據」的機率不高\u0026ndash;\u0026gt;合成的慢慢有真實會有的了 (c) beta:0.75 \u0026ndash;\u0026gt; 全典型(百分之75的合成數據)\u0026ndash;\u0026gt;(c)真實數據出現「全典型合成數據」的機率不高\u0026ndash;\u0026gt;合成的是真實會有的了, 只是比例不太對.   技法-可測量世界:造二元分類器 估計衡量測度(Estimate Evaluation Metric):三種二元分數:\n 三大度量都是「樣本等級(Sample-level)」, 現在要討論實際上要怎麼算出這三大度量.   準確度: 對「合成數據」, 給一個二元分數$\\widehat{P}_{\\alpha, j}\\in\\{0,1\\}$.    召回率:對「真實數據」, 給一個二元分數$\\widehat{R}_{\\beta, i}\\in\\{0,1\\}$.    本真性: 對「合成數據」, 給一個二元分數$\\hat{A}_{j}\\in\\{0,1\\}$    估計衡量測度(Estimate Evaluation Metric):三種二分類器:\n 如何得到上述的三種二元分數, 依靠訓練「二元分類器(Binary Classifier)」$f_{P}, f_{R}, f_{A}: \\widetilde{\\mathcal{X}} \\rightarrow{0,1}$   準確度:     召回率:    本真性:     準確度分類器實踐: 離群值偵測技術造分類器 準確度分類器實踐:\n 分類器準則: 看看「合成數據分佈」有沒有「落入真實數據alpha支撐」 主要難點: 計算「真實數據」的「alpha支撐」 主要作法: 訓練「衡量鑲嵌(Evaluation Embedding)」, 在embedding空間找支撐. 做法背景: one-class SVMs (Schölkopf et al., 2001); outliner detection (Ruff et al., 2018). 實際alpha支撐: 到對的空間後, 用那邊的分位數(quantile)函數就可以處理. 會依靠embedding後, 實際資料的中心點.  分類準則: 「一個合成數據」得1分如果「其落入真實數據的alpha支撐」:   召回度分類器實踐: 離群值偵測技術造分類器 召回度分類器實踐:\n 分類器準則: 看看「真實數據分佈」有沒有「落入合成數據beta支撐」 主要難點: 計算「合成數據」的「beta支撐」 做法背景: one-class SVMs (Schölkopf et al., 2001); outliner detection (Ruff et al., 2018). 實際beta支撐: 到對的空間後, 用那邊的分位數(quantile)函數就可以處理. 會依靠embedding後, 實際資料的中心點.  分類準則: 「一個合成數據」得1分如果「其落入合成數據的beta支撐」.    但這裡用了「k最近鄰居」來當半徑, 不知道有什麼考量?\n 本真性分類器實踐: 似然比檢定造分類器 本真性分類器實踐:\n 分類器準則: 看看「合成數據分佈」是不是「用了很少的訓練數據」, 因此比較多「原創數據」. 主要難點: 判斷「混合分佈(mixture distribution)」的成分, 是在分佈等級上做某種分類器. 做法背景: 假設檢定-null:來自訓練數據;alternative:來自合成數據. Likelihood-ratio test (LRT) statistic (Van Trees, 2004); Neyman- Pearson Lemma, the LRT above is the most powerful test for authenticity (Huber \u0026amp; Strassen, 1973).  實際似然比檢定: 由於「似然比」上下的分佈都不知道, 所以要用替代的「充分統計量(Sufficient statistic)」來做事.   實際作法是計算「單體合成數據」與「實際數據集合」的最短距離;     也計算「最近點實際數據」與「離一實際數據集合」的距離.     兩種距離比大小得到的統計量, 表示「此合成數據」是否比「最近點實際數據」還要靠近「真實數據集」?     這種似然比真的蠻奇怪, 他想要側「混合分佈中的成分」, 但實際執行卻是用「幾何上靠近真實數據集的程度」. 為何「靠近」就能做到想要的本真性測試?\n  分類準則: 似然比愈大, 就越表達是「原創合成數據」; 與真實資料太靠近, 就說是「不本真(unauthentic)」( unauthentic if it is closer to 最近訓練數據 than any other real sample in the training data)   記得null是來自訓練數據; A=0 是完全來自訓練數據; A=1是完全來自合成數據.     似然比的分母, 是「訓練數據(A=0)」比「最近訓練數據」更靠近真實資料集, 此時分母為1.     似然比的分佈, 在「合成數據(A=1)」的時候, 會靠近0(因為合成數據很難再靠近資料集?), 此時分母靠近0, 似然比跑到無窮.   哲學: 如果「單體合成數據」太靠近「最近點真實數據」, 那就不本真, 就給A=0; 離原本的訓練及夠遠, 才算本真, 才真的能造出新的合成數據, 展現泛化能力.     如果要「合成梯度」那還能真的找到最小值嗎?\n  泛化到沒看過的地方, 那遷移聯邦學習(Transfer Federated Learning)這類的任務會有幫助嗎? 誰幫助誰?\n 推廣準確召回分析 兩個分佈要像: domain要一樣, shape也要一樣; 而準確召回曲線刻畫了兩者. 為何看alpha-beta支撐, 能推廣準確召回分析?:\n 不只考慮「真實數據分佈」「合成數據分佈」的之聲, 也考慮其「機率密度」. 這邊的做法, 「不會一致對待每個樣本」, 而是給「密集地區的樣本」更高的重要性 好的準確召回曲線, 能夠表達出「真實數據分佈」與「合成數據分佈」達到了「同樣的眾數」而非單純的「同樣的支撐」.   兩個分佈要像: domain要一樣, shape也要一樣; 而準確召回曲線刻畫了兩者.\n   準確召回度量的三大缺點: 對離群者的穩健性, 無法偵測配對分佈, 無法診斷分佈失敗 傳統準確召回度量的三大缺點:\n 準確召回曲線克服了準確召回度量的缺點 缺點一: 對離群者的穩健性(Lack of robustness to outliers) 缺點二: 無法偵測配對分佈(Failure to detect matching distributions) 缺點三: 無法診斷分佈失敗(眾數崩潰,眾數發明, 密度遷移)(Inability to diagnose different types of distributional failure) (Naeem et al., 2020)    最優的準確召回曲線: 分佈(shape)要重和, 密度(support)也要重和 最優的準確召回曲線:\n 當兩個分佈與其密度皆重合, 那要很靠近兩圖的中間線.(難怪要畫圖列出中間線)   很棒的point, 可以定義準確召回曲線的「統計距離測度(measure of statistical distance)」\n 統計出入(Statistical Discrepancy):與中線的面積  Fidelity \u0026amp; Diversity\n 測度統計出入(Statistical Discrepancy):\n 計算「與中線的面積」來當作「統計出入」 $\\text{IP}_{\\alpha}$: 愈大愈好, 表達「合成數據」對「真實數據分佈」的「代表性(adequately represented)」   生成一堆在真實沒有代表性的\n  $\\text{IR}_{\\beta}$: 愈大愈好, 表達「真實數據」對「合成數據分佈」的「代表性(adequately represented)」   只能生成沒有代表性的\n  如「f-散度」這類的度量, 會需要要求「共同支撐(Common support)」, 如此就產生了限制.   這個與ROC, AUC之間的關聯? 之後可以研究研究\n    ****: ****: ****: ****: ****:\n用法 合成COVID-19數據(Synthesizing COVID-19 data):\n合成圖像數據-MNIST數據生成:\n合成時間序列數據-Hide-and-Seek挑戰:\n****: ****: ****:\n語法 找到「非離群值們」其實不容易:\n   在1維數據, 只需要用quantile就可以描述「非離群值們」.    在多維數據, 利用「衡量鑲嵌(Evaluation Embedding)」來投射到「半徑r的超球(Hypersphere)」; 借助「離群值偵測(Outliner detection)」問題中的損失函數.    在超球上就可以使用熟悉的機率測度來算機率了    ****: ****: ****: ****: ****:\n後記 Ver0.1(1/31): 今天對文章中各種圖, 文章結構想強調的點, 進行了80分鐘的思考.本文章的結構非常好, 從「連環圖」的角度講了很棒的科學研究故事. 多欣賞好文章, 來讓自己的文章的readability上升.\n 對於證明類的文本, 如何做圖與可視化呢? 收集好的文獻?\n 2022.01.31. 紫蕊 於 西拉法葉, 印第安納, 美國.\nVer0.2(2/1): 今天對3 metrics的估計細節去仔細思考. 首先, 分類器的構造, 依賴於「1-class SVM」, 「K-nearest neiborhood」以及「Likelihood Ratio Test」. 很多作法在統計的觀點下感覺很粗糙, 但是實際要怎樣才能做更好其實也很未知. 持續用文章思考, 來讓更多的概念融會貫通.\n2022.02.01. 紫蕊 於 西拉法葉, 印第安納, 美國.\nVer0.3(2/2): 今天對三種failure mode做仔細地解釋. 現在看到的是, 我們要先定義「典型」與「離群」, 接著移動「典型離群邊界」來看「合成分佈產生真實存在數據」以及「真實分佈會有合成數據」兩個指標. 當真實數據有兩種貓, (1) 只學一種貓 (2) 兩種貓都沒學到 (3) 學到兩種貓但比例不對, 的三種狀況, 都會contribute不合格的數據生成模型. 然而, 怎麼樣的數據生成模型才是好的呢? 如果要與訓練數據很像, 為何不要resample就好呢? resample可以確保與訓練數據很像, 但這就喪失了「泛化能力(Generalizability)」. 很多謎團得慢慢解\u0026hellip;\n2022.02.02. 紫蕊 於 西拉法葉, 印第安納, 美國.\nVer0.4(2/3): 今天思考了準確召回曲線, 其最優的狀況, 可以翻譯成圖中的中線. 如此, 就可以計算「與中線的面積」來當作「統計出入」來橫量不同的「合成數據模型」. 這個思想還蠻深邃的, 再多加上檢查「泛化能力」後, 形成鐵三角度量: 忠貞, 多樣, 泛化. 寫作上也是很清楚, 每個地方的細節恰到好處.\n2022.02.03. 紫蕊 於 西拉法葉, 印第安納, 美國.\n    Version Date Summary     0.1 2022-01-31 初次閱讀   0.2 2022-02-01 仔細思考3個度量的estimation所會遇到的問題   0.3 2022-02-02 對三種failure mode做仔細地解釋   0.4 2022-02-03 思考最優準確召回曲線, 研究統計出入, 形成鐵三角度量: 忠貞, 多樣, 泛化.    ","date":"2022-01-31","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur033-%E5%A6%82%E4%BD%95%E8%A1%A1%E9%87%8F%E5%90%88%E6%88%90%E6%95%B8%E6%93%9A%E7%9A%84%E7%9C%9F%E5%AF%A6%E6%80%A7%E8%88%87%E6%BA%96%E7%A2%BA%E6%80%A7/","series":["每日文章"],"tags":[],"title":"MUR033 如何衡量合成數據的真實性與準確性? (Ver 0.4)"},{"categories":[],"content":"思考Information Ratio  紫式晦澀每日一篇文章第32天\n 前言   今天是2022年第30天, 全年第5週, 一月的第五個週日. 今天來爬梳「資訊比(Information Ratio)」的相關知識.\n  今天的素材主要來自各種關於Information Ratio的文章.\n   2019: Connections Between Mirror Descent, Thompson Sampling and the Information Ratio  2020 Thesis: Adversarially robust stochastic multi-armed bandits  2014 MOR: Learning to Optimize via Posterior Sampling  2018: Learning to optimize via information-directed sampling   心法 資訊比的非形式化定義與直覺:\n 證明的基礎: 資訊比 $$ \\text { information ratio }_{t}=\\frac{(\\text { expected regret in round } t)^{2}}{\\text { expected information gain in round } t}, $$ 資訊比小: 學習者的後悔夠小, 或者得到資訊夠多.   要馬「決策得好」要馬「學很多」\n 資訊獲取(Information gain)的實踐法:\n 相互資訊 (Mutual Information)[25] 廣義Bregman divergence (Generalization on a Bregman divergence)[21]  資訊理論分析搭非負熵勢能-minimax最優:\n 資訊理論分析(Information-Theoretic Analysis) 非負熵勢能(negentropy potential) 得到的bounds, 在K-armed bandit與Exp3雷同. 在Tsallis entropy與INF strategy也成立[6]J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceedings of Conference on Learning Theory (COLT), pages 217–226, 2009.  資訊比與線上隨機鏡面下降:\n 分析技巧的雷同 Bound資訊比(Bound information ratio) 控制線上隨機鏡面下降的穩定度(Control stability of online stochastic mirror descent)  技法  用OSMD的技術設計TS算法\n但這樣bandit與full information的邊界在哪裡?\n 連結四大問題:資訊理論, 鏡面下降, 貝氏後悔, 對抗學習:\n 分析「線上隨機鏡面下降」的工具, 可以得到一種版本的「Thompson取樣」, 把「鏡面下降更新(Mirror descent update)」換成「貝氏更新(Bayesian update)」. 資訊理論分析\u0026lt;-\u0026gt;線上隨機鏡面下降(OSMD)更新分析 OSMD \u0026lt;-\u0026gt; 貝氏後悔分析 貝氏後悔分析\u0026ndash;\u0026gt;對抗框架  關鍵函數:\n 取樣策略P: 決定算法如何探索 勢能函數F: 凸勢能, 估計函數E: 估計無法觀察的損失向量  最小化「獲得每單位資訊的成本」:\n 權衡:小的期望後悔 VS 學那個行為是最優新資訊 範圍: 各種取樣分佈 IDS會短視近利, 要最小化「獲得每單位資訊的成本」.  用法 K-armed對抗強調是minimax最優:\n Lattimore與Szepesvari[21]: 證明k-armed adversarial bandits is minimax optimal 應用1: graph feedback 應用2: linear bandits on $l_p$-balls.  後記 今天大概看一看相關的文獻, 之後在陸續累積insights.天天向上, 共勉之！\n2022.01.29. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-30","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur032-%E6%80%9D%E8%80%83information-ratio/","series":["每日文章"],"tags":[],"title":"MUR032 思考Information Ratio"},{"categories":["DataCamp"],"content":"DataFrame操作基礎  紫式晦澀每日一篇文章第31天\n 前言   今天是2022年第29天, 全年第5週, 一月的第五個週六. 今天來爬梳DataFrames的操作基礎.\n  今天的素材主要來自DataCamp課程:Data Manipulation with pandas . 內容有四大章節, 可以蒸餾成心法, 技法, 用法.\n   The Pandas DataFrame: Working With Data Efficiently   課程概述:\n Pandas: 最受歡迎, 可作「資料操作(Data manipulation)」與「資料分析(Data analysis)」. 操作(Manipulation): extract, filter, transform 核心資料課學概念 統計工作流: import, clean, calculate statistics, create visualizations.  A. 轉換DataFrames (Transforming DataFrames) 基礎pandas: 有4大任務\n   調查DataFrames (Inspect)    排序列 (Sorting rows)    子集 (Subsetting)    增加新欄位 (Adding new columns)    心法：找到滿足特定條件的子資料, 過濾排序產生洞察 找出滿足特定條件的資料:\n 當我們有很大的資料庫, 就會需要去搜尋特定條件的子資料集來研究. 接著我們會篩選出這些資料集, 根據我們感興趣的某些量值, 來做排序, 找最佳表現的. 最佳表現的某個程度上會降低我們的風險, 往目標邁進.  技法: 排序sort_values(), 過濾isin() 物件導向:\n 方法: head(), info(), describe(), sort_values(), isin() 屬性: .shape, .values, .columns, .index, [] 資料結構: Index  用法: 人定指標, 過濾排序 工作流:\n 定義指標 根據指標過濾資料 排序過濾後的資料 印出排序過濾資料的特定欄位  例子:\n 定義贏過大盤. 根據「贏過大盤」過濾股票. 排序「贏過大盤的股票」, 以報酬率. 印出「報酬率由高到低的股票」的詳細資訊.  語法: 8小單元 a. 查DataFrame. Inspecting a DataFrame:\n   .head()方法: 回傳前幾列. returns the first few rows (the “head” of the DataFrame).    .info()方法: 每個欄位(column)的資訊, 包含「資料類型 (Data type)」與「遺失值數量 (number of missing values)」 shows information on each of the columns, such as the data type and number of missing values.    .shape屬性: 回傳DataFrame的維度.returns the number of rows and columns of the DataFrame.    .describe()方法 : 計算每一列的總結統計量calculates a few summary statistics for each column.    b. DataFrame的組成. Parts of a DataFrame:\n   .values屬性: 二維度陣列值. A two-dimensional NumPy array of values.    .columns屬性: 行的索引, 行的名字. An index of columns: the column names.    .index屬性: 列的索引, 列的名字. An index for the rows: either row numbers or row names.     邏輯在pands的Index資料結構上\nYou can usually think of indexes as a list of strings or numbers, though the pandas Index data type allows for more sophisticated options.\n  c. 根據column排序橫列. Sorting rows:\n sort_values()方法: 根據row名稱作排序(遞增或遞減) 根據一row排序 df.sort_values(\u0026quot;breed\u0026quot;) 根據二row排序df.sort_values([\u0026quot;breed\u0026quot;, \u0026quot;weight_kg\u0026quot;])  d. 選column. Subsetting columns.:\n 中括號 []可以來選columns. 選一個column. df[\u0026quot;col_a\u0026quot;] 選二個columns. df[[\u0026quot;col_a\u0026quot;, \u0026quot;col_b\u0026quot;]]  e. 過濾選擇rows. Filtering/Selecting Rows:\n 找到資料集有趣的部分 找「滿足特定條件的rows」 找高度超過60公分的狗: dogs[dogs[\u0026quot;height_cm\u0026quot;] \u0026gt; 60] 找特定顏色的狗: dogs[dogs[\u0026quot;color\u0026quot;] == \u0026quot;tan\u0026quot;] 找特定顏色且高度超過60公分的狗: dogs[(dogs[\u0026quot;height_cm\u0026quot;] \u0026gt; 60) \u0026amp; (dogs[\u0026quot;color\u0026quot;] == \u0026quot;tan\u0026quot;)]   如何找某些股票?\n f. 利用類別變數過濾rows. Subsetting rows by categorical variables:\n 使用or算子 |來選擇多個類別, 不是很容易操作多個類別. 使用.isin()方法: 寫條件來分類 例子: colors定義出要找的三種顏色; condition把符合這些顏色的狗的indicator收集出來. dogs[condition]就留下了需要的那些狗.  1colors = [\u0026#34;brown\u0026#34;, \u0026#34;black\u0026#34;, \u0026#34;tan\u0026#34;] 2condition = dogs[\u0026#34;color\u0026#34;].isin(colors) 3dogs[condition]  target_stock = [需要的股票號碼]\ncondition = data[\u0026ldquo;number\u0026rdquo;].isin(target_stock)\ndata[condition]就把需要的股票列了出來.\n  g. 增加新欄位 (Adding new columns)三招:\n   轉換 Transforming.    改變 Mutating.    特徵工程 Feature Engineering.     如何判斷是否比大盤好?\n h. 組合技(Combo-attack!):\n 資料操弄四招: a. row排序, b. column選取, c. rows過濾, d. column擴增  1# 造欄位: Create indiv_per_10k col as homeless individuals per 10k state pop 2homelessness[\u0026#34;indiv_per_10k\u0026#34;] = 10000 * homelessness[\u0026#34;individuals\u0026#34;] / homelessness[\u0026#34;state_pop\u0026#34;] 3 4# 過濾rows: Subset rows for indiv_per_10k greater than 20 5high_homelessness = homelessness[homelessness[\u0026#34;indiv_per_10k\u0026#34;] \u0026gt; 20] 6 7# 排序rows: Sort high_homelessness by descending indiv_per_10k 8high_homelessness_srt = high_homelessness.sort_values(\u0026#34;indiv_per_10k\u0026#34;, ascending=False) 9 10# 選取rows: From high_homelessness_srt, select the state and indiv_per_10k cols 11result = high_homelessness_srt[[\u0026#34;state\u0026#34;, \u0026#34;indiv_per_10k\u0026#34;]] 12 13# See the result 14print(result) B. 聚合DataFrames (Aggregating DataFrames) 統計Pandas: 有3大任務\n   計算行總結統計量 (Summary statistics on DataFrame columns)    群化總結統計量 (Grouped summary statistics)    樞紐分析（Pivot table)    樞紐分析（pivot table)\n 用來匯總其它表的數據。 首先把源表分組（grouping），然後對各組內數據做匯總操作如排序、平均、累加、計數或字符串連接等。 透視表用於數據處理，在數據可視化程序如電子表格或商業智能軟件中常見。 這種「旋轉」或者pivoting匯總表的概念得以命名。 靜態版本: 列連表(Contingency Table)  心法: 總結統計量, 操作表格, 樞紐分析 總結統計量: 既然是表格, 我們常常需要往各種狀況去做總結, 算一些指標來做. 操作表格: 排序, 丟掉重複的, 計算數量, 計算比例 群總結, 樞紐分析: 分小群, 做函數操作\n技法: 客製化函數df['column'].agg(function), 樞紐分析pivot_table() 統計函數: .mean(), .median(), .max(), .min(), .cumsum(), .cummax(). 客製化函數: .agg(my_function); 執行上df['column'].agg(function). 操作表格: sort_values(), drop_duplicates(), subset 排序, 計算比例: .value_counts(sort = True, normalize = True). 群總結, 樞紐分析: .groupby(), agg(), pivot_table().\n用法: 分小群, 做函數操作, 樞紐分析 分小群: 不同的時間點, 不同的報酬率 做函數操作: 與大盤比較, 損益率下降, 上頂, 下頂. 樞紐分析: 滿足特定條件的股票收益率.\n語法: 11個小單元 a. 平均與中位數 (Mean and Median):\n 可以直接對DataFrame操作統計方法 也可以先得到Series數據結構. .mean()方法: 得到平均. .median()方法: 得到中位數.  1# Print the head of the sales DataFrame 2print(sales.head()) 3 4 5# Print the info about the sales DataFrame 6print(sales.info()) 7 8# Print the mean of weekly_sales 9print(sales[\u0026#34;weekly_sales\u0026#34;].mean()) 10 11# Print the median of weekly_sales 12print(sales[\u0026#34;weekly_sales\u0026#34;].median()) 13 14# Pandas的Series 15In [1]: 16type(sales[\u0026#34;weekly_sales\u0026#34;]) 17Out[1]: 18pandas.core.series.Series 19 20# Pandas的DataFrame 21In [2]: 22sales.median() 23Out[2]: 24 25store 13.000 26department 40.000 27weekly_sales 12049.065 28is_holiday 0.000 29temperature_c 16.967 30fuel_price_usd_per_l 0.743 31unemployment 8.099 32dtype: float64 33 34# 整個表格是DataFrame 35In [4]: 36type(sales) 37Out[4]: 38pandas.core.frame.DataFrame b.總結日期 Summarizing Dates:\n 資料結構: datetime64 .max():找到最近日期 .min():找到最遠日期  1# Print the maximum of the date column 2print(sales[\u0026#34;date\u0026#34;].max()) 3 42012-10-26 00:00:00 5 6# Print the minimum of the date column 7print(sales[\u0026#34;date\u0026#34;].min()) 8 92010-02-05 00:00:00 10 11In [1]: 12type(sales[\u0026#34;date\u0026#34;]) 13Out[1]: 14pandas.core.series.Series 15In [2]: 16type(sales[\u0026#34;date\u0026#34;][1]) 17Out[2]: 18pandas._libs.tslibs.timestamps.Timestamp c.敏捷總結 (Efficient Summaries):\n .agg()方法: 客製化DataFrame函數, 可以對每個column執行函數 語法: df['column'].agg(function)  1# 客製化函數 A custom IQR function 2import numpy as np 3def iqr(column): 4 return column.quantile(0.75) - column.quantile(0.25) 5 6# 執行客製化函數到某一欄位 Print IQR of the temperature_c column 7print(sales[\u0026#34;temperature_c\u0026#34;].agg(iqr)) 8 9# 執行函數到其他欄位 Update to print IQR of temperature_c, fuel_price_usd_per_l, \u0026amp; unemployment 10print(sales[[\u0026#34;temperature_c\u0026#34;, \u0026#34;fuel_price_usd_per_l\u0026#34;, \u0026#34;unemployment\u0026#34;]].agg(iqr)) 11 12# 執行兩種函數到三個欄位 Update to print IQR and median of temperature_c, fuel_price_usd_per_l, \u0026amp; unemployment 13print(sales[[\u0026#34;temperature_c\u0026#34;, \u0026#34;fuel_price_usd_per_l\u0026#34;, \u0026#34;unemployment\u0026#34;]].agg([iqr, np.median]))  結果  1\u0026lt;script.py\u0026gt; output: 2 temperature_c fuel_price_usd_per_l unemployment 3 iqr 16.583 0.073 0.565 4 median 16.967 0.743 8.099  如何對各種股票, 來做一些總結統計量?\n d. 累積統計量(Cumulative statistics):\n 統計量: 累積合(Cumulative Sun), 累計最大(Cumulative Max) sort_values(): 排序rows .cumsum(): 得到累計合 .cummax(): 得到累積最大  1# 排序時間 Sort sales_1_1 by date 2sales_1_1 = sales_1_1.sort_values(\u0026#34;date\u0026#34;) 3 4# 製造新欄位:累計合 Get the cumulative sum of weekly_sales, add as cum_weekly_sales col 5sales_1_1[\u0026#34;cum_weekly_sales\u0026#34;] = sales_1_1[\u0026#34;weekly_sales\u0026#34;].cumsum() 6 7# 製造新欄位:累計最大 Get the cumulative max of weekly_sales, add as cum_max_sales col 8sales_1_1[\u0026#34;cum_max_sales\u0026#34;] = sales_1_1[\u0026#34;weekly_sales\u0026#34;].cummax() 9 10# 印出製造的新欄位 See the columns you calculated 11print(sales_1_1[[\u0026#34;date\u0026#34;, \u0026#34;weekly_sales\u0026#34;, \u0026#34;cum_weekly_sales\u0026#34;, \u0026#34;cum_max_sales\u0026#34;]])  可以造累計獲利, 累計報酬率.\n e. 丟掉複製的(Dropping duplicates):\n 把重複的資料刪掉 .drop_duplicates() subset: 紀錄欄位名字的list.  1# 丟掉重複的 Drop duplicate store/type combinations 2store_types = sales.drop_duplicates(subset=[\u0026#34;store\u0026#34;, \u0026#34;type\u0026#34;]) 3print(store_types.head()) 4 5# 丟掉重複的 Drop duplicate store/department combinations 6store_depts = sales.drop_duplicates(subset=[\u0026#34;store\u0026#34;, \u0026#34;department\u0026#34;]) 7print(store_depts.head()) 8 9# 找特定的rows, 丟掉重複的 Subset the rows where is_holiday is True and drop duplicate dates 10holiday_dates = sales[sales[\u0026#34;is_holiday\u0026#34;]].drop_duplicates(subset=\u0026#34;date\u0026#34;) 11 12# Print date col of holiday_dates 13print(holiday_dates[\u0026#34;date\u0026#34;])  有什麼應用?\n f. 計數類別變數 (Counting categorical variables):\n .value_counts(): 計算數量. .value_counts(normalize= True): 計算比例. .value_counts(sort = True, normalize = True): 排序, 計算比例.  1# 計算每一個種類的個數 Count the number of stores of each type 2store_counts = store_types[\u0026#34;type\u0026#34;].value_counts() 3print(store_counts) 4 5# 計算各種的比例. Get the proportion of stores of each type 6store_props = store_types[\u0026#34;type\u0026#34;].value_counts(normalize=True) 7print(store_props) 8 9# 計算部門的數量 Count the number of each department number and sort 10dept_counts_sorted = store_depts[\u0026#34;department\u0026#34;].value_counts(sort=True) 11print(dept_counts_sorted) 12 13# 計算部門的比例 Get the proportion of departments of each number and sort 14dept_props_sorted = store_depts[\u0026#34;department\u0026#34;].value_counts(sort=True, normalize=True) 15print(dept_props_sorted) g. 各種類型商店的發生比例:暴力解(What percent of sales occurred at each store type?):\n .groupby():根據群去計算總結統計 計算總數, 計算個別數. 接著算比例.  1# Calc total weekly sales 2sales_all = sales[\u0026#34;weekly_sales\u0026#34;].sum() 3 4# Subset for type A stores, calc total weekly sales 5sales_A = sales[sales[\u0026#34;type\u0026#34;] == \u0026#34;A\u0026#34;][\u0026#34;weekly_sales\u0026#34;].sum() 6 7# Subset for type B stores, calc total weekly sales 8sales_B = sales[sales[\u0026#34;type\u0026#34;] == \u0026#34;B\u0026#34;][\u0026#34;weekly_sales\u0026#34;].sum() 9 10# Subset for type C stores, calc total weekly sales 11sales_C = sales[sales[\u0026#34;type\u0026#34;] == \u0026#34;C\u0026#34;][\u0026#34;weekly_sales\u0026#34;].sum() 12 13# Get proportion for each type 14sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all 15print(sales_propn_by_type) h. 優雅解:使用.groupby():\n .groupby():分群函數 sales.groupby(\u0026quot;type\u0026quot;): 是DataFrameGroupBy資料類型 sales.groupby(\u0026quot;type\u0026quot;)[\u0026quot;weekly_sales\u0026quot;].sum():是Series資料類型 sales_by_type_is_holiday = sales.groupby([\u0026quot;type\u0026quot;, \u0026quot;is_holiday\u0026quot;])[\u0026quot;weekly_sales\u0026quot;].sum()可以做四種子族的和  1# Group by type; calc total weekly sales 2sales_by_type = sales.groupby(\u0026#34;type\u0026#34;)[\u0026#34;weekly_sales\u0026#34;].sum() 3 4# Get proportion for each type 5sales_propn_by_type = sales_by_type / sum(sales_by_type) 6print(sales_propn_by_type) 7 8# Group by type and is_holiday; calc total weekly sales 9sales_by_type_is_holiday = sales.groupby([\u0026#34;type\u0026#34;, \u0026#34;is_holiday\u0026#34;])[\u0026#34;weekly_sales\u0026#34;].sum() 10print(sales_by_type_is_holiday) 11 12type is_holiday 13A False 2.337e+08 14 True 2.360e+04 15B False 2.318e+07 16 True 1.621e+03 17Name: weekly_sales, dtype: float64  可以選不同的控制變量, 剩下積分掉！\n i. 多重群統計量(Multiple grouped summaries):\n .agg(): 計算多個變數的多個統計量. np.min, np.max, np.mean, np.mean: 都是Numpy上方便用的統計量函數. sales.groupby(\u0026quot;type\u0026quot;)[\u0026quot;weekly_sales\u0026quot;].agg([np.min, np.max, np.mean, np.median]): 根據每週銷量分類(A,B兩類), 計算其最小最大平均中位數. unemp_fuel_stats = sales.groupby(\u0026quot;type\u0026quot;)[[\u0026quot;unemployment\u0026quot;, \u0026quot;fuel_price_usd_per_l\u0026quot;]].agg([np.min, np.max, np.mean, np.median]) 根據兩變數分類, 計算其最小最大平均中位數.  1# Import numpy with the alias np 2import numpy as np 3 4# 輸出分位數: For each store type, aggregate weekly_sales: get min, max, mean, and median 5sales_stats = sales.groupby(\u0026#34;type\u0026#34;)[\u0026#34;weekly_sales\u0026#34;].agg([np.min, np.max, np.mean, np.median]) 6 7# Print sales_stats 8print(sales_stats) 9 10 amin amax mean median 11type 12A -1098.0 293966.05 23674.667 11943.92 13B -798.0 232558.51 25696.678 13336.08 14 15# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median 16unemp_fuel_stats = sales.groupby(\u0026#34;type\u0026#34;)[[\u0026#34;unemployment\u0026#34;, \u0026#34;fuel_price_usd_per_l\u0026#34;]].agg([np.min, np.max, np.mean, np.median]) 17 18# Print unemp_fuel_stats 19print(unemp_fuel_stats)  如何定義出有意義的分類, 接著使用分類的統計量呢? 如何將股票分類? 分好的類要看什麼的統計量分佈呢?\n  如果要match 時間前幾位, 是否要用regular expression?\n j. 單變數樞紐(Pivoting on one variable):\n 樞紐表(Privot tables): spreadsheets聚合資料的標準方法 .pivot_table(): 其實就是替代了.groupby().  1# 對各種店舖類型, 樞紐每週銷量 Pivot for mean weekly_sales for each store type 2mean_sales_by_type = sales.pivot_table(values=\u0026#34;weekly_sales\u0026#34;, index=\u0026#34;type\u0026#34;) 3 4# Print mean_sales_by_type 5print(mean_sales_by_type) 6 7 weekly_sales 8type 9A 23674.667 10B 25696.678  在.pivot_table()可以用aggfunc形式參數, 來給「多個函數」來總結values.  1# Import NumPy as np 2import numpy as np 3 4# Pivot for mean and median weekly_sales for each store type 5mean_med_sales_by_type = sales.pivot_table(values=\u0026#34;weekly_sales\u0026#34;, index=\u0026#34;type\u0026#34;, aggfunc=[np.mean, np.median]) 6 7# Print mean_med_sales_by_type 8print(mean_med_sales_by_type) 9 10 mean median 11 weekly_sales weekly_sales 12type 13A 23674.667 11943.92 14B 25696.678 13336.08  兩個維度的樞紐表: 根據「類型」與「是否是假日」來找到「每週銷量」的樞紐表 第一維度用index, 第二維度用columns.  1# 根據「類型」與「是否是假日」來找到「每週銷量」的樞紐表 Pivot for mean weekly_sales by store type and holiday  2mean_sales_by_type_holiday = sales.pivot_table(values=\u0026#34;weekly_sales\u0026#34;, index=\u0026#34;type\u0026#34;, columns=\u0026#34;is_holiday\u0026#34;) 3 4# Print mean_sales_by_type_holiday 5print(mean_sales_by_type_holiday) 6 7 is_holiday False True 8 type 9 A 23768.584 590.045 10 B 25751.981 810.705  那要怎麼組裝更多呢?\n 1import numpy as np 2 3new = sales.pivot_table(values=\u0026#34;weekly_sales\u0026#34;, index=\u0026#34;type\u0026#34;, columns=\u0026#34;is_holiday\u0026#34;, aggfunc=[np.mean, np.median]) 4 5print(new) 6 mean median 7is_holiday False True False True 8type 9A 23768.584 590.045 12028.955 37.500 10B 25751.981 810.705 13348.680 810.705  組合起來非常方便, 可以得到各種樞紐分析！\n k. 利用樞紐表來填補缺失值與和值(Fill in missing values and sum values with pivot tables):\n 繼續使用.pivot_table()方法的形式參數: fill_value與margins. fill_value可以補遺失值(Imputation). Dealing with Missing Data in Python 可以詳細學. margins:用兩個變數做樞紐 fill_value=0: 把missing value都補成0.  1# Print mean weekly_sales by department and type; fill missing values with 0 2print(sales.pivot_table(values=\u0026#34;weekly_sales\u0026#34;, index=\u0026#34;department\u0026#34;, columns=\u0026#34;type\u0026#34;, fill_value=0)) 3 4type A B 5department 61 30961.725 44050.627 72 67600.159 112958.527 83 17160.003 30580.655 94 44285.399 51219.654 105 34821.011 63236.875 11... ... ... 1295 123933.787 77082.102 1396 21367.043 9528.538 1497 28471.267 5828.873 1598 12875.423 217.428 1699 379.124 0.000  margins=True: 多了一個群體的mean, 以及各部門的mean.  1# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols 2print(sales.pivot_table(values=\u0026#34;weekly_sales\u0026#34;, index=\u0026#34;department\u0026#34;, columns=\u0026#34;type\u0026#34;, fill_value=0, margins=True)) 3 4type A B All 5department 61 30961.725 44050.627 32052.467 72 67600.159 112958.527 71380.023 83 17160.003 30580.655 18278.391 94 44285.399 51219.654 44863.254 105 34821.011 63236.875 37189.000 11... ... ... ... 1296 21367.043 9528.538 20337.608 1397 28471.267 5828.873 26584.401 1498 12875.423 217.428 11820.590 1599 379.124 0.000 379.124 16All 23674.667 25696.678 23843.950 17 18[81 rows x 3 columns]  加入aggfunc, 兩張大表就都有了  1print(sales.pivot_table(values=\u0026#34;weekly_sales\u0026#34;, index=\u0026#34;department\u0026#34;, columns=\u0026#34;type\u0026#34;, fill_value=0, margins=True, aggfunc=[np.mean, np.median])) 2 mean median 3type A B All A B All 4department 51 30961.725 44050.627 32052.467 24743.070 31986.360 25478.905 62 67600.159 112958.527 71380.023 68614.770 112812.985 70001.020 73 17160.003 30580.655 18278.391 13396.805 23145.625 13788.100 84 44285.399 51219.654 44863.254 42639.470 51485.930 44011.535 95 34821.011 63236.875 37189.000 30299.045 60400.660 30943.785 10... ... ... ... ... ... ... 1196 21367.043 9528.538 20337.608 25187.875 9503.140 23935.495 1297 28471.267 5828.873 26584.401 27016.580 5856.705 26354.365 1398 12875.423 217.428 11820.590 12636.275 34.100 11943.840 1499 379.124 0.000 379.124 167.000 0.000 167.000 15All 23674.667 25696.678 23843.950 11943.920 13336.080 12049.065 16 17[81 rows x 6 columns] C. 切與標DataFrames (Slicing and Indexing DataFrames) 統計Pandas: 2大太極\n 標(Indexes): 給行與列名分. 切(Slicing): 過濾過濾過濾.  心法 ****: ****: ****:\n技法 ****: ****: ****:\n用法 ****: ****: ****:\n語法 a.設定與移除索引(Setting and removing indexes):\n pandas可以指定columns為「索引(index)」 .ser_index(): 得到column的name .reset_index(): 重設索引; Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels.  1# Look at temperatures 2print(temperatures) 3 4# 把city當作新的索引 Index temperatures by city 5temperatures_ind = temperatures.set_index(\u0026#34;city\u0026#34;) 6 7# Look at temperatures_ind 8print(temperatures_ind) 9 10每個column都是城市的名字 11print(temperatures_ind) 12 date country avg_temp_c 13city 14Abidjan 2000-01-01 Côte D\u0026#39;Ivoire 27.293 15Abidjan 2000-02-01 Côte D\u0026#39;Ivoire 27.685 16Abidjan 2000-03-01 Côte D\u0026#39;Ivoire 29.061 17Abidjan 2000-04-01 Côte D\u0026#39;Ivoire 28.162 18Abidjan 2000-05-01 Côte D\u0026#39;Ivoire 27.547 19... ... ... ... 20Xian 2013-05-01 China 18.979 21Xian 2013-06-01 China 23.522 22Xian 2013-07-01 China 25.251 23Xian 2013-08-01 China 24.528 24Xian 2013-09-01 China NaN 25 26[16500 rows x 3 columns] 27 28# 重設索引, 讓欄位名字讓出來變數字Reset the index, keeping its contents 29print(temperatures_ind.reset_index()) 30 31 city date country avg_temp_c 320 Abidjan 2000-01-01 Côte D\u0026#39;Ivoire 27.293 331 Abidjan 2000-02-01 Côte D\u0026#39;Ivoire 27.685 342 Abidjan 2000-03-01 Côte D\u0026#39;Ivoire 29.061 353 Abidjan 2000-04-01 Côte D\u0026#39;Ivoire 28.162 364 Abidjan 2000-05-01 Côte D\u0026#39;Ivoire 27.547 37... ... ... ... ... 3816495 Xian 2013-05-01 China 18.979 3916496 Xian 2013-06-01 China 23.522 4016497 Xian 2013-07-01 China 25.251 4116498 Xian 2013-08-01 China 24.528 4216499 Xian 2013-09-01 China NaN 43 44[16500 rows x 4 columns] 45 46# 丟掉城市索引 Reset the index, dropping its contents 47print(temperatures_ind.reset_index(drop=True)) 48 49 date country avg_temp_c 500 2000-01-01 Côte D\u0026#39;Ivoire 27.293 511 2000-02-01 Côte D\u0026#39;Ivoire 27.685 522 2000-03-01 Côte D\u0026#39;Ivoire 29.061 533 2000-04-01 Côte D\u0026#39;Ivoire 28.162 544 2000-05-01 Côte D\u0026#39;Ivoire 27.547 55... ... ... ... 5616495 2013-05-01 China 18.979 5716496 2013-06-01 China 23.522 5816497 2013-07-01 China 25.251 5916498 2013-08-01 China 24.528 6016499 2013-09-01 China NaN 61 62[16500 rows x 3 columns] 63 b.用.loc[]取子集(Subsetting with .loc[]):\n .loc[]:索引(Indexes)的殺手功能. 舊有方法: df[df[\u0026quot;column\u0026quot;].isin(values)] 新方法: df_ind.loc[values] 想找的城市-莫斯科\u0026amp;聖彼得堡: cities = [\u0026quot;Moscow\u0026quot;, \u0026quot;Saint Petersburg\u0026quot;]. 用之前.isin()的方法: print(temperatures[temperatures[\u0026quot;city\u0026quot;].isin(cities)]). 現在更簡潔, 用.loc[]方法: print(temperatures_ind.loc[cities]).  1# Make a list of cities to subset on 2cities = [\u0026#34;Moscow\u0026#34;, \u0026#34;Saint Petersburg\u0026#34;] 3 4# 單純的取子資料- Subset temperatures using square brackets 5print(temperatures[temperatures[\u0026#34;city\u0026#34;].isin(cities)]) 6 7 date city country avg_temp_c 810725 2000-01-01 Moscow Russia -7.313 910726 2000-02-01 Moscow Russia -3.551 1010727 2000-03-01 Moscow Russia -1.661 1110728 2000-04-01 Moscow Russia 10.096 1210729 2000-05-01 Moscow Russia 10.357 13... ... ... ... ... 1413360 2013-05-01 Saint Petersburg Russia 12.355 1513361 2013-06-01 Saint Petersburg Russia 17.185 1613362 2013-07-01 Saint Petersburg Russia 17.234 1713363 2013-08-01 Saint Petersburg Russia 17.153 1813364 2013-09-01 Saint Petersburg Russia NaN 19 20 21# 以選取的索引擺前面 Subset temperatures_ind using .loc[] 22print(temperatures_ind.loc[cities]) 23 date country avg_temp_c 24city 25Moscow 2000-01-01 Russia -7.313 26Moscow 2000-02-01 Russia -3.551 27Moscow 2000-03-01 Russia -1.661 28Moscow 2000-04-01 Russia 10.096 29Moscow 2000-05-01 Russia 10.357 30... ... ... ... 31Saint Petersburg 2013-05-01 Russia 12.355 32Saint Petersburg 2013-06-01 Russia 17.185 33Saint Petersburg 2013-07-01 Russia 17.234 34Saint Petersburg 2013-08-01 Russia 17.153 35Saint Petersburg 2013-09-01 Russia NaN  可能要先把需要的column整理到前面?\n c.設定多等級索引(Setting multi-level indexes):\n 多欄位索引:索引可以來自很多行. Indexes can also be made out of multiple columns, forming a multi-level index (sometimes called a hierarchical index). 好處: 「巢狀類別變數(Nested Categorical Variables)」比較自然. 壞處: 操作多等級索引會比操作columns還要複雜一點. .set_index(): 可以指定兩個column names當作list [(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;), (\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)]: 節選的subset得用tuple資料型態來寫. .loc[]:帶入需要的subset.  1# 指定索引 : Index temperatures by country \u0026amp; city 2temperatures_ind = temperatures.set_index([\u0026#34;country\u0026#34;, \u0026#34;city\u0026#34;]) 3 4In [1]: 5type(temperatures_ind) 6Out[1]: 7pandas.core.frame.DataFrame 8\u0026gt; 得到DataFrame 9 10In [2]: 11temperatures_ind.head() 12Out[2]: 13 14 date avg_temp_c 15country city 16Côte D\u0026#39;Ivoire Abidjan 2000-01-01 27.293 17 Abidjan 2000-02-01 27.685 18 Abidjan 2000-03-01 29.061 19 Abidjan 2000-04-01 28.162 20 Abidjan 2000-05-01 27.547 21 22 23# List of tuples: Brazil, Rio De Janeiro \u0026amp; Pakistan, Lahore 24rows_to_keep = [(\u0026#34;Brazil\u0026#34;, \u0026#34;Rio De Janeiro\u0026#34;), (\u0026#34;Pakistan\u0026#34;, \u0026#34;Lahore\u0026#34;)] 25 26# Subset for rows to keep 27print(temperatures_ind.loc[rows_to_keep]) 28 29\u0026gt; 用巢狀列表排好, 只留下兩個城市 30print(temperatures_ind.loc[rows_to_keep]) 31 date avg_temp_c 32country city 33Brazil Rio De Janeiro 2000-01-01 25.974 34 Rio De Janeiro 2000-02-01 26.699 35 Rio De Janeiro 2000-03-01 26.270 36 Rio De Janeiro 2000-04-01 25.750 37 Rio De Janeiro 2000-05-01 24.356 38... ... ... 39Pakistan Lahore 2013-05-01 33.457 40 Lahore 2013-06-01 34.456 41 Lahore 2013-07-01 33.279 42 Lahore 2013-08-01 31.511 43 Lahore 2013-09-01 NaN 44 45[330 rows x 2 columns]  如何設計適當的「巢狀索引」來過濾出需要的股票收益率?\n d.利用索引值排序(Sorting by index values):\n 之前使用.sort_values()來改變rows的順序 現在介紹.sort_index()來根據索引排序.  1# 照「國家」名稱ABCD排序: Sort temperatures_ind by index values 2print(temperatures_ind.sort_index()) 3 4 5 date avg_temp_c 6country city 7Afghanistan Kabul 2000-01-01 3.326 8 Kabul 2000-02-01 3.454 9 Kabul 2000-03-01 9.612 10 Kabul 2000-04-01 17.925 11 Kabul 2000-05-01 24.658 12... ... ... 13Zimbabwe Harare 2013-05-01 18.298 14 Harare 2013-06-01 17.020 15 Harare 2013-07-01 16.299 16 Harare 2013-08-01 19.232 17 Harare 2013-09-01 NaN 18 19# 照「城市」名稱ABCD排序: Sort temperatures_ind by index values at the city level 20print(temperatures_ind.sort_index(level=\u0026#34;city\u0026#34;)) 21 22 date avg_temp_c 23country city 24Côte D\u0026#39;Ivoire Abidjan 2000-01-01 27.293 25 Abidjan 2000-02-01 27.685 26 Abidjan 2000-03-01 29.061 27 Abidjan 2000-04-01 28.162 28 Abidjan 2000-05-01 27.547 29... ... ... 30China Xian 2013-05-01 18.979 31 Xian 2013-06-01 23.522 32 Xian 2013-07-01 25.251 33 Xian 2013-08-01 24.528 34 Xian 2013-09-01 NaN 35 36[16500 rows x 2 columns] 37 38# 「國家」名稱ABCD排序;「城市」名稱ZYXW排序. Sort temperatures_ind by country then descending city 39print(temperatures_ind.sort_index(level=[\u0026#34;country\u0026#34;, \u0026#34;city\u0026#34;], ascending = [True, False])) 40 41 date avg_temp_c 42country city 43Afghanistan Kabul 2000-01-01 3.326 44 Kabul 2000-02-01 3.454 45 Kabul 2000-03-01 9.612 46 Kabul 2000-04-01 17.925 47 Kabul 2000-05-01 24.658 48... ... ... 49Zimbabwe Harare 2013-05-01 18.298 50 Harare 2013-06-01 17.020 51 Harare 2013-07-01 16.299 52 Harare 2013-08-01 19.232 53 Harare 2013-09-01 NaN 54 55[16500 rows x 2 columns]  股票編號排序, 時間排序.\n e.切片索引值(Slicing index values):\n 選取範圍: first:last只要在可排序的東西上都可以切片. 切片之前要用.sort_index()來排序 切片公式: df.loc[\u0026quot;a\u0026quot;:\u0026quot;b\u0026quot;]. 切片公式進階: df.loc[(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;):(\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)]  1# Sort the index of temperatures_ind 2temperatures_srt = temperatures_ind.sort_index() 3 4# 篩選國家 Subset rows from Pakistan to Russia 5print(temperatures_srt.loc[\u0026#34;Pakistan\u0026#34;:\u0026#34;Russia\u0026#34;]) 6 7# 篩選城市 Try to subset rows from Lahore to Moscow 8print(temperatures_srt.loc[\u0026#34;Lahore\u0026#34;:\u0026#34;Moscow\u0026#34;]) 9 10# 篩選國家城市 Subset rows from Pakistan, Lahore to Russia, Moscow 11print(temperatures_srt.loc[(\u0026#34;Pakistan\u0026#34;, \u0026#34;Lahore\u0026#34;):(\u0026#34;Russia\u0026#34;, \u0026#34;Moscow\u0026#34;)]) f.雙方向切片(Slicing in both directions):\n 列切片(Row slicing):.loc[]可以先subset特定國家城市; df.loc[(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;):(\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)] 行切片(Column slicing):.loc[]再一次切片「時間」與「溫度; df.loc[:, \u0026quot;e\u0026quot;:\u0026quot;f\u0026quot;] 雙切片(Row\u0026amp;Column slicing): df.loc[(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;):(\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), \u0026quot;e\u0026quot;:\u0026quot;f\u0026quot;]  1# Subset rows from India, Hyderabad to Iraq, Baghdad 2print(temperatures_srt.loc[(\u0026#34;India\u0026#34;, \u0026#34;Hyderabad\u0026#34;):(\u0026#34;Iraq\u0026#34;, \u0026#34;Baghdad\u0026#34;)]) 3 4# Subset columns from date to avg_temp_c 5print(temperatures_srt.loc[:, \u0026#34;date\u0026#34;:\u0026#34;avg_temp_c\u0026#34;]) 6 7# Subset in both directions at once 8print(temperatures_srt.loc[(\u0026#34;India\u0026#34;, \u0026#34;Hyderabad\u0026#34;):(\u0026#34;Iraq\u0026#34;, \u0026#34;Baghdad\u0026#34;), \u0026#34;date\u0026#34;:\u0026#34;avg_temp_c\u0026#34;]) g. 切片時間序列(Slicing time series):\n 利用「時間範圍(Date range)」來過濾資料 增加date索引, 使用.loc[]來取子集. ISO 8601格式: yyyy-mm-dd 利用兩個布林條件來過濾: df[(condition1) \u0026amp; (condition2)]  1# 制定日期範圍的bool變數 Use Boolean conditions to subset temperatures for rows in 2010 and 2011 2temperatures_bool = temperatures[(temperatures[\u0026#34;date\u0026#34;] \u0026gt;= \u0026#34;2010-01-01\u0026#34;) \u0026amp; (temperatures[\u0026#34;date\u0026#34;] \u0026lt;= \u0026#34;2011-12-31\u0026#34;)] 3print(temperatures_bool) 4 5# 設定日期為索引 Set date as the index and sort the index 6temperatures_ind = temperatures.set_index(\u0026#34;date\u0026#34;).sort_index() 7 8# 選日期區間 Use .loc[] to subset temperatures_ind for rows in 2010 and 2011 9print(temperatures_ind.loc[\u0026#34;2010\u0026#34;:\u0026#34;2011\u0026#34;]) 10 11# 選日期區間 Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011 12print(temperatures_ind.loc[\u0026#34;2010-08\u0026#34;:\u0026#34;2011-02\u0026#34;]) h.利用row/column數字取子集:\n 取具體數字: iloc[]. Row/column numbers start at zero. df.iloc[a:b] subsets only rows. Use : without first/last values to select all rows (or columns).  1# Get 23rd row, 2nd column (index 22, 1) 2print(temperatures.iloc[22, 1]) 3 4# Use slicing to get the first 5 rows 5print(temperatures.iloc[:5]) 6 7# Use slicing to get columns 3 to 4 8print(temperatures.iloc[:, 2:4]) 9 10# Use slicing in both directions at once 11print(temperatures.iloc[:5, 2:4]) i.利用程式與年份「樞紐」溫度(Pivot temperature by city and year):\n 看不同城市不同年份的溫度變化 You can access the components of a date (year, month and day) using code of the form dataframe[\u0026quot;column\u0026quot;].dt.component. For example, the month component is dataframe[\u0026quot;column\u0026quot;].dt.month, and the year component is dataframe[\u0026quot;column\u0026quot;].dt.year. 使用.pivot_table(), 利用index設定國家與城市, columns設定年份.  1# 增加年份的欄位 Add a year column to temperatures 2temperatures[\u0026#34;year\u0026#34;] = temperatures[\u0026#34;date\u0026#34;].dt.year 3 4# 利用年份, 來以國家城市, 對溫度做樞紐 Pivot avg_temp_c by country and city vs year 5temp_by_country_city_vs_year = temperatures.pivot_table(\u0026#34;avg_temp_c\u0026#34;, index = [\u0026#34;country\u0026#34;, \u0026#34;city\u0026#34;], columns = \u0026#34;year\u0026#34;) 6 7# See the result 8print(temp_by_country_city_vs_year) 9 10 11year 2000 2001 2002 2003 2004 ... 2009 2010 2011 2012 2013 12country city ... 13Afghanistan Kabul 15.823 15.848 15.715 15.133 16.128 ... 15.093 15.676 15.812 14.510 16.206 14Angola Luanda 24.410 24.427 24.791 24.867 24.216 ... 24.325 24.440 24.151 24.240 24.554 15Australia Melbourne 14.320 14.180 14.076 13.986 13.742 ... 14.647 14.232 14.191 14.269 14.742 16 Sydney 17.567 17.854 17.734 17.592 17.870 ... 18.176 17.999 17.713 17.474 18.090 17Bangladesh Dhaka 25.905 25.931 26.095 25.927 26.136 ... 26.536 26.648 25.803 26.284 26.587 18... ... ... ... ... ... ... ... ... ... ... ... 19United States Chicago 11.090 11.703 11.532 10.482 10.943 ... 10.298 11.816 11.214 12.821 11.587 20 Los Angeles 16.643 16.466 16.430 16.945 16.553 ... 16.677 15.887 15.875 17.090 18.121 21 New York 9.969 10.931 11.252 9.836 10.389 ... 10.142 11.358 11.272 11.971 12.164 22Vietnam Ho Chi Minh City 27.589 27.832 28.065 27.828 27.687 ... 27.853 28.282 27.675 28.249 28.455 23Zimbabwe Harare 20.284 20.861 21.079 20.889 20.308 ... 20.524 21.166 20.782 20.523 19.756 24 25[100 rows x 14 columns]  「城市」對應「股票」;「年份」對應「年份」;「溫度」對應「報酬率」\n j. 對樞紐表取子集(Subsetting pivot tables):\n 樞紐表(Pivot table): 實際是DataFrame+Sorted Indexes. .loc[]+切面組合 To subset rows at the outer level, use .loc[\u0026quot;country1\u0026quot;:\u0026quot;country2\u0026quot;]. To subset rows at the outer level, use .loc[(\u0026quot;country1\u0026quot;, \u0026quot;city1\u0026quot;):(\u0026quot;country2\u0026quot;, \u0026quot;city2\u0026quot;)]. To subset in both directions, pass two arguments to .loc[].  1# 選一 Subset for Egypt to India 2temp_by_country_city_vs_year.loc[\u0026#34;Egypt\u0026#34;:\u0026#34;India\u0026#34;] 3 4# 選二 Subset for Egypt, Cairo to India, Delhi 5temp_by_country_city_vs_year.loc[(\u0026#34;Egypt\u0026#34;, \u0026#34;Cairo\u0026#34;):(\u0026#34;India\u0026#34;, \u0026#34;Delhi\u0026#34;)] 6 7# 選三 Subset in both directions at once 8temp_by_country_city_vs_year.loc[(\u0026#34;Egypt\u0026#34;, \u0026#34;Cairo\u0026#34;):(\u0026#34;India\u0026#34;, \u0026#34;Delhi\u0026#34;), \u0026#34;2005\u0026#34;:\u0026#34;2010\u0026#34;] k. 計算樞紐表(Calculating on a pivot table):\n 樞紐表都是總結統計量, 實際可能還會想做很多操作 Call .mean() without arguments to get the mean of each column. Use Boolean filtering of the form mean_temp_by_year equal to mean_temp_by_year.max(). Call .mean(), setting axis to \u0026quot;columns\u0026quot; to get the mean of each row. Use Boolean filtering of the form mean_temp_by_city equal to mean_temp_by_city.min()  1# 計算每個國家城市每一年的平均氣溫 Get the worldwide mean temp by year 2mean_temp_by_year = temp_by_country_city_vs_year.mean() 3 4# 找到最大氣溫的那年 Filter for the year that had the highest mean temp 5print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()]) 6 7year 82013 20.312 9dtype: float64 10 11# 每個城市的平均溫度 Get the mean temp by city 12mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\u0026#34;columns\u0026#34;) 13 14# 最低氣溫的國家城市 Filter for the city that had the lowest mean temp 15print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()]) 16 17country city 18China Harbin 4.877 19dtype: float64 D. 創造與可視化DataFrames (Creating and Visualizing DataFrames) 具現Pandas: 有3大技能\n 可視化DataFrames: Visualize DataFrames 處理遺失值: Handle missing data values CSV輸入輸出 (CSV files: import data from \u0026amp; export data)  心法 ****: ****: ****:\n技法 ****: ****: ****:\n用法 ****: ****: ****:\n語法 a.: b.: c.:\nd.: e.: f.: g.: h.: i.: j.: ****: ****: ****: ****: ****: ****: ****: ****:\n後記 四個單元有點太多, 先做兩個單元, 之後補上剩下兩個單元.\n220131: 投入50分鐘, 把樞紐分析相關的內容跑通, 做了總結, 完成第二模塊. 這些模塊都很扎實, 持續投入完善內容!\n2022.01.29. 紫蕊 於 西拉法葉, 印第安納, 美國.\n    Version Date Summary     0.1 2022-01-29 完成1.5個模塊   0.2 2022-01-31 完成第二的模塊   0.3 2022-02-01 花30分鐘, 學習怎麼操作「索引(Index)」來做切片(Slicing). 此功能對「特定範圍」, 「特定日期」的股票報酬率會有幫助, 像是一種更進階的樞紐操作.   0.4 2022-02-02 花30分鐘, 將第三模組的程式碼看過一趟, 之後寫總結.    ","date":"2022-01-29","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur031-dataframe%E6%93%8D%E4%BD%9C%E5%9F%BA%E7%A4%8E/","series":["每日文章"],"tags":["Python"],"title":"MUR031 DataFrames操作基礎 (Ver 0.4)"},{"categories":[],"content":"演算法交易入門  紫式晦澀每日一篇文章第30天\n 前言   今天是2022年第28天, 全年第4週, 一月的第四個週五. 今天來入門程式交易, 由於完全沒有經驗, 先以收集專有術語開始. 會講鬼話後, 才可以搞鬼.\n  今天的素材主要來自Python：股票演算法交易實務147個關鍵技巧詳解(第二版) . 感覺是對新手很容易學的材料!\n  算法交易 算法交易（Algorithmic trading):   程序化交易，或自動交易，是指運用自動化的電腦程式，或運用人工智慧，根據預設的算法，進行自動化的投資和買賣行為。 一般而言，算法交易是指人們開發程式，自動根據指定的市場的技術資料和財務比率（例如市盈率、市帳率、移動平均線）等，自動由電腦操作買賣行為。 交易演算法是將主觀交易的想法具體量化, 也就是寫成明確的規則, 並轉爲程式語言。 對於一般交易者而言, 往往無法明確提供量化的規則, 而程式撰寫者對於金融交易普遍陌生, 無法進入交易的領域。 多數的交易者使用看盤軟體, 授用制式的圖表 與統計後的數據, 對於交易所原始的報價, 往往不知該如何處理。 因此交易演算法是結合「金融交易」、「程式撰寫」與「數據分析」三大領域的新興產業, 具有較難進入的門檻。  網路擷取股票數據資源 爬蟲(網路擷取):   爬蟲能夠讓我們將網站上的資源擷取下來後進行分析, 也就是說, 任何在網際網 路上的資源都可以成為我們程式交易的資料來源。 而本書中所介紹到的網路擷取技巧, 將不限於特定的應用, 不一定要做實單交易才用網路擷取, 也可以每天固定時間進行擷取, 並產生分析結果, 作為輔助我們交易判決的工具。  技巧25: 網頁的組成結構(心法) 網頁組成結構: 爬蟲會接觸三部分:   HTML: 網頁的主體 JavaScript: 定義網頁的動態行為 CSS: 定義網頁樣式  技巧26: 網頁的標籤介紹(技法) HTML網頁例子:  1\u0026lt;html\u0026gt; 2 \u0026lt;head\u0026gt; 3 \u0026lt;title\u0026gt;Hello Jack\u0026lt;/title\u0026gt; 4 \u0026lt;/head\u0026gt; 5 \u0026lt;body\u0026gt; 6 \u0026lt;h2 class=\u0026#34;small\u0026#34;\u0026gt;test Header\u0026lt;/h2\u0026gt; 7 \u0026lt;p\u0026gt;範例網站 \u0026lt;/p\u0026gt; 8 \u0026lt;a id=\u0026#34;l1\u0026#34; href=\u0026#34;#\u0026#34;\u0026gt;第一行\u0026lt;/a\u0026gt; 9 \u0026lt;a id=\u0026#34;l2\u0026#34; href=\u0026#34;#\u0026#34;\u0026gt;第二行\u0026lt;/a\u0026gt; \u0026lt;p\u0026gt;Hello, \u0026lt;b\u0026gt;您好\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; 10 11 \u0026lt;table\u0026gt; 12 \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;234\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;567\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;890\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; 13 \u0026lt;table\u0026gt; 14 \u0026lt;body\u0026gt; 15\u0026lt;html\u0026gt;  \u0026lt;標籤名稱(tag)\u0026gt; \u0026lt;html\u0026gt;: 最上層的標籤 \u0026lt;head\u0026gt;: 網頁的表頭, 包含許多資訊 \u0026lt;body\u0026gt;: 網站內容 \u0026lt;title\u0026gt;: 網頁標題 \u0026lt;meta\u0026gt; \u0026lt;h2\u0026gt;: 標題2 \u0026lt;a\u0026gt;:超連結 \u0026lt;p\u0026gt;: 文字段落  標籤+屬性\u0026ndash;\u0026gt;特定目的   標籤+屬性\u0026ndash;\u0026gt;特定目的 h2 class a href img src p style   進行歷史數據回測 交易策略歷史回測   當我們面對不理解的事物, 必須先去了解並在其中找到自己的適應方式, 好比我 們在爬山以前, 必須要做足功課, 先理解登山路徑並做好裝備準備等。 而交易市場也是如此, 若我們沒有做足準備, 很有可能讓自己身處在危險的環境之中。 在交易前應該要理解市場, 最常見的方法是將想法或策略量化並做歷史回測, 了解 我們的交易方法在歷史中的績效為何。- 本章將討論如何透過既有的歷史數據進行交易策略的回測, 從策略的角度切入市場, 找出自己的交易邏輯。  歷史回測以得穩定獲利機會   對於交易者而言, 歷史回測通常能幫助我們更快找到穩定獲利的機會。 在交易的 過程中, 歷史回測是否必要, 答案為 「是」, 因為透過程式驗證數據, 能查看更多的細節, 這往往無法由主觀交易得知。 許多數據一閃即逝, 容易被人忽略, 而 這些情況在計量回測中都能被正確驗證, 本章將會說明建構歷史回測的細節, 讓 大家快速投入計量回測的世界。  技巧62 認識歷史回測 (心法) 歷史回測:數學計量模型+市場行為分析:   當我們在市場上交易、但對於當前市場的趨勢變動沒有把握時, 就需要歷史回測來驗證自己的想法是否可行 歷史回測不僅是數學計量模型的計算, 也包含了計量模型以外的市場行爲分析。 以往大家沒有完整的歷史數據時, 若只有網路上散播的統計資訊、盤後資料, 並沒有辦法準確進行歷史回測, 而現在強調大數據的時代, 計量回測已經成爲一門不可或缺的技術了。  回測:用歷史資料回朔測試   回測」的意思就是使用歷史資料回溯測試。 當我們有一個交易的想法時, 首先會將規則明確列出, 並寫爲具體的程式碼, 接著就會拿出歷史資料加以驗證, 了解我們的想法在之前的交易日中具體成效爲何。 這時如果擁有夠多的歷史資料, 就能更準確了解可用性, 並在末來的預測中提供更準確的依據。  技巧63 回測流程建構(心法) 股票交易:選股+交易策略:   選定「交易標的」 做「交易策略」歷史回測  回測演算法:   「回測演算法」是透過歷史資料模擬開盤的情況, 進行資料解讀、計算、制斷, 以 決定是否進場、出場, 並且獲得進出場點的紀錄, 最後進行歷史績效的評估, 這樣的流 程是演算法開發的必要流程。 本技巧將會依序闆述建構的流程, 引導讀者去取得歷史資 料, 接著開始歷史回測。   歷史資料: 逐筆資料, K線資料:   轉換回測指標:\n   「轉換回測指標」是將歷史資料轉換爲量化數據, 讓交易者易於根據這些數據進行交易的制斷, 而每個交易者對於指標的定義都不盡相同, 所以必須明確定義指標後, 才能 進一步設計演算法。 股票逐筆報價可以轉換指標, 例如：內外盤、大單, K線可以計算 MA, RSI 等技術指標, 主要是我們需要建構怎樣的策略, 再去思考要如何計算指標。  歷史演算法設計: List為逐筆資料; Dictionary 為K線資料:   回測演算法所運用的資訊是歷史資料, 所以在編制回測演算法時, 可以依照需求去䫛取需要部分的資訊, 這裡也會依照「逐筆資料」、「K 線資料」而有所不同, 本書中將會針對這兩種資料分別去做回測。 逐筆資料會透過 Python 的 List 物件來操作, 而 K 線資料會透過 Python 的 Dictionary 來操作, 以迴圈的方式去進行歷史回測, 本章將會介紹逐筆資料以及 K 線資料的回測撰寫方式。 回測演算法也是交易演算法, 依照流程會有趨勢判斷、進場、出場、停損等相關步驟,這部分會在本章接下來的策略技巧中介紹到。  歷史回測回傳明細格式設計:   回測交易格式的設計是希望完整保存回測交易紀錄, 並且忠實表達交易事件的細節, 最後讓這些紀錄能夠被適度的分析, 讓回測的效益最佳化。   爲什麼欄位中沒有盈虧欄位呢 ? 因爲若要計算盈㿥, 只會增加撰寫演算法的麻煩度, 且回測所帶來的效盆也不一定是盈虧所產生的數字, 怎麼說呢 ? 就好比一支回測程式雖 然一個月的總盈虧是 $-1000$, 但它並不代表就是一支不好的策略, 或許買方的部位淨利是 3000 , 賣方的部位淨利是 $-4000$, 那只要將這支策略設定爲只做買方, 便會是一支賺錢的策略。 除了盈虧, 也有很多角度可以分析策略的好壞, 例如 : 交易時間、在倉時間等, 這些對於交易而言也是相當重要的, 在下一個技巧會介紹到。 每個欄位都有存在的價值, 第一個欄位的交易序號代表唯一值, 所以每筆資料並不會 發生重複的現象。 以上的交易回傳格式不一定符合每種交易類型的需求, 可以依照自己 的需求來更改, 接下來的回測範例程式將會記錄進出場資訊, 讀者可以自行改善及延伸。  績效計算工具:    取得交易紀錄後, 就可以依照交易回傳的資料去加以計算分析。\n  績效不單可從盈虧去 觀察, 也可以從買賣、交易次數、交易時間點來進行分析。\n  本書提供的績效分析範例雖 然不多, 但主要讓讀者熟悉系統分析指令的用法。\n  某些策略會符合某些時期的赹勢條件, 但不代表那些策略會符合長期市場的走勢, 畢竟交易市場是瞬息萬變的, 若要校調出一個長期穩定獲利的策略, 必須經過長期回測的測試的。\n  而計算績效不一定是從獲利盈餘的數字上來看, 以下提供其他參考績效的方向 讓讀者參考：交易次數勝率, 買賣個別成交結果.\n  後記 這次稍微讀了幾個技巧, 之後根據需求再來找書繼續研究. 天天向上！共勉之！  2022.01.28. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-28","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur030-%E7%A8%8B%E5%BC%8F%E4%BA%A4%E6%98%93%E5%85%A5%E9%96%80/","series":["每日文章"],"tags":["Python","演算法交易"],"title":"MUR030 演算法交易入門"},{"categories":["機器學習","可信任的AI"],"content":"思考水平聯邦學習  紫式晦澀每日一篇文章第29天\n 前言   今天是2022年第2˙天, 全年第4週, 一月的第四個週四. 今天來仔細思考「水平聯邦學習 (Horizontal Federated Learning)」的相關故事與細節. 即將截稿ICML, 打鐵趁熱, 剛好我們設計的communication protocol與水平聯邦學習的結構相似, 可以藉著相關的故事與文獻串起來.\n  今天的素材主要來自Federated Machine Learning: Concept and Applications 裡面關於水平聯邦學習的段落.\n  心法 聯邦學習-隱私保護去中心化的機器學習 聯邦學習概覽:   Google提出聯邦學習[36,37,41]. Google主要想法: 防止「資料洩漏(Data leakage), 建立機器學習模型, 基於分佈在多個裝置上的資料集. 主要研究: 統計挑戰[60, 77], 安全性改良[9,23], 個人化[13, 60]. On-device聯邦學習: 分佈式裝置用戶互動, 溝通成本高, 非均衡資料分佈, 裝置可靠性, 都是優化的挑戰. 水平分割(Horizontal partitioned data): 資料以用戶id裝置id被分割了. 隱私保護機器學習(Privacy-preserving ML): 聯邦學習考慮隱私, 以「去中心協力學習 (decentralized collaborative-learning)」的框架來保護隱私. 組織之間的協力學習(Collaborative-learning among organizations): 聯邦學習= 隱私保護去中心化的機器學習技術. (privacy-preserving decentralized collaborative machine-learning) 在[71]有聯邦學習技術\u0026amp; 聯邦遷移學習技術 本文章研究「安全性基礎(security foundations)」, 相關的「多代理人理論(Multiagent theory)」, 「隱私保護資料探勘(Privacy-preserving data mining)」.   我們也是去中心協力探索(Decentralized collaborative exploration).\n  The concept of federated learning was proposed by Google recently $[36,37,41]$. Google\u0026rsquo;s main idea is to build machine-learning models based on datasets that are distributed across multiple devices while preventing data leakage. Recent improvements have been focusing on overcoming the statistical challenges $[60,77]$ and improving security $[9,23]$ in federated learning. There are also research efforts to make federated learning more personalizable $[13,60]$. The above works all focus on on-device federated learning in which distributed mobile-user interactions are involved and communication cost in massive distribution, unbalanced data distribution, and device reliability are some of the major factors for optimization. In addition, data is partitioned by user Ids or device Ids, therefore, horizontally in the data space. This line of work is highly related to privacy-preserving machine learning, as reported in [58] because it also considers data privacy in a decentralized collaborative-learning setting. To extend the concept of federated learning to cover collaborative-learning scenarios among organizations, we extend the original \u0026ldquo;federated learning\u0026rdquo; to a general concept for all privacy-preserving decentralized collaborative machine-learning techniques. In [71], we have given a preliminary overview of the federated-learning and federated transfer-learning technique. In this article, we further survey the relevant security foundations and explore the relationship with several other related areas, such as multiagent theory and privacypreserving data mining. In this section, we provide a more comprehensive definition of federated learning that considers data partitions, security, and applications. We also describe a workflow and system architecture for the federated-learning system.\n 水平聯邦學習-用戶不同, 業務相似 水平聯邦學習: 用戶不同, 但業務相同:   水平聯邦學習=基於樣本的聯邦學習: 資料集有同樣的特徵空間, 但樣本的空間不同. 例子: 兩個銀行可以有很不同的用戶群, 但他們的業務很類似, 所以特徵空間相同. [58]: 協同深度學習-參與者獨立訓練, 只分享參數更新的子集 [41]: 2017年Google利用水平聯邦學習來做Android的模型更新. 協同學習一個中心模型. 安全聚合方案(Secure aggregation scheme)來保護被拒和用戶更新的隱私, 在聯邦學習的框架下 同態加密( homomorphic encryption):同態加密指的是一種加密方式，使得使用者可以對密文做計算，計算出來的結果，再做解密的時候的明文是預期的計算結果。   2.3.1 Horizontal Federated Learning. Horizontal federated learning, or sample-based federated learning, is introduced in the scenarios in which datasets share the same feature space but different space in samples (Figure 2(a)). For example, two regional banks may have very different user groups from their respective regions, and the intersection set of their users is very small. However, their business is very similar, so the feature spaces are the same. The authors of [58] proposed a collaboratively deep-learning scheme in which participants train independently and share only subsets of updates of parameters. In 2017, Google proposed a horizontal federated-learning solution for Android phone model updates [41]. In that framework, a single user using an Android phone updates the model parameters locally and uploads the parameters to the Android cloud, thus jointly training the centralized model together with other data owners. A secure aggregation scheme to protect the privacy of aggregated user updates under their federated-learning framework is also introduced in [9]. The authors of [51] use additively homomorphic encryption for model parameter aggregation to provide security against the central server.\n 水平聯邦學習的安全性: 誠實但好奇的服務器 水平聯邦學習的安全性:   誠實的參與者(honest participants); 誠實但好奇的服務器(honest-but-curious server) [9,51] 只有服務器可以破壞資料參與者的隱私. [29] 惡意用戶 服務器完成訓練, 會把參數分享給所有資料參與者.   Security Definition. A horizontal federated learning system typically assumes honest participants and security against an honest-but-curious server $[9,51]$. That is, only the server can compromise the privacy of data participants. Security proof has been provided in these works. Recently, another security model considering malicious users [29] was also proposed, posing additional privacy challenges. At the end of the training, the universal model and all of the model parameters are exposed to all participants.\n 水平, 垂直, 遷移聯邦學習:    聯邦學習 實例     🤩 水平聯邦學習: 用戶不同, 業務相同 兩個城市的「銀行」   😍 垂直聯邦學習: 用戶相同, 業務不同 同一個城市的「銀行」與「電子商務」   🤯 遷移聯邦學習: 用戶不同, 業務不同 美國的電子商務, 亞洲的銀行      技法 資料: (特徵, 標記, 樣本身份)=(feature, label, sample ID) 資料: (特徵, 標記, 樣本身份)=(feature, label, sample ID):   資料矩陣: row是sample, column是feature. 資料符號: feature是X, label是Y, sample ID是 I 金融的例子: label = 信用 (credit) 行銷的例子: label = 用戶購買慾(user\u0026rsquo;s purchase desire) 教育的例子: label = 學位(degree of the students. ) 分類: 水平聯邦學習, 垂直聯邦學習, 聯邦遷移學習(feature, label都不一樣)   Let matrix $\\mathcal{D}_{i}$ denote the data held by each data owner $i$. Each row of the matrix represents a sample, and each column represents a feature. At the same time, some datasets may also contain label data. We denote the feature space as $\\mathcal{X}$, the label space as $\\mathcal{Y}$, and we use $\\mathcal{I}$ to denote the sample ID space. For example, in the financial field, labels may be users' credit; in the marketing field, labels may be the user\u0026rsquo;s purchase desire; in the education field, $y$ may be the degree of the students. The feature $\\mathcal{X}$, label $\\mathcal{Y}$, and sample Ids $\\mathcal{I}$ constitute the complete training dataset $(I, \\mathcal{X}, \\mathcal{Y})$. The feature and sample spaces of the data parties may not be identical, and we classify federated learning into horizontally federated learning, vertically federated learning, and federated transfer learning based on how data is distributed among various parties in the feature and sample ID space. Figure 2 shows the various federated learning frameworks for a two-party scenario.\n 水平聯邦學習在資料下的表達:   (特徵, 標記)相同\u0026ndash;\u0026gt;業務相同 樣本ID不同\u0026ndash;\u0026gt; 用戶不同   We summarize horizontal federated learning as $X_{i}=X_{j}, \\quad y_{i}=y_{j}, I_{i} \\neq I_{j}, \\quad \\forall \\mathcal{D}{i}, \\mathcal{D}{j}, i \\neq j .$\n 水平聯邦學習執行流程:   有相同資料結構的k個參與者, 協同學習機器學型模型, 利用雲端服務器來幫忙. 假設參與者誠實, 服務器誠實但好奇. 所以沒有資訊洩漏從參與者到服務器. 訓練流程:   參與者計算局部的訓練梯度, 給梯度戴面具, 用加密[51], 差分隱私[58], 秘密分享[9]. 將面具後的結果傳給服務器.    服務器計算聚合結果.    服務器傳回聚合結果給參與者.    參與者更新模型, 利用加密梯度.   接著持續跑, 讓損失函數收斂, 完成整個訓練過程. 所有參與者分享最後的結果.   我們只會傳Lasso, 但不會傳其訓練出來的regularization level, 所以也不會洩漏隱私.\n  2.4.1 Horizontal Federated Learning. A typical architecture for a horizontal federated-learning system is shown in Figure 3 . In this system, $\\mathrm{k}$ participants with the same data structure collaboratively learn a machine-learning model with the help of a parameter or cloud server. A typical assumption is that the participants are honest whereas the server is honest but curious; therefore, no leakage of information from any participants to the server is allowed [51]. The training process of such a system usually contains the following four steps.\n  Step 1: Participants locally compute training gradients; mask a selection of gradients with encryption [51], differential privacy [58], or secret sharing [9] techniques; and send masked results to the server. Step 2: The server performs secure aggregation without learning information about any participant. Step 3: The server sends back the aggregated results to participants. Step 4: Participants update their respective model with the decrypted gradients.   Iterations through the above steps continue until the loss function converges, thus completing the entire training process. This architecture is independent of specific machine-learning algorithms (logistic regression, DNN, etc.) and all participants will share the final model parameters.\n 用法 聯邦學習比分佈機器學習更沒有資料的完全使用權 聯邦學習VS分佈機器學習:對資料有沒有完全使用權:   分佈式機器學習: 訓練資料的分佈儲存, 計算任務的分佈執行, 模型結果的分佈分發. 參數服務器: 加速訓練過程, 存資料, 分配計算資源, 配資源, 讓模型訓練更有效率. 水平聯邦學習: node是資料擁有者, 不一定需要配合 聯邦學習面對更複雜的學習環境 聯邦學習強調對資料擁有者於訓練過程中的「資料隱私保護 (Data-privacy protection)」 保護資料隱私的度量, 要在未來加強資料安全規範環境上更增強 聯邦學習要討論non-IID資料.   3.2 Federated Learning versus Distributed Machine Learning Horizontal federated learning at first sight is somewhat similar to distributed machine learning. Distributed machine learning covers many aspects, including distributed storage of training data, distributed operation of computing tasks, and distributed distribution of model results. A parameter server [30] is a typical element in distributed machine learning. As a tool to accelerate the training process, the parameter server stores data on distributed working nodes and allocates data and computing resources through a central scheduling node to train the model more efficiently. For horizontally federated learning, the working node represents the data owner. It has full autonomy for the local data; it can decide when and how to join the federated learning. In the parameter server, the central node always takes control; thus, federated learning is faced with a more complex learning environment. In addition, federated learning emphasizes the data-privacy protection of the data owner during the model training process. Effective measures to protect data privacy can better cope with the increasingly stringent data privacy and data security regulatory environment in the future. As in distributed machine-learning settings, federated learning will also need to address non-IID data. The authors of [77] showed that, with non-IID local data, performance can be greatly reduced for federated learning. The authors in response supplied a new method to address the issue similar to transfer learning.\n 後記  讀了Federated Machine Learning: Concept and Applications 後, 收穫很多！ 感覺對聯邦學習很多的技術細節更了解了! 這樣做問題就可以抓住核心需要的概念, 寫出對方覺得重要的點!\n  持續利用文章輸出的方法來學習與整理資訊成知識, 效果真的比之前邊找邊讀還要好很多! 加上打字夠快, 把讀過的記下來的東西都變成之後reference的資料庫, 成為時間的朋友, 感受知識的複利! 天天向上, 共勉之!\n  2022.01.27. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-27","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur029-%E6%80%9D%E8%80%83%E6%B0%B4%E5%B9%B3%E8%81%AF%E9%82%A6%E5%AD%B8%E7%BF%92/","series":["每日文章"],"tags":["聯邦學習"],"title":"MUR029 思考水平聯邦學習"},{"categories":["文獻閱讀"],"content":"Bregman鞅集中不等式基礎-Ver(0.1)  紫式晦澀每日一篇文章第28天\n 前言   今天是2022年第26天, 全年第4週, 一月的第四個週三. 今天來嘗試寫「非漸進鞅集中不等式 (Non-Asymptotic Martingale Concentration Inequality)」, 提升自己對集中不等式的理解.\n  今天的素材主要來自Bregman Deviations of Generic Exponential Families . 作者Odalric-Ambrym Maillard的文章是我學習非漸進鞅不等式的主要材料, 值得持續投資時間來累積自己的機率功力.\n   代碼BEF\n 000 摘要 拉普拉斯方法(Laplace method): We revisit the method of mixture technique, also known as the Laplace method, to study the concentration phenomenon in generic exponential families.\n控制「有限樣本估計」與「指數族參數」之間的「Bregman散度」: Combining the properties of Bregman divergence associated with log-partition function of the family with the method of mixtures for super-martingales, we establish a generic bound controlling the Bregman divergence between the parameter of the family and a finite sample estimate of the parameter.\nBregman資訊得 (Bregman information gain) 建立時間均勻結果: Our bound is time-uniform and makes appear a quantity extending the classical information gain to exponential families, which we call the Bregman information gain.\n經典參數族的具體「信心集(confidence sets)」與「Bregman資訊得」: For the practitioner, we instantiate this novel bound to several classical families, e.g., Gaussian, Bernoulli, Exponential and Chi-square yielding explicit forms of the confidence sets and the Bregman information gain.\n數值上與SOTA表現相當:We further numerically compare the resulting confidence bounds to state-of-the-art alternatives for time-uniform concentration and show that this novel method yields competitive results.\n實踐在線性脈絡多臂強盜問題: Finally, we highlight how our results can be applied in a linear contextual multi-armed bandit problem.\n100 簡介 1.0 Introduction (P.1-P.2) BEF101 集中不等式以拿到估計誤差:\n 集中不等式: 理論統計的工具箱, 在機器學習有核心應用 原因: 機器學習中, 常常用「來自未知分佈的樣本」估計東西, 且想要知道「估計誤差(estimation error)」 例子: 可測實隨機變數的平均, 利用i.i.d.樣本的經驗平均估計. 脈絡: a. Ste ́phane Boucheron, Ga ́bor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. b. Maxim Raginsky and Igal Sason. Concentration of measure inequalities in information theory, communications and coding. arXiv preprint arXiv:1212.4663, 2012. c. A Dembo O Zeitouni and A Dembo. Large deviations techniques and applications. Applications of Mathe- matics, 38, 1998.   Concentration inequalities are a powerful set of methods in statistical theory with key applications in machine learning. This is due to the fact that in many machine learning applications, a learner often estimates some quantity solely based on samples from an unknown distribution, and would like to know the magnitude of the estimation error. The typical example is that of the mean $\\mu$ of some measurable real-valued random variable $X$, estimated by its empirical mean built from a sample of $n$ independent and identically distributed (i.i.d.) observations. We refer the interested reader to the monographs of Boucheron et al. (2013); Raginsky and Sason (2012); Zeitouni and Dembo (1998) for standard results in this and related topics.\n BEF102 指數族是估計向量參數融通的參數族模型:\n 需求: 許多狀況下也會要估計「向量參數(Vector Parameter)」, 例如線性迴歸分析. 需求: 估計參數族分佈的參數 指數族: 很融通的參數族分佈$p_{\\theta}(x) \\propto h(x) \\exp (\\langle\\theta, F(x)\\rangle)$ 指數族實例: Gaussian (possibly multivariate, with or without known covariance), Poisson, Exponential, Gamma, Chi-square, Bernoulli and Multinomial (over a fixed alphabet) distributions.   In many situations, one may want to further estimate some vector parameter, as in e.g. linear regression (Abbasi-Yadkori et al., 2011). A closely related problem is to estimate the parameter of a distribution coming from a parametric family (Chowdhury et al., 2021). Exponential families are a flexible way to formalize such distributions, where a distribution on some set $\\mathcal{X}$ is described by its density $p_{\\theta}$ such that $p_{\\theta}(x) \\propto h(x) \\exp (\\langle\\theta, F(x)\\rangle)$, for some given feature function $F: \\mathcal{X} \\mapsto \\mathbb{R}^{d}$ and base function $h$ (see details in Section 2). Most classical distributions fall into this category, including Gaussian (possibly multivariate, with or without known covariance), Poisson, Exponential, Gamma, Chi-square, Bernoulli and Multinomial (over a fixed alphabet) distributions, to name a few; see e.g. Amari (2016).\n BEF103 序貫主動取樣策略下, 需要隨機停時集中不等式:\n 序貫主動取樣策略: 當代機器學習問題, 很多都有「序貫主動取樣策略(Sequential, active data-sampling strategies)」. 實例: 多手臂強盜 multi-armed bandits, 強化學習 reinforcement learning, 主動學習 active learning, 聯邦學習 federated learning. 需求: 「取樣決策 (decision to sample)」來自學習算法與環境之間的互動, 與過去觀察相關, 因此需求「樣本數是隨機停時」的集中不等式結果. 作法: 時間均勻集中不等式, 信賴區間序列, 而不再只是對固定樣本成立的結果.   A number of problems in current-day machine learning involve sequential, active data-sampling strategies (Cesa-Bianchi and Lugosi, 2006). This includes multi-armed bandits, reinforcement learning, active learning and federated learning, to mention a few application domains. Since the decision to sample a novel observation results from the interaction between the learning algorithm and the environment, and depends on past observations, one needs to design concentration inequalities working with a random number of observations typically at a random stopping time (Durrett, 2019). A natural way to handle this difficulty is to derive time-uniform concentration inequalities, producing sequences of confidence sets valid uniformly over all number of observations with high probability, as initiated by Robbins (1970), as opposed to being valid for a single number of observation.\n BEF104 時間剝皮法-超鞅與幾何時間格:\n 來自「強盜理論 (bandit theory)」的技術, 是使用「超鞅(Super-martingale)」與「幾何時間格 (geometric time grid)」, 又稱「時間剝皮 (time peeling)」的技術. 近期的進展看Howard 2020.   A popular technique in bandit theory is to combine super-martingale techniques with union bound arguments over a geometric time grid, a technique known as time peeling - see Bubeck (2010), Cappé et al. (2013) for early uses in bandits, as well as Garivier (2013), or more recently Maillard (2019). See also Howard et al. (2020) for a recent, complementary survey of the history of this field.\n BEF105 拉普拉斯法的進展:1949, 2008, 2011, 2017, 2018, 2019, 2020, 2021:\n 替代方法: 混合法(method of mixtures). 線性強盜: Abbasi-Yadkori的Improved linear bandit那篇, 影響力很大的文章. Emilie Kaufmann and Koolen (2018): 混合法與指數族, 姐具體Gaussian (known variance)與Gamma(known shape) Shafer and Vovk(2019): 對「有界分佈(bounded distributions)造另一種capital process, 由Waudby-Smoth and Ramdas(2020)推廣. Kuchibhotla and Zheng (2021): 結合Bentkus集中不等式結果, 結合剝皮法, 得到很sharp的有界隨機變數deviation結果.   The method of mixtures, initiated by Robbins and Pitman (1949) and popularized in Peña et al. (2008) is a powerful alternative to develop time-uniform confidence sets. It has been applied to sub-Gaussian families in Abbasi-Yadkori et al. (2011), leading to a variety of applications (see Chowdhury and Gopalan (2017); Durand et al. (2017); Kirschner and Krause (2018) among others). In Kaufmann and Koolen (2018), a generalization to handle exponential families is considered, with applications to the cases of Gaussian (with known variance) and Gamma distributions (with known shape). A fairly different \u0026lsquo;capital process\u0026rsquo; construction technique has been recently developed for bounded distributions in Shafer and Vovk (2019), and popularized further in Waudby-Smith and Ramdas (2020). Very recently, Kuchibhotla and Zheng (2021) incorporate Bentkus' concentration results (Bentkus, 2004) with the peeling technique to prove deviation results for bounded random variables.\n BEF106 拉普拉斯法的積分計算, 可由貝式參數更新化約:\n 本作: 參數指數族的拉普拉斯法, 得到Bregman散度的偏離結果. 拉普拉斯法主要難點: 需要計算積分, 對廣義分佈有實踐阻礙. 貝氏解釋: 用共軛先驗, 可以把積分計算轉為「參數更新(parameter update)」 計算可追(Computationally tractable)信賴集: sharp與computationally tractable, 對「通用指數族(generic exponential families)」執行拉普拉斯法   In this article, we revisit the method of mixtures for the case of parametric exponential families, expressing deviations in the natural (Bregman) divergence of the problem. Our motivation is the following: On the one hand, an a priori difficulty of applying the method of mixture is that it involves computation of integrals, which can be tedious for general distributions. On the other hand, the setting of parametric exponential families is known to be convenient when dealing with integration in a Bayesian setup, thanks to the notion of a conjugate prior that enables us to reduce computation of a tedious integral to a simple parameter update. While the case of Gaussian distributions has been extensively studied for several decades, in this work, we investigate to which extent one can obtain both sharp and computationally tractable confidence sets using the method of mixture beyond the Gaussian case, for generic exponential families.\n 很精彩資訊豐富的Introduction, 對瞭解相關理論技術的人, 是非常好的文件, 來學習最進階的拉普拉斯法與時間均勻集中不等式.\n200 方法 2.0 Exponential Families and Bregman Divergence (P.3) BEF201 指數族(Exponentil Families):\n 參數化分佈: 用開集合當參數範圍, 指定具體的密度函數形式. 密度函數成員: 特徵函數(feature), 基礎函數(base), 對數分割函數(log-partition) 優化正規性條: (1) 對數分割函數有限 (2) 對數分割函數的Gradient是一對一(3) 對數分割函數的Hessian矩陣可逆. 期望泛函(Expectional Functional): 在此參數族內, 執行機率測度與期望算子.    BEF202 指數族的Bregman散度 (Bregman Divergence):\n KL散度: 指數族中兩個密度函數的距離, 可以表達成一種「泰勒二階餘項 (Taylor second order remainder)」 對偶參數(Dual parameter): 期望值, 在這個框架下, 可以被理解為對偶參數, 而因為在指數族中, 期望等於對數分割函數的梯度(Gradient) 在這個意義下, 「指數族中兩密度的KL散度」等同於「兩對偶參數的Bregman散度」 在這個新的脈絡下, 對數分割函數, 成為「勢能函數 (Potential function)」, 而Bregman散度成為勢能函數, 貨真價實的泰勒二階餘項.  BEF203 Bregman散度, 好控制尾端行為, 好操作代數:\n 典範Bregman散度(Canonical Bregman divergnce): 指數族的典範Bregman散度有兩個基礎性質. 性質一: 隨機變數的「對數動差生成函數(log-moment generating function)」, 讓Bregman散度能很好控制「隨機變數尾端行為 (Tail-behaviour of random variables)」. 性質二: 對偶性(Duality)可以讓我們做方便的代數操作.   Tail and Duality Properties The canonical Bregman divergence of an exponential family enjoys two fundamental properties that we recall below. The first one links it to the log-moment generating function (MGF) of the random variable $F(X)$, which makes Bregman divergences especially well suited to control the tail-behaviour of random variables appearing for instance in concentration inequalities. The second one highlights duality properties that enables convenient algebraic manipulations.\n BEF204 Bregman散度: 位移, 對偶, 內差梯度:\n 位移: Bregman散度下的位移, 可以被對數動差生成函數給描述 對偶: 如果勢能函數的梯度是一對一, 則有「Bregman對偶性」: 兩對偶參數的Bregman散度, 等同「勢能梯度」之間的「對偶Bregman散度」. 內插梯度: 梯度面向的位移, 與對偶參數內差量有個互相表達的關係, 與內插梯度有關.  3.0 Time-Uniform Bregman Concentration (P.4) 3.1 A Generic Deviation Inequality (P.4-P.6) 3.2 Specification to Classical Families (P.6-P.7) 3.3 Proof Sketch: Theorem 1 (P.7-P.8) Another section BEF: BEF: BEF: BEF: BEF: BEF:\nAnother section BEF: BEF: BEF: BEF: BEF: BEF:\n300 結果 4.0 Numerical Experiments (P.8-P.11) 5.0 Linear Bandits Revisited (P.11-P.12) 400 討論 6.0 Conclusion (P.12-P.13) 900 形式結果 A.0 Proof of the main result about Bregman deviations (P.15) B.0 Properties of Bregman Divergence (P.20) C.0 Specification to illustrative exponential families (P.21) D.0 Empirical Comparision with existing time-uniform confidence sequences (P.25) E.0 Tuning of the regularization parameter c (P.33) 後記 到此解讀了摘要以及製作十張知識卡片. 之後再持續累積, 快思慢想把這個文章吃透. 天天向上, 共勉之!  2022.01.26. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-26","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur028-bregman%E9%9E%85%E9%9B%86%E4%B8%AD%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%9F%BA%E7%A4%8E/","series":["每日文章"],"tags":["鞅集中不等式"],"title":"MUR028 Bregman鞅集中不等式基礎-Ver(0.1)"},{"categories":["寫作","學術工作流","語言學"],"content":"系統思考子句理論  紫式晦澀每日一篇文章第27天\n 前言   今天是2022年第24天, 全年第4週, 一月的第四個週二. 今天來系統思考「子句(Clause)」與「句子(Sentence)」相關的知識. 增加對文字的感受能力, 快速學習找出更深的體會.\n  今天的素材主要來自Grammar choices for graduate and professional writers . 這本書在博士一二年級時有仔細研究過, 而現在又有更多寫文章的經驗, 再來讀讀一定會有新收穫.\n   Finite Clause vs. Nonfinite Clause  子句 Clauses  Finite and Non-Finite Verbs | Definition, Useful Rules \u0026amp; Examples  Exploring the grammar of the clause  English clause syntax   子句 Clause 心法: 有界主句表達主要命題, 無界子句附加背景證據  子句(Clause): 表達完整命題(Complete Proposition)的最小文法單位. (A clause is often said to be the smallest grammatical unit that can express a complete proposition.)\n  有界子句(Finite Clause):\n   有「動詞 (Verb)」表達「時態,人稱, 性數」, 有「主詞 (Subject)」. A finite clause includes a primary verb — a verb that can be inflected for tense, person and sometimes number — and includes a subject. 有限子句: 獨立子句, 對等子句, 附屬子句, 補充子句. A finite clause can stand alone as an independent clause (a sentence), or it can serve as a coordinate clause, a subordinate clause, or a supplementary clause.  無界子句 (Non-finite clause):   相依子句, 鑲嵌子句, 表達狀態或事件, 不會因為造句而改變. 因此使用時「不用指定時間」. A dependent or embedded clause that represents a state or event in the same way no matter whether it takes place before, during, or after text production.  技法: 有界動詞與主體表達, 時態, 語貌, 語態, 語氣; 無界動詞構造, 動名詞, 不定詞, 分詞 有界動詞(Finite Verb):   主要動詞(Main verb), 會隨著主詞(Subject)而改變形式 表達時態, 語貌, 語態, 語氣(Tense/Aspect/Voice) 時態(Tense): 在時間位置上 語貌, 體 (Aspect): 完成, 未完成 語態(Voice): 主動, 被動 語氣(Mood): 必要性程度, 義務, 可能性, 能力  無界動詞(Non-finite Verb):   無法成為主要動詞 無法表達時態, 語氣, 性別 於句子中如同「名詞(Nouns)」, 「副詞(Adverbs)」與「形容詞(Adjectives)」. 構造「無界子句(Non-finite clause)」成為相依子句  無界動詞構造三法(Verbal):     動名詞 (Gerunds): 如同「名詞(Nouns)」    不定詞 (Infinitives): 如同「副詞(Adverbs)」與「形容詞(Adjectives)」    分詞 (Participles): 如同「形容詞(Adjectives)」    用法: 句型分析, 立即組成分析 句子組成分析:   從子句的理論, 我們可以去研究「句子組成分析 (Sentence Constituents)」   立即組成分析(Immediate Constituent Analysis; IC analysis):   非常酷的領域！之後研究一番!  文法: 獨立子句完整命題; 相依子句豐富命題 獨立子句類型(Type of independent clause):     直述句(Declarative)    疑問句(Interrogative)    命令句(Imperative)    感嘆句(Exclamative)    相依子句類型(Type of dependent clause):     關係子句(Relative Clauses)   1.1. 非wh關係子句(Non-wh-relatives) 1.2. Wh關係子句 (Wh-relatives)  比較子句(Comparative Clauses)    內容子句, 名詞子句, 補充子句 (Content clause, Noun clause, Complement clause)   3.1 直述名詞子句(Declarative content clause) 3.2 疑問名詞子句(Interrogative content clause)  無界子句(Non-finite clauses):   裸不定(bare infinitive): go to the party To不定子句(to-infinitive clause): to go to the party 過去分詞子句(past participial clause): 完成體構造: I had seen to it. 現在分詞子句(present participial clause): 繼承主要子句的主詞, 做形容詞或副詞: The king being in good health, his physician was able to take a few days' rest. 動名詞子句(gerund clause): 繼承主要子句的主詞, 做名詞  無動詞子句(Verbless clauses):   借「謂詞(Predicand)」來造句: With the children so sick, 這句裡面並沒有動詞, 原型是 the children are so sick. 這裏把are 替代成 with了 其他命題例子: although, once, when, while 都有類似表達 例如: Although no longer a student, she still dreamed of the school. 裡面的原句是 she is no longer a student.  後記  到此我們從子句的概念, 看到動詞以及組成分析(Constituent Analysis)相關的資訊. 之前由於對動詞的理論, 以及「語法學(Syntax)」有相當程度的研究, 現在看到這些資訊感覺醍醐灌頂！分析英文句子用組成分析, 來看各種paper是如何建構抽象的思想的!\n  每天寫文章, 把一個問題解答完整, 不只可以練習摘要的能力, 還可以確保資訊不再是碎片而是由自己的體系組織, 成為以後可複用的學習紀錄. 天天向上, 共勉之!\n  2022.01.25. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-25","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur027-%E7%B3%BB%E7%B5%B1%E6%80%9D%E8%80%83%E5%AD%90%E5%8F%A5%E7%90%86%E8%AB%96/","series":["每日文章"],"tags":[],"title":"MUR027 系統思考子句理論"},{"categories":["寫作","學術工作流"],"content":"讀山口拓郎的模板寫作法  紫式晦澀每日一篇文章第26天\n 前言   今天是2022年第24天, 全年第4週, 一月的第四個週一. 今天要寫ICML的文章, 因此來學習增且自己組織資訊的能力. 得到App提供很好的電子書服務, 做筆記學習, 抽取書的內容非常方便. 在寫這文章的途中, 連帶思考自己於「學術元工作流」中的用例與工作流程.\n  今天的素材主要來自山口拓朗 的模板寫作法 . 本書的結構十分整齊漂亮, 守破離是邁向獨立學者必經的三個段落.\n  三大模板的心法技法用法 心法: 守破離-守基本功以模板; 破基本功以寫作; 離基本功以文章  模板就是輸出結構. 好的結構讓資訊能有效重用(Reusable).\n 模板是作者展現個性的基礎舞台:   模板只是起到規劃文章走向的向導作用。 在使用模板的基礎上，在填充內容、詞匯選擇、表現方式以及文章體裁等方面再下些功夫，作者的個性會自然而然地散發出來，從而創作出扣人心弦的文章。   Input: 守基本功以模板\nProcess: 破基本功以寫作\nOutput: 離基本功以文章\n 守破離:   “守破離”原指以劍道和茶道為首的武道以及藝道的“修行階段”。 “守”，是指最初階段需遵從老師和流派的教誨，勤練基本功及技巧，達到熟練的境界。 “破”，是指聽取其他老師或流派的教誨，達到吸取精華並發展心術的境界。 “離”，是指脫離某個流派後，獨自創新招數，另開辟新境界。   本書傳授的模板相當於守破離中的“守”。\n 技法: 列舉, 結論優先, 故事; 詳細訊息(事實), 獲得認同(左腦), 引起共鳴(右腦) 三種基礎模板:列舉, 結論優先, 故事:   所謂模板，就是指文章走向的“結構模式”。 如果按照“先寫什麽，然後寫什麽，接著寫什麽，最後寫什麽”的指定順序來寫的話，你就不會在文章構思上消耗大量的時間了。 與此同時，你會非常流暢地寫出通俗易懂的文章。  A. 闡述多個訊息-列舉型:   【模板1】無壓力閱讀的“列舉型” 將手頭的信息按要點分門別類後具體闡述的就是“列舉型”。 寫作順序為“A:整體構思→B:列舉要點1→C:列舉要點2→D:列舉要點3→E：總結”。 當你想闡述多個信息時，此模板非常有用。  少則分; 多則合:   當手頭的信息處於待具體化的狀態時，需要有“敢於分成幾個要點”的想法。 相反，手頭的信息分得過於詳細時，需要有“敢於把信息合並在一起”“敢於給信息排優先順序（按排名高到低)”的想法。  B. 獲得讀者認同-結論優先型:   【模板2】提升信服度的“結論優先型” 當你想「重點傳達核心內容」時，這個模板再適合不過了。 寫作順序為“A：結論→B：理由·依據→C：具體事例·詳細內容→D：總結”。 當你想通過文章得到讀者的認同，就用這個“結論優先型”模板  結論-理由-具體事例:   圍繞核心內容(結論)寫, 會使邏輯更清晰, 降低跑題的風險 「理由」與「具體事例」是使結論或觀點,更有說服力, 而需要的支撐信息.   Screening是一個Federation後的得到的好處, 也是加速整個任務表現的關鍵. 理由: Federation帶來好處 具體事例: Federated Lasso理論可以證明; 實驗也有好效果.\n C. 產生讀者共鳴-故事型:   想通過故事記敘時就用這個模板。 寫作順序為“A：消沈狀態→B：轉機→C：轉變與成長→D：展望未來”。 當你想喚起讀者的共鳴時，就用這個“故事型”模板。  問題-轉機-轉變-未來:   問題: 找出研究斷層 轉機: 利用新的研究方法來填補研究斷層 轉變: 新的研究方法對存在的問題提供新的知識 未來: 同樣的套路, 成為一個系列作法, 產生系統知識.  用法: 寫文章從一行字開始, 整理資訊, 抽象具體概念互換 一行字概括是邁向“條理清晰”的第一步:   一言以蔽之:一篇文章最終都可以用一行字來概括，與文章的長短無關。論文的題目、書籍的名稱，都是用一行左右的文字來體現的。 中心思想力：如果作者無法用一行文字概括的話，那麽這篇文章也不會好到哪去，因為這說明作者沒有把握好自己想傳達的核心內容。用一行文字傳達不了的東西，用10行、100行都是無法傳達的。這就是把握好中心思想的重要性。 模板化輸出: 本書介紹的模板都分為4~5個部分。如果是平時經常寫的文章（已整理寫作素材），很快就可以進入寫作狀態中了。 提綱軟實力: 如果是第一次寫或者是還未整理寫作素材的情況，可以先練習列提綱，一行字概括各部分內容。  整理信息:   实际上，如果能用一行文字准确地概括每段的内容的话，就能写出条理清晰、易于理解的文章。 其实有些人读完上述提纲后已经理解了文章想表达的内容，这是因为作者已经准确地提炼出了想要传达的中心思想。 反过来说，如果无法用一行文字提取文章中心思想的话，通往“条理清晰的文章”和“有魅力的文章”的道路就会很艰难。 如果提纲不是很完美，或者不太满意，则说明尚未达到扩充内容的阶段。“零”再怎么扩充也只不过是“零”。 列提纲就是用一句话提炼每段内容的中心思想的过程。 当无法列出提纲时，要意识到自己头脑尚处于混乱状态，需要先整理信息。  抽象概念与具体概念相互转换:   写作就是将抽象概念和具体概念进行融合的过程。 比如，在第37页的提纲中提到的“成本”，这个词本身就很抽象，不同的人理解的意思是不同的。 成本的释义中含有生产成本、成本价格等，也包含劳力和时间的含义。 在C“阐述具体事例·详细内容”中，针对B“阐述理由·依据”中体现的“成本”做了具体的描述。 “去医院看病”指的是时间和劳力上的成本，“会增加医疗费、药费的开销”指的是金钱上的成本。  具體抽象的取捨:   写文章时，具体到什么程度，要根据写作者想传达到什么程度和读者想了解到什么程度而变化。 需要注意的是，有时即使写作者认为这样写应该可以看得懂，但是对于读者来说还没有具体到能够理解、认同的水平（高度抽象）。 重要的是，写作者不能随心所欲地决定具体化的程度，而是需要结合读者的理解、认知水平而定。 从列提纲到扩展句子的过程可以理解为将中心思想（=高度抽象）具体化的过程。  三種模板比較:  列舉型模板-提供讀者詳細清楚的資訊 心法: 組織傳達多條資訊 組織並傳達多條信息的“列舉型”:   列舉型模板是針對某一個主題、分成幾個要點傳達的模板。 在文章開頭部分表明接下來要傳達的主題和列舉的要點的數量後，再依次傳達要點。 通過傳達整體構思，可以了解到接下來具體會講幾方面的內容。這樣可以讓讀者提前做好心理準備，更容易接受接下來講的內容。 這種結構不僅可以應用於寫作，善於表達的人經常會套用於各種場合。    “傳達整體構思”也可以理解成“交出文本地圖”。拿到“地圖”的讀者，通過了解話題的行進方向，可以專注地聽下去。\n   技法: 說明意圖, 簡潔列舉, 共通項抽象化 使用技巧①說明文章的寫作意圖:   在文章開頭就明確表達講什麽、講幾點，這樣的話，讀者更容易掌握文章的來龍去脈，從而更容易理解下文。 在傳達整體構思時，也可以不用數字，而是用“有如下幾點”這樣的表達方式：   有幾種提高時間效率的方法。    想順利畢業，必須通過幾項考試。    想要解決這個問題，需要研究幾個方案。    使用技巧②列舉的要點要簡潔:   在開頭部分傳達整體構思後，再將需要傳達的事項（列舉要點） 一一闡述清楚（B~D）。  使用技巧③總結時把共通項抽象化:   整理完B~D的列舉要點後，最後寫E的總結部分。寫文章的總結部分時，要意識到將所有的列舉要點概括起來。 前面講的例文中將增加知識和提高自身素養、加強詞匯能力、變得更加寬容這3個好處作為未來的志向進行了概括。  用法: 宣傳介紹, 會議報告, 提供經驗, 諮詢郵件  例文①PR文章（宣傳類文章）“關於上門做飯服務的介紹:   例文②會議報告（商務文件）“關於展會上展位的決定事宜”:   例文③　提供經驗（博客文章）“預防夏日倦怠癥的3種食材”:   例文④　咨詢文（郵件）“關於貴公司提供的新服務”:   文法: 最多七項, 重要先說, 分類列舉, 刻意說一個關鍵點. 立竿見影的技巧①列舉的數量控制在2~7項: 立竿見影的技巧②按照重要性由高到低的順序排列: 立竿見影的技巧③分類別列舉: 立竿見影的技巧④在“整體構思”部分只闡述一個關鍵點: 編排①按照“首先”“其次”“最後”的順序:  編排②按照“首先”“其次”“再者”的順序:  編排③8條以上的信息采用逐條列舉的方式:  結論優先型模板-使讀者認同 心法: 開門見山, 幹枝葉, 摘要力 開門見山:   “結論優先型”，顧名思義就是開門見山先說結論，再針對結論闡述詳細的見解。 在開頭部分先痛快地推出結論，然後寫“為什麽會得出此結論”的理由，再寫具體事例和詳細內容來提高結論的說服力。 按照“結論→理由→具體事例”的順序寫的文章更能提高讀者的認可度。 結論優先型文章開頭就揭曉文章的結論（最重要的信息)。 推出結論後，接下來闡述的理由和具體事例也會令人過目不忘了。  幹枝葉:   遵循結論優先型的流程，就是按照“幹（結論）→枝（理由·依據）→葉（具體事例·詳細內容） ”的順序傳達。  精選資訊:摘要力:   並不是說所有的信息都要寫出來，而要為了讀者能夠正確地理解、認同，判斷哪些信息需要發表，哪些信息需要隱藏。 這個取舍和選擇是寫作者應具備的能力。 寫作者必須按照讀者易於理解的順序寫，而不是自己想怎麽安排就怎麽寫。  技法: 以結論引起興趣, 提供具有說服力的證據理由, 使用具體事例表達真實情感 使用技巧①用結論引起興趣:   結論優先型模板最典型的特點就是A部分，即“推出結論（傳達最重要的信息）”。 結論要簡單:開門見山式地亮出結論，可以提高讀者的理解度。 如果結論不易理解或過於錯綜覆雜，那麽讀者的理解度會下降。千萬不要寫得啰里啰唆，直截了當地傳達到位才是最重要的。 結論要有趣: 但是，這並不意味著只要簡潔且準確地傳達好就萬事大吉了（盡管在某些情況下僅作為傳達信息的目的使用）。為什麽這麽說呢？那是因為如果結論寫得一點吸引力都沒有，讀者根本不會有讀的欲望。 文章的魅力不是單由“寫法”決定的，而是由“內容+寫法”的完美結合決定的。其中“結論”是內容的核心部分，起著至關重要的作用。 推出結論時，不采用“不容易引起興趣的結論”，采用“容易引起興趣的結論”；不采用“過於平庸的結論”，采用“新穎的結論”。  使用技巧②提供足夠有說服力的理由或依據:   B部分的“理由·依據”為結論提高說服力起著重要的作用。下面①~③的句子中，你最認同的是哪一個呢？ 理由和依據是為了使開頭推出的結論更具有說服力。 因此，在闡述理由和依據時要充分考慮到，理由和依據是否“合情合理”。 在闡述理由和依據時，也要考慮插入「客觀事實」。如果能加上具有說服力的數據和資料的話，其邏輯性會更加突出（提高有效性） 個人的主觀感覺和判斷，作為支撐結論的理由和依據是比較弱的。 因為讀者會認為“這只能代表你個人的體驗（意見/判斷/感覺） ”而不認同你的觀點（根據結論的類型，某些情況下個人體驗也是可以說服讀者的）  使用技巧③　使用具體事例表達真實情感:   從B“闡述理由·依據”轉到C部分，C“闡述具體事例·詳細內容”要求闡述個人的體驗經歷和實際案例。 理由和依據要求具有一定的“邏輯性”，而具體事例要傳達情感（情緒）。 前者是方便讀者理解的必要信息，後者是為了讓讀者心服口服所需的支撐信息。 在C的“具體事例·詳細內容”, 引起讀者的“伴隨體驗（自己親身體驗別人的經驗） ”。 即使自己認為“結論+理由·依據”很清晰，讀者卻可能沒有完全明白。在這一點上，具有真實感的經驗之談和實際案例才會有滿滿的誘惑力。 想要提高自己寫的文章的說服力，就要有意識地插入具體事例，這要比講一堆大道理強很多。  用法:介紹文, 異議書, 簡歷, 聯絡文 例文①介紹文（博客文章）“個人色彩診斷的介紹”:   按照“結論→理由·依據→具體事例·詳細內容”的順序讀取的過程中加深“個人色彩診斷”知識的文章  例文②異議書（商務文件）“我反對”:   不管是讚同的文章還是反對的文章，理由都起著極其關鍵的作用。 理由合情合理，結論的說服力就會高；反之，則說服力就會下降。 結論優先型就是像這樣按照“結論→理由·依據→具體事例·詳細內容”的順序展現給讀者並獲得讀者的認可的. 當發表自己的見解或觀點時，結論優先型模板是非常有效的。  例文③PR文章（簡歷）“回答‘你的長處是什麽？’”:   填寫書寫空間有限的簡歷時，結論優先型模板是很有用的。如果先寫一堆沒用的廢話，空間會瞬間被填滿。 在刪減無用的語句之後，為了寫出具有說服力的文章，按照“結論→理由→具體事例”的順序寫是最理想的。 在商務場景中“結論在先”的表達方式很被看好。企業的面試官通過審核簡歷推斷應聘者的表達能力和傳達能力是很常見的事情。 因此，通過B部分的“闡述理由·根據”和C部分的“具體事例·詳細說明”，展示具有獨創性的經驗之談是很重要的。 像例文那樣舉出具體的經驗之談（活動成功案例），就可以證明結論是“真的”。 你想在眾人中脫穎而出，那麽取勝的關鍵就在於如何闡述“經驗之談”。  例文④聯絡文（商務文件）“會議內容的變更事項”:   這是一篇通知舉辦與以往不同的會議的文章，像這樣的聯絡文也可以使用結論優先型模板，每個部分中隱藏的問題如下：  在傳達這次不分組的意向（結論）的同時，說明了變更的理由。如果未談及其理由或理由不夠充分，則得不到讀者的認同。 正是因為是一篇聯絡事項的文章，所以需要在C的“具體事例·詳細內容”部分描述具體有哪些變更事項。 反之，如果沒有詳細內容的記載（為了促進交流而準備的具體方案），讀者可能還是會擔心“盡管是這種形式，恐怕也沒有太多的交流和互動的機會”。 而且，如果之前有過舉辦的經歷，那麽可以考慮在C的“具體事例·詳細內容”的部分插入之前的“成功經驗（具體案例） ”。 當了解到曾經有過成功案例，讀者就會產生“有過成功案例就放心了”的想法。  文法: 慣用表達, 客觀理由依據, 具體事例親身經歷, 自反駁讓步, 背景補充 立竿見影的技巧①結論優先型的常用短語: 立竿見影的技巧②將可靠的數據作為“理由、依據”: 立竿見影的技巧③具體事例中插入親身經歷: 編排①表明某種觀點時，加入對可預見反駁的回應:  編排②加入“背景”，提供有益信:   故事型模板-引起讀者共鳴 心法: 逆轉勝, 引發共鳴 什麽是“故事型”模板？:   故事型模板是指，以引起讀者共鳴為目的，將某個故事情節通過“連續劇形式”呈現出來的模板。 在開頭部分描述主人公的消沈狀態（不良狀態），接著描述是什麽改變了主人公、主人公具體發生了哪些轉變，是如何成長的，最後以快樂圓滿的結局收尾。 故事型模板可以用以下示意圖表示。  傳達信息為目的的商務場景, 不適合用故事型模板:   結論優先型適合取得共識 列舉型適合給詳細資訊  技法：個體困境社會困境, 多重轉機, 製造大落差 使用技巧①消沈狀態分個體性和社會性兩方面:   在A“闡述消沈狀態”中，打開心扉講述自己失敗的經驗、丟人的經歷和自卑的表現等，讀者更容易投入感情傾聽。 同樣是成功的經驗，如果不講述取得成功的詳細過程，那麽只能讓人認為你是在自吹自擂。當然，也不會引起讀者的共鳴，不容易被理解。 在談成功經驗時，建議以失敗點為起點，按照漸入佳境的趨勢寫故事。  使用技巧②思考不同的轉機:   B部分“闡述轉機”的內容（轉折點）可以寫人、物、作品、場所、發生的事情等各種各樣的內容。  使用技巧③制造大的“落差”:   故事中一定會存在落差的。在A“闡述消沈狀態”中描述的狀況，到D“展望未來”中發生了巨大變化。 為了引起讀者的強烈共鳴，“強烈的痛苦”和“巨大的喜悅”這兩個要素是必不可少的。  用法: 敘事方法故事, 敘事產品推銷, 讀書心得, 老人言當年勇 例文①敘事文（博客文章）“推薦三明治溝通法”:    實驗與因果\n 例文②敘事文（散文風格）“移動的便當屋”:   紀錄新體驗\n 例文③書評（博客文章）“《花錢買來的幸福感》書評”:   使用故事型模板寫的文章，並不是寫書中的內容，而是寫出通過讀這本書給自己帶來了怎樣的變化，以經驗之談為中心豐富文章的內容。 通過寫出不斷轉變、成長的經歷，很容易讓讀者心動、產生共鳴，從而對這本書感興趣的讀者會越來越多。 用故事型模板描述自身變化時，一定要縮小描述要點的範圍。 這篇文章是在各種學習和領悟的過程中鎖定了“對金錢的消極印象只不過是自己情感的投射”這一要點展開的。 如果要點過多，那麽每個要點都會被稀釋，讀者可能不太好理解，還有些讀者會因書中的信息量過大而中途放棄閱讀。 我想再強調一遍，信息量不在於多，而在於精。通過篩選出最值得講述的信息，讀者會更好地理解文章。   讀書心得\n 例文④寫給學生的寄語“為失敗而高興吧”:   適合講以前的經驗, 來改變後輩的行動   當年勇, 老人言\n 文法: 強大敵人深刻矛盾, 簡單易懂故事, 圓滿結局, 應用場景, 悲傷和自嘲,擔憂與成效  立竿見影的技巧①寫強大的敵人和深刻的矛盾:\n  立竿見影的技巧②講故事的基本原則是“簡單易懂”:\n  立竿見影的技巧③圓滿的結局更容易催釋多巴:\n  立竿見影的技巧④應用到商務場景中:\n  編排①在悲傷的故事中加入自嘲:   編排②寫軟文時加入“擔憂”和“成效”:\n   “軟文”是指，以銷售商品或服務為目的編寫的文章。 想在讀者看完“軟文”後勾起他們“想買”的欲望，那麽這篇文章必須動人心弦。 這是因為，消費者的購買欲望根據情感上下浮動。 能夠引起共鳴的故事，可以直接編排成軟文。   組合模板-套路中的套路 心法:組合更高的複雜度 模板是可以组合使用的:   到目前为止，我们介绍了“列举型”“结论优先型”和“故事型”这3种模板。 写长篇文章时，可以把这3种模板组合起来使用。 一般来说，列举型和结论优先型的文章容易影响左脑派（擅长逻辑性思考的人），故事型的文章则容易影响右脑派（情绪和感情丰富的人）。 如果能将这两种组合起来的话，那么无论是左脑派还是右脑派就都容易产生共鸣。 当然，模板不能盲目地组合使用，应充分考虑文章的目的、想要传达的信息、想留给读者的印象、各种模板所具备的效果后，选择最佳的组合方式。  技法: 故事+結論, 結論+故事, 結論+列舉+故事 模板组合①“故事型+结论优先型”:   首先，介绍一下故事型中插入结论优先型的样式，在故事型的“B：阐述转机”部分插入结论优先型模板。 动人心弦的“故事型”与提高理解能力和认可度的“结论优先型”相结合是能够引起大众兴趣和关注的方法之一。  模板组合②“结论优先型+故事型”:   同样的内容，可以采用结论优先型为主、故事型为辅的风格。 这时，相比之前逻辑性会更占优势。 文章的内容虽然基本相同，但是给人的印象是不同的。 将体验经历作为依据编入理论的“故事型+结论优先型”的文章更能打动人心，而将理论作为依据编入体验经历的“结论优先型+故事型”的文章更容易让人理解。 如果是发布到微博上的文章就应用前者，如果是登载到商务期刊的文章就应用后者，可以根据“文章的目的”“读者的属性”“传递的信息”区分使用。  模板组合③“结论优先型+列举型+故事型”:   这次介绍的是结论优先型中编入列举型和故事型的风格。 在B“阐述理由·依据”中编入列举型，在C“阐述具体事例·详细内容”中编入故事型。  用法: 列舉, 結論優先, 故事 列舉型:     傳達的事項較多時（例：商品有多個特征）    有多個疑問事項和確認事項時    整理並記錄談話的內容時    想讓讀者零壓力閱讀時    整理並傳達覆雜的信息時    介紹\u0026hellip;的流程時（例：\u0026hellip; 的做法/到\u0026hellip;路線)    歸納總結時    有諸多同類的信息時    從大量的信息中笑選重要信息時    做筆記時（例：感悟/知識/靈感/優勢/劣勢 ）    寫待辦事項時    結論優先型:     有一個非常重要的事項必須傳達到位時    針對一個主題進行深入探究時    有邏輯性地傳達事項時    想提高讀者的理解度時    想得到讀者的認同時    想提高說服力時    做 \u0026hellip; 的匯報時    做 \u0026hellip; 的說明時    商量 \u0026hellip; 時    通知 \u0026hellip; 時    提醒注意 \u0026hellip; 時    推薦 \u0026hellip; 時    寫小論文或報告書時    故事型:   1.想引起對方共鳴時 2.想給對方留下深刻印象 3.想讓對方投入感情時 4.講述 \u0026hellip; 的經驗之談時 5.想吸引對方關注時 6.寫故事梗概時（例：廣告構思、視頻簡介） 7.推銷產品或服務時 8.招攬顧客時 9.編寫公司簡介時 10.編寫童話故事和小說時 11.編寫淺顯易懂的舉例說明的文章時 12.請求對方做某事時 13.做 \u0026hellip; 的方案或策劃時  後記  高竿的學術文章, 整個文章是個故事型, 標題小標題是列表型, 具體段落是開門見山型.\n  到此終於把模板寫作法的內容做複製重新編碼. 平時紀錄資訊用「列表」, 寫論文要說服人用「結論優先」,講故事與人聊天改變對方行為用「故事」.\n  左腦是理性, 右腦是感性. 寫證明時兩種都用, 寫paper時兩種都用, 高級的溝通能力, 需要實踐寫作與編輯於每一天, 每一份遭遇的資訊與文本. 天天向上, 共勉之！\n  2022.01.23. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-24","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur026-%E8%AE%80%E5%B1%B1%E5%8F%A3%E6%8B%93%E9%83%8E%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%AF%AB%E4%BD%9C%E6%B3%95/","series":["每日文章"],"tags":["山口拓郎"],"title":"MUR026 讀山口拓郎的模板寫作法"},{"categories":["語言學"],"content":"思考俄語第三格  紫式晦澀每日一篇文章第25天\n 前言   今天是2022年第23天, 全年第3週, 一月的第四個週日. 今天來系統思考俄語的「與格 (Dative case)」. 三年前有從語言學的角度瞎子摸象各種相關知識. 今天利用俄語, 再來實踐知識成為自己的智慧.\n  今天的素材主要來自各種網路文章\n   Dative case-Wiki  Russian Dative Case-Russian Language Lesson 13  THE DATIVE CASE IN RUSSIAN  The Dative Case in Russian: Usage and Examples  How To Master The Russian Dative Case Without The Stress   心法: 動作的接受者 定義: 行為的接受者或受益者:   表達「行為的接受者或受益者(the recipient or beneficiary of an actio)」 英語中的「間接受詞 (Indirect Object)」  問句: 給誰? 為了什麼?   To whom? кому To what? чему  介系詞: to的思維-向誰, 往哪邊:   有兩種可以to的對象 give to: to indirect objects 向間接受詞 go to: to directions of movement 往哪邊去  俄語與格: 到誰那邊:   在俄語, 與格表達「動作的間接受詞 (the indirect object of an action)」 動詞一般來說接「受格 (Accusative)」, 但在「人是動作的目的 (a person is the goal of motion)」時, 會使用「與格 (Dative)」. κ + destination in dative case К врачу, meaning \u0026ldquo;to the doctor.\u0026rdquo;  俄語與格: 沿著某客體:   與介系詞搭配, 可以表達其他想法. 沿著: 當 по 表達「沿著 (along)」, 那他的「客體(Object)」使用與格. As in По бокам, meaning \u0026ldquo;along the sides.\u0026rdquo;  與格的哲學:   State of a Subject (Emotional or Physical): 感覺冷熱快樂有趣開心無聊 Direction: к 往, по沿著  技法: 代名詞, 名詞, 形容詞變格 人稱代名詞與格:  所有代名詞與格:   形容詞單數與格:   名詞與格:   用法: 動詞, 謂詞, 介系詞 動詞 Давать (Give; 給):   Иван даёт цветы Анне - Ivan gives flowers to Anna. 他給花 Я даю цветы Анне - I give flowers to Anna. 我給花 Они дают цветы Елене - They give flowers to Elena. 他們給花 Дайте мне \u0026hellip; - Give me \u0026hellip; 給我 Дайте мне ваш адрес и номер телефона. - Give me your address and telephone number. 給我你的住址與電話號碼  Помогать (To give help to; 幫助):   Я помогаю Анне - I am helping Anna. 我幫助女生 Я помогаю Ивану - I am helping Ivan. 我幫助男生 Мы помогаем маме каждый день - We help mother every day. 我們幫助媽媽每天 Я помогаю этой девушке - I am helping this girl. 我幫助這個女孩 Нужно помочь маме. 需要幫忙媽媽.  Звонить (To make a phone call to; 打電話給):   Она звонит Ивану каждый день - She calls Ivan every day. 她打給他 Иван звонит Анне каждый день - Ivan calls Anna every day. 他打給他  нравится (單數物品令人喜歡):   Мне нравится Москва - I like Moscow. (lit: Moscow is pleasing to me.) 我喜歡 Ей нравится Москва - She likes Moscow. (lit: Moscow is pleasing to her.) 她喜歡 Ему нравится Москва - He likes Moscow. 他喜歡 Вам нравится Москва - You like Moscow. 你們喜歡 Адаму нравится Москва - Adam likes Moscow. Adam喜歡 Как вам нравится \u0026hellip;? - How do you like \u0026hellip;? 你喜歡怎樣的? Как вам нравится фильм? - How do you like the film? 你喜歡怎樣的電影? Вам нравится Москва? - Do you like Moscow?你喜歡莫斯科嗎? Олегу не нравится моя собака (Oleg doesn’t like my dog) 他不喜歡我的狗  нравятся (複數物品令人喜歡):   Мне нравятся дети - I like the children. (lit: children are pleasing to me.) 我喜歡小孩 Мне нравятся ваши дети - I like your children. 我喜歡你的小孩 Мне нравятся цветы - I like the flowers. 我喜歡花 Мне очень нравятся цветы - I really like the flowers. 我很喜歡花 Ане нравятся блюда (Anya likes the dishes)她喜歡這些菜餚.  нравится (動作令人喜歡):   Мне нравится гулять (I like to go for walks) 我喜歡去散步  謂詞 Dative+ холодно/жарко/скучно 感覺冷熱無聊:   Мне холодно - I am cold (to me it’s cold) 我感覺冷 Мне жарко - I am hot (to me it’s hot) 我感覺熱 Ему холодно - He is cold (to him its cold) 他感覺冷 Ей холодно - She is cold (to her its cold) 她感覺冷 Нам холодно - We are cold (to us its cold) 我們感覺冷 Мне скучно - I am bored (to me its boring)我感覺無聊 Зрителям было скучно. - The audience was bored. 受眾感覺無聊 Что-то мне сегодня плохо думается. 今天不知為何我很難思考. 👻  年齡:   Мне восемнадцать лет - I am 18 years old (to me there are 18 years) Мне двадцать три года - I am 23 years old. Ребенку три года. 小孩三歲. Сколько вам лет? 你幾歲? Мне 30 лет. 我三十歲  Можно 可能:   Можно мне завтра прийти (Tomorrow I can arrive at work a bit earlier.) 明天我可以早點到工作.   Нельзя 禁止:\n  Надо/Нужно 應該/必須:\n   Нам нужно работать сегодня (We need to work today) 我們今天必須工作 Борису надо поехать в Москву (Boris has to go to Moscow) Boris必須去莫斯科.  介系詞 к 往(towards/to):   定向動詞 Я иду к вокзалу - I am walking towards the station. 我走去車站 Мы едем к друзьям - We are going to our friends. 我們去朋友那邊 Они едут к бабушке в деревню. They are going to their grandma\u0026rsquo;s in the country. 他們往他們奶奶家去. готовится к экзаменам (He’s getting ready for the exam) 他準備好考試了. Завтра я иду к врачу (I’m going to the doctor tomorrow) 明天我去看醫生 Я хочу идти в гости к Юлии (I want to visit Yulia) 我想拜訪Yulia.  по 沿著(along):   不定向 Идти по дороге. To walk on the road/down the road. 沿著路走 Мы гуляем по набережной. We are walking down the seafront. 我們沿著海提走. Мы шли по полям. 我們沿著這片區域走過. Летать по небу (To fly through the sky.) 飛過天空 Мы гуляли по улице (We are walking down the street/along the street) 我們走過街口.  по 藉著(via):   Говорить по скайпу (To talk through Skype/by Skype) 藉著Skype交談 Посылать по почте (to send by mail/by post) 藉著郵政寄信  по 原因(Cause):   По какой причине вы отказались? (Why (For what reason) did you refuse?) 為何你當時拒絕? По ошибке подписал этот бланк (By mistake he signed the form) 他錯簽了表格.  по 重複時間(Time for Repeating Actions):   По вечерам (in the evenings/at night) 每天晚上 По средам (on Wednesdays) 每週三 По червергам (on Thursdays) 每週四  Благодаря 多虧(Thanks to):   Благодаря вашей помощи (Thanks to your help) 謝謝你的幫忙  Согласно 根據(According to/in accordance with):   согласно статье 101 (According to Article 101) 根據文章101.  後記  到此系統思考了俄語與格的各種資訊. 用一個文章收集資訊的好處就是整理自己的體系. 第三格可淺可深, 之後可以用續寫文章的方式持續更新寫過的東西.\n  今天上課跟Siqi問一問, 得到的方向是,之後研究定向不定向動詞(Motion Verb), 來仔細思考介系詞背後的哲學. 天天向上, 共勉之！\n  2022.01.23. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-23","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur025%E6%80%9D%E8%80%83%E4%BF%84%E8%AA%9E%E7%AC%AC%E4%B8%89%E6%A0%BC/","series":["每日文章"],"tags":["俄語"],"title":"MUR025 思考俄語第三格"},{"categories":[],"content":"思考調式與風格  紫式晦澀每日一篇文章第24天\n 前言   今天是2022年第22天, 全年第3週, 一月的第四個週六. 今天來持續累積音樂的知識, 建立自己的知識體系. 今天來思考「調式 (Mode)」.\n  今天的素材主要來自各種網路文章\n   什麼是調式 ? 一分鐘搞懂7種自然調式的音階與風格介紹~  【乐理101】6. 音乐里的楼梯  什麼是大小關係調與平行調? 1篇學會找出大小調的關係大小調  小调   音階與調式 (Scale and Mode) 調式是音階的類型 音階(Scale):   八度中選取任意一組音，它們的音高由低到高依次排列就能組成一條「音階(Scale)」。 音階就像是樓梯，音階中每個音都比它前面的那個音要高出一個台階。 每級台階的高度不一定均勻，不同音階的「形狀」也因此各不相同。 大調音階的音程組合是：全全半全全全半，在全音程組合中摻雜著兩個半音程. 小調音階的音程組合是: 全半全全半全全，與大調音階形狀就有所不同。   幾個音都可以是音階:音階不需要包括所有十二個音，它可以只包含八個音，三個音，甚至只有一個音也叫音階。\n  自然音階: 雙全音群+半音+三全音群:\n   像鋼琴鍵盤白鍵這樣，一個雙全音群和一個三個全音的群，兩個群由半音銜接在一起，這樣組成的音階就叫做自然音階。 自然音階有七個不同的音，因此屬於七聲音階。由 於八度的循環性，自然音階中的任意一個音作為主音向上數都能發展出一種新的調式音階。   七聲音階: 像大調這類有七個獨立的音的調式音階就叫做七聲音階。\n  音階主音:音階中最「穩定」的一個音叫做主音。\n   何謂穩定?\n 音階音級:   以主音為第一級，音階中每節台階都有自己的編號，叫作音級。 音級體現了每個音與主音之間的關係，它也可以用來衡量音與音之間的音程距離。 在大調中，主音Do的音級數字是1，Re的音級是2，Mi是3，以此類推。 而平均律中，非大調音階的音級可以在數字前標記升降號來表示，比如降Mi是b3。  音階調式:音階的類型叫做調式(mode)。   從主音出發，特定的音程組合形成特定的調式類型，如大調，小調，或下文會介紹的Dorian等。 每一類調式都具有固定的音階「形狀」，「台階數量」，以及獨特的聽覺色彩。 調式的名稱是由「主音的音名」搭配「調式類型」構成的，比如C Ionian或D Dorian。  調式是不同的音程排列 調式是不同的音程排列:   調式就是一種音與音之間排列順序的『固定模組』 「不同調式都擁有屬於自己不同的音程排列模式」 知道「調式的模組」後, 就能判斷屬於哪種「調式」  大調:   音階只要符合【全-全-半-全-全-全-半】的「固定排列模組」 C大調, 主音Do, 「1 – 2 – 3 – 4 – 5 – 6 -7 」 D大調, 主音Re, 「2 – 3 – 升4 – 5 – 6 – 7 – 升1 」  調式用於音樂形式的風格 調式的感覺與風格:   「調式的不同」跟「曲子」的「演奏與音樂風格」有著極大關係 音樂形式: 現代流行音樂, 獨奏, 爵士, 即興 大小調常在現代流行音樂出現 其餘調式在獨奏, 爵士, 即興常出現  自然調式, 中古教會調式 (Gregorian mode, church mode) 中古教會調式:   自然調式也稱「中古教會調式」，之所以被稱為「自然調式」，就代表是自然的也沒有經人為變化的調式，並且由C大調音階的各個音，為主音開頭的白鍵音階； 另外一種就是人為特意變化而成的變化調式，人為的變化調式種類相當複雜繁多，而自然調式相對於較為簡單且直白.  Do - 開心快樂: Ionian(艾奧尼安調式) - 大調 Do - 開心快樂: Ionian(艾奧尼安調式):   即是我們平常最常見的「大調」 聽起來開心愉快，並具有活潑開朗的音樂風格 最適合在流行音樂當中，也是大眾最常使用的調式   「Ionian(大調)」的調式就是以『Do』開始往上經過7個音後到達高八度的『Do』所形成音與音之間的排列模式(上圖) 固定模組為「全-全-半-全-全-全-半」 不管從哪個音(主音)開始到高八度的音，只要每個音之間的距離是「全-全-半-全-全-全-半」就是一個「Ionian(大調)」  Re- 神秘神聖: Dorian(多利安調式) Re- 神秘神聖: Dorian(多利安調式):   「Dorian」是僅次於，在大調與小調之後最常被使用的調性 「Dorian」跟小調音階的差異就是該調式的第6音是大6度音，而小調是小6度音，所以雖含有小調的特性 因為第6音的不同的關係，更添加了一股神祕感與神聖感   「Dorian 」的調式就是以『Re』開始往上經過7個音後到達高八度的『Re』所形成音與音之間的排列模式(上圖) 固定模組為「全-半-全-全-全-半-全」 不管從哪個音(主音)開始到高八度的音，只要每個音之間的距離是「全-半-全-全-全-半-全」就是一個『Dorian 』調式 主音換作為『Do』所形成的『Dorian 』調式為「1 – 2 – b3 – 4 – 5 – 6 – b7」  Mi - 金屬搖滾: Phrygian (弗利吉安調式) Mi - 金屬搖滾: Phrygian (弗利吉安調式):   光聽完音階後，會有西班牙異國風情的感覺，而現在使用上，常常把音階上的第3個音，升高半音變成「Phrygian Dominant」的親戚調式 很多西班牙與多佛朗明哥的音樂都是在此調式發展，並且因為「Phrygian」調式特色聲音豐富擁有色彩 因此也常常出現在金屬樂、搖滾樂當中。   「Phrygian 」的調式就是以『Mi』開始往上經過7個音後到達高八度的『Mi』所形成音與音之間的排列模式(上圖) 固定模組為「半-全-全-全-半-全-全」 不管從哪個音(主音)開始到高八度的音，只要每個音之間的距離是「半-全-全-全-半-全-全」就是一個『Phrygian 』調式 主音換作為『Do』所形成的『Phrygian』調式為「1 – b2 – b3 – 4 – 5 – b6 – b7 」  Fa- 科技浩瀚: Lydian (利地安調式) Fa- 科技浩瀚: Lydian (利地安調式):   此調式的主要特色在於除了音階的｢增4度｣音之外，其餘的音跟大調一樣，所以也有大調活潑開朗的特性 因為｢增4度｣音添加了魔法般的聲音效果，變得可以滑稽、科技感，神秘感，浩瀚的感覺，讓聽覺上有更多不同的感受   「Lydian 」的調式就是以『Fa』開始往上經過7個音後到達高八度的『Fa』所形成音與音之間的排列模式(上圖) 固定模組為「全-全-全-半-全-全-半」 不管從哪個音(主音)開始到高八度的音，只要每個音之間的距離是「全-全-全-半-全-全-半」就是一個『Lydian 』調式 主音換作為『Do』所形成的『Lydian 』調式為「1 – 2 – 3 – #4 – 5 – 6 – 7 」  So - 藍調爵士: Mixolydian (米克索利地安調式) So - 藍調爵士: Mixolydian (米克索利地安調式):   其調式最特別的關鍵音在於「Mixolydian」調式音階的小7度音 非常適合拿來彈奏藍調、爵士等的音樂風格   「Mixolydian」的調式就是以『Sol』開始往上經過7個音後到達高八度的『Sol』所形成音與音之間的排列模式(上圖) 固定模組為「全-全-半-全-全-半-全」 不管從哪個音(主音)開始到高八度的音，只要每個音之間的距離是「全-全-半-全-全-半-全」就是一個『Mixolydian 』調式 主音換作為『Do』所形成的『Mixolydian 』調式為「1 – 2 – 3 – 4 – 5 – 6 – b7 」  La - 憂鬱悲傷: Aeolian (伊奧利安調式) - 小調 La - 憂鬱悲傷: Aeolian (伊奧利安調式):   即為我們最常說的「小調」 音樂風格表現出憂鬱悲傷的感覺，是跟大調一樣非常常用的一種調式   「Aeolian (小調)」的調式就是以『La』開始往上經過7個音後到達高八度的『La』所形成音與音之間的排列模式(上圖) 固定模組為「全-半-全-全-半-全-全」 不管從哪個音(主音)開始到高八度的音，只要每個音之間的距離是「全-半-全-全-半-全-全」就是一個『Aeolian (小調)』調式 主音換作為『Do』所形成的『Aeolian (小調) 』調式為「1 – 2 – b3 – 4 – 5 – b6 – b7 」  Si - 恐怖懸疑: Locrian (洛克利安調式) Si - 恐怖懸疑: Locrian (洛克利安調式):   此調式的最大特色就是聽起來糾結、黑暗 常常運用在恐怖片、懸疑片當中，製造出相當沉重且瀰漫著死亡與暗黑的氣息   「Locrian」的調式就是以『Si』開始往上經過7個音後到達高八度的『Si』所形成音與音之間的排列模式(上圖) 固定模組為「半-全-全-半-全-全-全」 不管從哪個音(主音)開始到高八度的音，只要每個音之間的距離是「半-全-全-半-全-全-全」就是一個『Locrian』調式 主音換作為『Do』所形成的『Locrian 』調式為「1 – b2 – b3 – 4 – b5- b6 – b7 」  調性音樂 (Tonal Music) 調性音樂(Tonal Music): 大調小調壟斷西方音樂 調性音樂-大調小調壟斷西方音樂:   公元1600-1900年在西方音樂史上被稱為「共性寫作時期」，時間貫穿巴洛克，古典主義，浪漫主義三個重要音樂時期。 一般大眾對古典音樂的理解都來源於共性寫作時期的作品。 在共性寫作時期的的作品中，大調與小調兩種調式類型確立了壟斷地位。 古典音樂家們在實踐過程中經過總結，建立出了以大調與小調為模板框架的西方傳統音樂體系—\u0026ndash;調性音樂(tonal music)。 現代流行樂也普遍建立在調性音樂之上。  調(Key): 調性音樂的24種基礎寫作模板- 12主音 x 大小調 調(Key): 調性音樂的24種基礎寫作模板- 12主音 x 大小調:   由12個不同的主音音名搭配大調或小調調式可以形成的調性音樂中的24種的基礎寫作模板框架，稱之為「調」(key)。 在某一個調上的樂曲，它所用到的音將主要來自於這個調的音階。 比如以C大調來定調的作品，主要是以C大調音階CDEFGAB這七個音作為基礎創作的。 而十二平均律中的另外五個音出現的頻率則低很多，作品中偶爾會穿插這些調外音以起到特定的過渡或驚喜作用。  調的命名: 主音音名 x 調式類型:   「調」的命名方式同調式一樣，由主音音名與調式類型構成，如C大調或a小調。 C 是主音音名, 大調是調式類型. 調名中所說的調式類型特指大調或小調。 一般習慣上，大調的調名中的英文字母為大寫，而小調調名中的英文字母為小寫。 調名中如果沒有標明「大」或「小」，則默認為大調，如\u0026quot;G調\u0026quot;等同於「G大調」。  調性音樂(Modal music): 用大小調來造出的音樂體系 「調性」(tonality): 模板名字:   「調性」(tonality)這個詞具有多重意思。 當我們說一首音樂作品具有調性的時候，往往意思是說這個作品具有明確的主音位置 根據主音的位置，我們可以聽出調中每一個音的功能以及它們與主音的位置關係。  調性音樂(Modal music): 用大小調來造出的音樂體系:   不過，有調性的音樂與「調性音樂」的意思並不相同。 「調性音樂」特指共性寫作時期所使用的基於大小調組織起來的音樂體系。 除了調性音樂體系，調式音樂(modal music)也是具有調性的。調式音樂使用的是非大小調的調式音階，它也可以通過有規律地組織音高與和聲形成明確的主音位置。 但由於調式音樂不具備大小調特有的和聲功能，寫作模式上與調性音樂不一樣（見本講的課後拓展）。 因此調式音樂與調性音樂，雖然都具有調性，但是在概念上是兩個概念。  無調性音樂(atonal music):離開古典音樂:   到了十九世紀末，傳統的調性音樂已經被挖掘到了極致，大量調外音的使用使得調性發生鬆動。 現代音樂家為了拓展音樂的可能性，甚至開始通過專業技巧故意使主音位置聽感上難以分辨，從而徹底模糊了樂曲的調性——這類音樂體系叫做無調性音樂(atonal music)​。  調是名字, 調性是性質:   日常用語中，「調性」常常和「調」發生混淆，會被用來討論調的主音位置，或是區分這個調是大調還是小調。 「調性」偶爾也被用來描述調式的聽感 ，如「Dorian調式的調性」。 此時這個詞的適用範圍就超出調性音樂的範疇，不僅限於大小調了。  後記  到此思考整理了音階與調式的相關知識. 主要了解了中古教會調式, 共有七種, 學習其音程的安排. 實際上就是「五全二半」的不同排列組合. 全全半全全全半, 然後選起始點.\n  之後可以繼續討論「大小調轉換: 關係調與平行調」以及「小調音階: 自然(關係, 平行), 和聲, 旋律, 現代」.\n  這次系統了解了曲式, 非常棒！感覺有抓到音樂學習的節奏！一次一次學習, 慢慢建立音樂的系統！天天向上, 共勉之！\n  2022.01.22. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-22","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur024%E6%80%9D%E8%80%83%E8%AA%BF%E5%BC%8F%E8%88%87%E9%A2%A8%E6%A0%BC/","series":["每日文章"],"tags":["音樂"],"title":"MUR024 思考調式與風格"},{"categories":["合成數據"],"content":"鳥瞰金融風險管理與合成數據  紫式晦澀每日一篇文章第23天\n 前言   今天是2022年第21天, 全年第3週, 一月的第三個週五. 今天接到新的家教需求, 關於金融方面的機器學習. 今天藉著文章來思考一下「金融風險管理」相關的機器學習技能. 把書上的知識很快實踐, 標準化成為結果, 是很重要的能力! 因此, 要去了解與Python相關的金融風險管理機器學習任務.\n  今天的素材主要來自Machine Learning for Financial Risk Management with Python 裡面的目錄來建立初步的知識體系.\n  本書十章節 目錄概覽:   Ch01. 主要「風險管理(Risk Management)」的概念. Ch02. 傳統模型「時間序列應用 (Time-series applications)」. Ch03. 實踐序列建模的「深度學習工具(Deep learning tools)」. Ch04. 波動率指數預測(Volatility prediction) Ch05. 提升傳統「市場風險模型(Market Risk models)」估計表現的「機器學習模型 (Machine learning-based models)」 Ch06. 估計「信用風險 (Credit Risk)」的機器學習方法. Ch07. 「流動性(Liquidity)」建模, 以高斯混合模型(Gauddian Mixture model). Ch08. 操作風險(Operational Risk) Ch09. 公司治理風險 (Corporate Governance Risk) Ch10. 利用「合成數據(Synthetic Data)」來估計各種不同的金融風險.  時間序列傳統模型:   移動平均模型 (Moving Average) 自回歸模型 (AutoRegressive) 自回歸移動平均模型 (Autoregressive Integrated Moving Average)  市場風險模型(Market Risk models):   風險價值模型 (Value-at-Risk, VaR): 量化基於市場移動所造成的潛在損失. 對共變矩陣除噪音. 條件風險值模型 (CVaR, Expected Shortfall): 大的為預期的損失. 結合資料的流動性維度(Liqudity dimension)  第十章內容 背景:   由於對保密性和數據要求不斷增加的擔憂，合成數據的生成在金融領域越來越受到關注。 那麼，為什麼不使用真實數據，而要用合成數據來餵養你的模型，只要它能模仿必要的統計屬性？這聽起來很吸引人，不是嗎？ 合成數據的生成是本章的一部分；另一部分是關於另一個未受重視但相當重要和有趣的話題：隱馬爾可夫模型（HMM）。 你可能會問：合成數據和HMM之間有什麼共同點？好吧，我們可以從HMM中生成合成數據，這是本章的目的之一。 另一個目的是介紹這兩個重要的主題，因為它們在機器學習中經常被使用。  合成數據生成 合成數據對金融產業的幫助:   金融數據的保密性、敏感性和成本大大限制了其使用。這反過來又阻礙了金融領域有用知識的進展和傳播。合成數據解決了這些缺點，幫助研究人員和從業人員進行分析和傳播結果。  產生合成數據有三種方法:   合成數據是由「模仿了真實數據的統計特性」而產生。 即使有一種觀點認為，數據必須以其原始形式進行建模，但從真實數據中生成合成數據並不是我們可以創造它的唯一方式（Patki, Wedge, and Veeramachaneni 2016）。 相反，我們有三種方式可以生成合成數據。  方法一: 基於真實數據:   合成數據可以從真實數據中生成。 這個過程的工作流程是從獲得真實數據開始的，然後繼續建模以揭開數據的分布，最後一步是從這個現有的模型中抽出合成數據。  方法二: 基於模型:   合成數據可以從一個模型或知識中獲得。 一般來說，這種類型的合成數據生成可以通過使用現有模型或研究者的知識來應用。  方法三: 基於混合:   一個混合過程包括前面兩個步驟，因為有時只有一部分數據變得可用，這部分真實數據被用來生成合成數據，另一部分合成數據可以從模型中獲得。  隱私與效用:   我們很快就會看到我們如何應用這些技術來生成合成數據。 就其性質而言，合成數據生成過程在隱私和效用之間有一個不折不扣的權衡。 確切地說，從真實的未披露的數據中生成的合成數據會產生高效用。 然而，合成數據生成的效用在很大程度上取決於真實公共數據的去身份化和聚合。 合成數據生成的效用取決於成功的建模或分析人員的專業知識。  衡量合成數據品質 四種衡量方法: KL散度, 可區分, ROC曲線, 統計量:   正如你所想象的，可以應用各種工具來衡量合成數據的有效性；然而，我們將把注意力限制在四個普遍接受的方法上。 KL-發散、可區分、ROC曲線，以及比較主要的統計數據，如平均值、中位數等。 由於KL-發散和ROC分別在第8章和第6章討論過，我們將跳過這些，從可區分方法開始討論。  可區分 (Distinguishable):   可區分法，顧名思義，試圖通過使用區分真實和合成數據的分類模型，在真實記錄和合成記錄之間進行區分，如果是真實的，就給1，如果不是就給0。 如果輸出結果更接近於1，它就預測該記錄是真實的，否則就使用「傾向得分(Propensity Score)」預測它是合成數據（El Emam 2020）。  統計量:   另一種方法簡單而強大，是基於比較真實和合成數據的主要統計數據。 鑒於所採用的模型，可以比較真實數據和合成數據的平均值（或其他統計量），以瞭解合成數據對真實數據的模仿程度。  合成數據的優勢與劣勢 優點一:增加數據的可用性:   合成數據的生成為我們提供了一個強大的工具，我們可以通過它來克服獲取真實數據的困難，因為真實數據可能是昂貴的和專有的。  優點二:提高分析能力:   合成數據作為真實數據的一個很好的代理，可以用於各種分析過程，這反過來又可以提高我們對特定主題的理解。 此外，合成數據可用於標記，為高度精確的分析鋪平道路。  優點三:處理常見的統計問題:   合成數據的生成可以緩解真實數據產生的問題。 真實數據可能伴隨著一些問題\u0026ndash;如缺失值、離群值等，嚴重影響模型的性能。 合成數據提供了一個應對這些統計問題的工具，從而使我們最終可能獲得更好的建模性能。  缺點一: 無法保持機密性:   由於網絡攻擊，合成數據可能成為私人信息洩露的來源。例如，客戶的證書可以通過反向工程獲得。  缺點二:質量問題:   在合成數據的生成過程中，有兩件重要的事情需要考慮：研究人員的能力和數據的特點。 這兩點決定了合成數據生成的質量過程。 如果缺乏這兩點，很可能會期待低質量的合成數據。  生成合成數據的手段 CTGAN:     real data: fetch_california_housing    library: CTGANSynthesizer    1from ctgan import CTGANSynthesizer 2 3ctgan = CTGANSynthesizer(epochs=10) 4ctgan.fit(california_housing_df) 5synt_sample = ctgan.sample(len(california_housing_df)) 描述統計量來檢查合成數據的相似性:   生成合成數據後，可以通過描述性統計來檢查合成數據的相似性。 像往常一樣，描述性統計是很方便的，但是我們還有一個工具，就是來自合成數據庫（SDV）的軟件包。 這個函數的輸出將是一個介於0和1之間的數字，它將表明兩個表的相似程度，0是最差的，1是可能的最佳分數。 此外，生成過程的結果可以被可視化（在生成的圖10-2和10-3中），並與真實數據進行比較，這樣我們就可以充分瞭解合成數據是否能很好地代表真實數據。   利用真實數據可以很好產生合成數據.\n 由模型產生合成數據:   用sklearn來做分類與迴歸分析模型. 用make_regression可從迴歸分析模型來產生合成數據. 用make_classification從分類模型來產生合成數據  由無監督學習產生合成數據:   用make_blobs來做合成數據.    在Bandit裡面, 一開始也都沒data, 可否用合成數據來幫助bandit的學習任務呢? 從real data出發?\n   後記  到此思考整理了Machine Learning for Financial Risk Management with Python 裡面的目錄, 了解與Python相關的金融風險管理機器學習任務.\n  這個書的第10章竟然就講了合成數據！這個對我們高度相關, 要仔細讀讀. 讀完之後感覺這是一個很好hands on的材料！之後可以持續發展！另外文章也有提到一些文獻, 之後可以逐漸調查.\n  之後做Data-centric的研究, 感覺是正確發展Data science的方向！如同Computer science是研究計算機的科學, Information Science是研究資訊的科學, Statistics是「科學可證偽性」的基礎! 持續向上, 共勉之！\n  2022.01.21. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-21","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur023%E9%B3%A5%E7%9E%B0%E9%87%91%E8%9E%8D%E9%A2%A8%E9%9A%AA%E7%AE%A1%E7%90%86%E8%88%87%E5%90%88%E6%88%90%E6%95%B8%E6%93%9A/","series":["每日文章"],"tags":["金融科技"],"title":"MUR023 鳥瞰金融風險管理與合成數據"},{"categories":[],"content":"文獻綜述評的九個任務  紫式晦澀每日一篇文章第22天\n 前言   今天是2022年第20天, 全年第3週, 一月的第三個週四. 今天來用文章系統思考「文獻綜述」這個技術. 要能夠寫更好的研究文章, 需要掌握文獻綜述的技術. 這個部分在以前我一直是缺乏的, 總是在最後關頭胡亂寫寫. 要往下一個層次前進, 要能標準化做文獻綜述的工作流, 輸出才能更快地動起來.\n  今天的素材主要來自郭澤德-寫好論文 裡面的「4.2節: 文獻綜述的執行流程」.\n  文獻綜述評的九個任務: 檢索, 閱讀, 篩選; 分類, 歸納, 記述; 分析, 批評, 建構 輸入-處理-輸出結構:同樣我們可以使用IPO框架來標準化「文獻綜述工作流」.   輸入: 檢索, 閱讀, 篩選 處理: 分類, 歸納, 記述 輸出: 分析, 批評, 建構     IPO框架 主要環節 關鍵動作一 關鍵動作二 關鍵動作三     輸入 綜 檢索 閱讀 監選   處理 述 分類 歸納 記述   輸出 評 分析 批評 建構    輸入: 綜 - 檢索, 閱讀, 篩選 第一步-綜: 文獻綜述第一個環節. 按照研究選題, 「檢索」, 「閱讀」, 「篩選」相關的文獻.  I. 檢索- 選題探索, 寫作支持, 修改補充 文獻檢索的三種階段-選題, 寫作, 修改:   選題階段: 嘗試性, 探索性檢索, 啟發驗證研究思路 寫作階段: 正式, 系統性檢索, 獲得支持內容的材料 修改階段: 定點, 查漏補缺式檢索, 獲得補充性文件.   沒想到文獻檢索階段還能這樣子分類. 不過這也是很有道理, 寫了10篇文章後感受檢索文獻也需要系統性的方法來做, 才可以很有效率完成大量任務.\n  我個人的資訊來源:: 我自己平常的做法, 是從google scholar與proceeding來找\n     Google scholar 找相關的文章.    Semantic scholar 找引用的關聯.    Machine Learning Proceeding 找近兩年熱點好刷的文章.    找認識的學者給建議.    II. 閱讀- 文字, 結構, 邏輯, 脈絡, 思想 文獻五步閱讀法: 實際操作中, 文件檢索與文獻閱讀常常是交叉進行. 找到要投入時間的文獻後, 可實踐「文獻五步閱讀法」     讀文字-識記    讀結構-拆解    讀邏輯-理解    讀脈絡-發展    讀思想-發展     我個人的經驗中, 從字, 句, 段落, 小節脈絡, 文件體裁一步步探索慢慢累積上來. 現在即將拿到博士, 這五年的工作經驗讓我現在輸出非常輕鬆有系統, 雖然也就因此得到的更大的煩惱, 但要達到「著作等身」的兒時夢想, 正在努力的路上!\n  讀文字-識記:\n   要能欣賞「字」為何是擺在這樣的位置, 思考作者的意圖. 「詞語」是字的最小組合, 要特別注意文章中詞語的特定用法 「概念」使人類榮感性認識到理性認識, 抽象出事物的本質特徵, 加以概括. 是人類認知的思維體系中, 最基本的購族單位, 成為互相交流的基本手段. 累積學術概念, 使用最嚴謹最科學最不下億引發歧義的語言系統, 學習累積.  金句思維:   「句子」: 詞語組成最小的完整表意單位. 讀文獻要收集「金句」也就是經典的句子 寫句子策略一: 累積自己的積累, 累積思想深度, 下筆如有神 寫句子策略二: 積累文獻中的金句, 分析句子結構, 應用時保留句子結構, 替換自己的內容.   金句思維是學術寫作非常重要的mindset! 之前想通金句思維後, 寫作就提升了很多層次, 也可以看見文章更深層的遊戲!\n 段落閱讀:   閱讀段落時, 要注意段落內部, 句子結構和段落之間的關係. 段落內部句子之間, 理想上是一個完整邏輯結構, 尤其段落開頭與結尾的句子. 「關鍵句(Key-line)」是學術論文寫作常用的段落結構. 第一句或最後一句, 簡明扼要概括該段落主題, 起到引導閱讀的作用.  文獻筆記習慣:   筆記記錄文獻核心要點 引導閱讀 系統整合: 透過文章的主題分類與管理, 對同一主題論文進行歸類篩選. 建立宏觀聯繫, 理解主題脈絡, 成為文獻寫作基礎. 方便引用: Community內的經典引用, 要累積自己的列表.  讀結構-拆解:   論文結構分析順序: 從大到小, 整體結構到段落結構. 優秀論文特點: 每句話都應該有明確的結構安排意識 「結構意識」是研究者提升學術寫作能力的有效方法 結構意識, 將學術寫作轉為「填空題型」.  篇章佈局:   講究框架結構, 利用細化結構做meta data 根據下圖, 有六個框架, 分別有 4, 4, 4, 6, 3, 6共有27個系化結構.  讀邏輯-理解:   論證結構基本要素: 論點, 論據, 推理 論點: 論證者提出的觀點 論據: 支撐論點的材料 推理: 論據與論點之間有效的邏輯關係   深有所感, 數學類似製造論據的方法, 其實做實驗也是製造論據的方法. 有論據, 還有有論點. 難的就是有好的特殊的論點. 而藉由寫論文, 寫文章, 才能夠練習推理.\n  批判性閱讀:\n   建立在「批判性思維」: 在良好的判斷基礎上, 使用嗆搭的評估標準, 對事物的真實價值, 進行判斷與思考 不只從事務的表面價值進行評價, 用清晰準確有邏輯的方式, 對「表面下的底層思維」進行分析和評價.  讀脈絡-發展:   學術文獻是「知識的載體」, 與其他文獻「共生存」 要懂文獻, 要循著文獻的基因, 繪製出文獻的「生命脈絡」  橫向發展:   定義: 同一主題, 下的知識關係網路. 願景: 展示知識起源, 形成, 發展的進程, 以便讀者更高效利用知識. 類似Semantic scholar就可以做類似的事情.  文獻的分類:   主要就是前前時代, 前時代, 後時代的方式, 來看時代發展. 引用文獻: 主體文獻作者, 在創作文獻時, 所引用或參考, 並在文章後列出的文獻題路. 引證文獻: 引用或參考主體文獻的文獻, 「來源文獻」. 共引文獻: 與主體文獻共同引用了某一篇或某幾篇的一組文獻. 同被引文獻: 主體文獻的引證文獻的參考文獻. 二級參考文獻: 主體文獻的參考文獻的操考文獻. 二級引證文獻: 主體文獻的引證文獻的引證文獻.  讀思想-發展:   文獻思想: 文獻的「觀點」+「論證過程」. 文獻有無集中表達某種觀點? 這種觀點有沒有「非共識」的特性? 這種觀點的論述過程, 是否充分合理?  知識內化:   理解文獻思想後的「應用」, 應用文章的思想去分析和解決問題 閱讀問現實要: 帶著問題, 假設, 視角, 走進文獻.  知識遷移:   思想內化後, 舉一反三 運用知識解釋更多現象, 提出新的見解.  III. 篩選 文獻飽和: 停止篩選之時:   是否找到本領域頻繁引用的「三個重要文獻」或「三個重要作者」? 找到的文獻, 是否已經顯示出「三個不同的意見立場」? 是否已經找到「正方」與「反方」以及具有綜述研究性質的關鍵文獻? 找到的文獻是否顯示出「三個不同研究階段」？後面的階段在哪些方面超過了前面的研究?  篩選工作流:     按期刊層次進行「初步篩選」: 根據學術評價來找文章.    泛讀時對「文獻打分」: 快速閱讀摘要, 前言, 結論, 判斷論文價值.    識別出「最核心文獻」和「補充性文獻」.   核心文獻: 結構要素 (研究對象, 研究視角, 研究方法, 研究結論) 與研究主題的結構要素相同, 對研究者構思結構, 具有啟發性. 補充文獻: 相關, 可先暫存.   自己給文獻的「打分標準」應該是怎樣的?\n 處理: 述 - 分類, 歸納, 記述 IV. 分類 結構化閱讀來分類:   利用建立的meta data找樞紐, 來找到研究的缺口.   之後要建立一個meta data 的列表, 給每個筆記上meta data, 之後用data view做表格. 😍 找機會實作!\n V. 歸納 歸納思維:   找出資料之間的共性與特性, 歸納出相似資料的主題. 歸納: 從許多個別事務中, 概括出一般性概念, 原則, 結論. 抽象: 發現事務的共同規律, 「規律」成為資料的主題.  VI. 記述 記述環節:   建立合理的論證方案, 有序記述資料與觀點. 要呈現出研究者的「分類體系」 按照一定邏輯形式, 組織安排一系列事實. 這一系列事實, 要證明推導研究課題的中心論點.  論證思維:   論證: 邏輯地呈現論據, 推導證明某個結論. 有說服力的論證: 論點, 論據, 推理.  推理的四種形式:   推理: 證據的組織方式, 目的為推導並證明論斷 四種推理: 一對一, 並行, 鏈條式, 聯合 一對一: 將原因與結果直接簡單對應 並行: 綜合運用多條資料, 提供相同理由, 來證實結論 鏈條式: 用一個或多個原因證實結論, 把結論組織起來, 證明另一個結論. 聯合: 每個理由不能獨立存在, 只有聯合起來才能提供充分的證據來證實結論.  論證的兩種形式:   論證方案:發現式論證, 支持式論證. 發現式論證: 討論並解釋有關研究對象的已有知識, 透過「現有證據」的呈現來證明觀點. 支持式論證: 以發現式論證為基礎, 對獲得的資料進行綜合評論分析, 提出解決問題的思路.   粽: 建立發現式論證方案\n評: 建立支持式論證方案\n 輸出: 評 - 分析, 批評, 建構 文獻評論:   文獻評論: 要恰如其分, 還要從批評引出作者想要論述的學術主張. 體現: 文獻閱讀, 文獻整合, 觀點提煉. 原則: 簡潔, 以觀點輸出為主 文獻評論: 文獻分析, 文獻批評, 建構思路.  VII. 分析 文獻分析:   總結性評價   記述文獻觀點的總結    對記述文獻的評價    VIII. 批評 文獻批評:   指出不足之處, 如研究侷限, 研究空白.  IX. 建構 建構思路:   建立研究根據 提出自己的研究角度 優秀文獻綜述的「評」, 基本就可以預期到論文的結論.  後記  到此思考整理了郭澤德-寫好論文 裡面的「4.2節: 文獻綜述的執行流程」, 反思自己的的學術工作流.\n  現在看這種寫作的書都有足夠的經驗, 之後可以持續寫文章, 來做系統性的認識. 系統性提升學術工作流. 天天向上, 共勉之！\n  2022.01.20. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-20","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur022%E6%96%87%E7%8D%BB%E7%B6%9C%E8%BF%B0%E8%A9%95%E7%9A%84%E4%B9%9D%E5%80%8B%E4%BB%BB%E5%8B%99/","series":["每日文章","學術工作流"],"tags":[],"title":"MUR022 文獻綜述評的九個任務"},{"categories":["文獻閱讀"],"content":"貝式動態定價基礎-Ver(0.1)  紫式晦澀每日一篇文章第21天\n 前言   今天是2022年第19天, 全年第3週, 一月的第三個週三. 今天來思考「貝式動態定價(Bayesian Dynamic Pricing)」的相關細節. 之前做動態定價的研究比較是從線上機器學習的角度, 在寫文章的過程也逐漸體會到「定價(Pricing)」本身就是另一個很困難的運籌學問題. 多累積見聞, 在literature裡面create space.\n  今天的素材來源為Management Science於2012年的文章J. Michael Harrison, N. Bora Keskin, Assaf Zeevi, (2012) Bayesian Dynamic Pricing Policies: Learning and Earning Under a Binary Prior Distribution. Management Science 58(3):570-586. .\n   代碼BDP\n 000 摘要 100 簡介 200 方法 2.1. Basic Model Elements BDP201 賣家定價機制與環境需求模型:\n   賣家(Seller): 公司, 提供單一產品, 給序冠而來的買家    販售期間(Sales period): 在第t個販售期間, 由買家們身上, 所得到的收益(Revenue), 稱為「t販售期間收益」    定價(Pricing): 每個期間, 賣家從給定區間中選擇價錢(price); 接著, 賣家經歷成功(於所選價錢的銷售)或失敗(無銷售)    環境需求函數 (Ambient demand model): 定價的函數, 返回賣家提供定價後, 成功銷售的機率.    邊際成本(Marginal cost): 販賣商品的邊際成本假設為零, 如此, 利潤(Profit)可與收益(Revenue)交替使用.      Consider a firm, hereafter called the seller(賣家), that offers a single product for sale to customers who arrive in sequential fashion. As a matter of convention, we associate with each successive customer(買家) a distinct sales \u0026ldquo;period\u0026rdquo; so that, for example, the phrase \u0026ldquo;period- $t$ revenue\u0026rdquo; simply means revenue realized from the $t^{\\text {th }}$ arriving customer. In each period $t=$ $1,2, \\ldots$, the seller must choose a price $p_{t}$ from a given interval $[l, u]$, where $0 \\leq l\u0026lt;u\u0026lt;\\infty$, after which the seller experiences either success (a sale at the offered price $p_{t}$ ) or failure (no sale). The probability of success when the seller offers price $p$ in any given period is $\\rho(p)$; we call $\\rho(\\cdot)$ the ambient demand model(環境需求模型). The marginal cost(邊際成本) of the product being sold is set to zero without loss of generality (because prices can always be expressed as increments above cost); given this normalization, the terms \u0026ldquo;profit\u0026rdquo; and \u0026ldquo;revenue\u0026rdquo; can and will be used interchangeably.   BDP202 先驗機率, 假說, 期望收益與銷售序列:\n   自然選擇環境需求模型(Nature chooses the ambient demand model): 自然從兩個環境需求模型做選擇, 賣家無法觀察到此選擇, 此選擇在整個銷售期間(selling horizon)都不會改變.    自然的選擇(Nature choice)與先驗機率(Prior probability): 自然的選擇, 為一個二元隨機變數; 在需求模型$\\rho_0, \\rho_1$中, 自然選擇了需求模型$\\rho_1$的機率記為$q_0$, 稱為先驗機率(Prior probability).    先驗機率(Prior probability): 先驗機率$q_0$就會與之後定義的, 觀察到銷售結果後, 產生的機率做區分.    設先驗機率以排除顯然狀況: 設置先驗機率為$0\u0026lt;q_0\u0026lt;1$, 以避免顯然的狀況. 否則$q_0=0$或者$q_0=1$, 賣家就會確定性知道特定到環境需求模型.    假說(Hypothesis): 對「自然的選擇$\\chi$」是需求模型$i$,所產生的事件${\\chi = i}$, 稱為假說$i$.    假說下的期望收益(Expected revenue in a period under hypothesis): 期望收益函數$r_{i}(p)$為在「假說$i$」下, 「定價$p$」所得到期望收益, 定為「價錢$p$」乘以「需求模型$i$下, 價錢$p$的購買機率.     銷售序列(Sales sequence): 隨機過程$X\\equiv (X_1, X_2, \\cdots)$ 紀錄了銷售的成功(1)與失敗(0)的過程.      Before the first customer arrives, nature chooses either $\\rho_{0}(\\cdot)$ or $\\rho_{1}(\\cdot)$ as the ambient demand model; this choice is not observed by the seller, and it remains fixed over the entire selling horizon. We encode this choice via the random variable and denote by $q_{0}$ the prior probability assigned by the seller to the event ${\\chi=1}$; this number is part of our problem data. (The subscript in the notation $q_{0}$ differentiates this initial probability assessment from the ones formed later, after sales outcomes are observed.) To exclude trivial cases, in which the seller knows the ambient demand model with certainty, we assume that $0\u0026lt;q_{0}\u0026lt;1$. We shall occasionally refer to the event or condition ${\\chi=i}$ as hypothesis $i$. If price $p$ is chosen in a given period, then the seller\u0026rsquo;s expected revenue in that period under hypothesis $i$ is  The only random variables other than $\\chi$ that will figure in the development to follow are indicator variables $X_{1}, X_{2}, \\ldots$ defined as follows: $X_{t}=1$ if there is a sale (success) in period $t$, and $X_{t}=0$ otherwise. Defining $X:=\\left(X_{1}, X_{2}, \\ldots\\right)$, we call $X$ the sales sequence.   BDP203 現有價格銷售成功率可知的假設下, 制約需求函數與最佳定價所需要的優化條件:\n 1.現有價格(Incumbent price): 賣家假設知道現有價格$\\hat{p} \\in(l, u)$的銷售成功機率$\\hat{\\rho}$. (為何能知道?) 2.需求假說制約(demand hypothese restriction): 在「現有價格銷售成功率可知」的假設下, 需求假說得到制約, 對 $i=0,1$ 滿足$$\\rho_{i}(\\hat{p})=\\hat{\\rho}.$$ 3.需求模型的光滑性要求: 需求模型在出價區域$[l, u]$上, 假設是(1) 連續可微分 與(2)嚴格遞減 (愈貴愈沒人要買). 4.價格彈性函數(Price elasticity functions) 價格彈性(price elasticity)是衡量由於價格變動所引起數量變動之敏感度指標. (🤯 為何是這樣的定義? 感覺與log有關係)  5.最佳定價的唯一性, 繼承於價格彈性的嚴格遞增性: 價格彈性的嚴格遞增, 蘊含期望收益函數在出價區間有唯一的最大值 6.兩個假說下的最佳定價是是出價區間的內點: 優化理論的假設要求 7.一階最優條件: 在此條件下, 得到一個微分方程$\\rho_{i}(p)+ p \\rho^{\\prime}_{i}(p)$. 感覺就是有解的ODE.     The seller is assumed to know the success probability $\\hat{\\rho}$ for an incumbent price $\\hat{p} \\in(l, u)$. Consistent with this assumption, we restrict attention to demand hypotheses that satisfy $\\rho_{i}(\\hat{p})=\\hat{\\rho}$ for $i=0,1$. The demand models $\\rho_{0}(\\cdot)$ and $\\rho_{1}(\\cdot)$ are also assumed to be continuously differentiable and strictly decreasing over $[l, u]$, and we define the associated price elasticity functions $\\varepsilon_{i}(\\cdot)$ as usual (here and later, a prime denotes a derivative):  Both $\\varepsilon_{0}(\\cdot)$ and $\\varepsilon_{1}(\\cdot)$ are assumed to be strictly increasing, from which it follows that each of the single-period expected revenue functions $r_{i}(\\cdot)$ has a unique maximizer $p_{i}^{*}$ in $[l, u]$. We assume are interior points of the feasible price range $[l, u]$ and without loss of generality that. The first-order conditions for optimality then give the following:    BDP204: 當需求函數值重合:\n   現有價錢在最優價錢區間之間: 假設現有價錢$\\hat{p}$在最優價錢區間$[p_{0}^{*}, p_{1}^{*}]$    解釋: 如果兩個需求函數沒有相交, 則「短視貝式策略(Myopic Bayesian policy)」會給完美結果, 也就不存在此文章討論的問題    價格彈性函數單調性, 蘊含結果: 兩個需求函數的函數值重合, 會在「現有價錢」這個點上.       We assume throughout that the incumbent price $\\hat{p}$ lies in the interval $\\left[p_{0}^{*}, p_{1}^{*}\\right]$ because the opposite case is essentially trivial. (If the demand curves $\\rho_{0}(\\cdot)$ and $\\rho_{1}(\\cdot)$ do not intersect within that interval, then the arguments to follow can easily be modified to show that the myopic Bayesian policy itself gives excellent results, and so the issues addressed in this paper simply do not arise.) Monotonicity of the price elasticity functions then has the following implication.    BDP205 唯一的無信息價格是現有價格:\n   無信息價格(Uninformative price): 一個價錢是「無信息」, 那他就對「環境需求函數」沒有信息.    白話Propsition 1: 在最佳價格之間內的「唯一」無信息價格, 是現有價錢$\\hat{p}$.      A price $p$ that satisfies $\\rho_{0}(p)=\\rho_{1}(p)$ is uninformative, providing no information about the ambient demand model, so Proposition 1 can be verbally paraphrased as follows: Under our assumptions, the unique uninformative price between $p_{0}^{}$ and $p_{1}^{}$ is the incumbent price $\\hat{p}$.   Appendix B. Proofs of Propositions 1-3 BDP206 現有價格的優化論證:\n   呼叫「現有價錢」的定義, 為兩種假說的需求函數所得到的購買機率相同的地方.    考慮在最優價格區間之間, 也讓兩種假說的購買機率相同的「另外一點」.    由於價格彈性函數的一個條件(此條件何來???), 得到「兩需求函數一階導數之差」為正值.    因此, 「兩需求函數一階導數之差」是局部遞增, 在「另外一點」消失; 這樣只能有一個「另外一點」    因此, 在最優價格區間之間, 只能有一個點, 使得購買機率在兩個假說下相等    由於「現有價錢」已經是這樣的「另外一點」, 於是那個點就是現有價錢     有點像是存在性證明. 先抓出一個核心概念, 然後證明他就是這樣.\n Proof of Proposition 1.\n  First, recall that the incumbent price $\\hat{p}$ satisfies $\\rho_{0}(\\hat{p})=\\rho_{1}(\\hat{p})$. Now, let $\\tilde{p}$ be an arbitrary price in $\\left[p_{0}^{*}, p_{1}^{*}\\right]$ such that $\\rho_{1}(\\tilde{p})=\\rho_{0}(\\tilde{p})$. Because $\\varepsilon_{1}(p)\u0026lt;1\u0026lt;$ $\\varepsilon_{0}(p)$ for all $p \\in\\left[p_{0}^{*}, p_{1}^{*}\\right]$, we deduce that $\\rho_{1}^{\\prime}(\\tilde{p})-\\rho_{0}^{\\prime}(\\tilde{p})\u0026gt;0$. Thus, the function $p \\mapsto \\rho_{1}(p)-\\rho_{0}(p)$ is locally increasing around $\\tilde{p}$ and vanishes at that point. This implies that there can be at most one price $p \\in\\left[p_{0}^{*}, p_{1}^{*}\\right]$ satisfying $\\rho_{1}(p)=\\rho_{0}(p)$. Because the incumbent price $\\hat{p}$ already satisfies that condition, we conclude that $\\hat{p}$ is the unique price $p \\in\\left[p_{0}^{*}, p_{1}^{*}\\right]$ such that $\\rho_{1}(p)=\\rho_{0}(p)$.   Another section BDP: BDP: BDP: BDP: BDP: BDP:\nBDP: BDP: BDP: BDP: BDP: BDP:\n300 結果 400 討論 900 形式結果 後記  到此, 我們學習了J. Michael Harrison, N. Bora Keskin, Assaf Zeevi, (2012) Bayesian Dynamic Pricing Policies: Learning and Earning Under a Binary Prior Distribution. Management Science 58(3):570-586. 中對「貝式動態定價」的2.1節.\n  本文章產出了六張知識卡片\n   BDP201 賣家定價機制與環境需求模型 BDP202 先驗機率, 假說, 期望收益與銷售序列 BDP203 現有價格銷售成功率可知的假設下, 制約需求函數與最佳定價所需要的優化條件 BDP204 當需求函數值重合 BDP205 唯一的無信息價格是現有價格 BDP206 現有價格的優化論證  這個小節給了問題的形式化, 定義了許多概念. 其中還有很多數學優化上的考量所得到的論證, 需要再累積多點經驗. 天天向上, 共勉之!  2022.01.19. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-19","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur021-%E8%B2%9D%E5%BC%8F%E5%8B%95%E6%85%8B%E5%AE%9A%E5%83%B9%E5%9F%BA%E7%A4%8E/","series":["每日文章","動態定價"],"tags":[],"title":"MUR021 貝式動態定價基礎-Ver(0.1)"},{"categories":["物件導向編程"],"content":"沈思輸入-處理-輸出模型  紫式晦澀每日一篇文章第20天\n 前言   今天是2022年第18天, 全年第3週, 一月的第三個週二. 今天來思考「IPO:Input-Process-Output」模型. 這種模型可看作認知模型, 也可看作計算模型, 隨處可見. 把算法思維帶入各種場景, 我們便能可視化流程工程, 優化系統.\n  今天的素材主要來自各種提到「IPO:Input-Process-Output」模型的文章. 相關的材料非常多, 也算是可以把所見所聞做個整理.\n  輸入-處理-輸出模型本體論 輸入-過程-輸出（IPO）模型:   輸入-處理-輸出（IPO）模型，或輸入-處理-輸出模式，是系統分析(systems analysis)和軟體工程(software engineering)中廣泛使用的方法 用於描述資訊處理程式或其他過程的結構。 The structure of an information processing program or other process. 許多介紹性寫程式和系統分析文字將此作為描述過程的最基本結構。 The most basic structure for describing a process.  系統(輸入-處理-輸出)\u0026ndash;環境\u0026ndash;反饋  5系統, 環境, 反饋: 建立輸入-過程-輸出（IPO）模型實際上是要建立「系統」. 有了系統以後, 就能夠與「環境」互動. 互動以後, 得到「反饋」來調整「系統狀態」.\n  視為過程的電腦程式:\n   電腦程式(Computer Program)對於另一種使用輸入-過程輸出模型的過程非常有用，接收來自使用者或其他來源的輸入，對輸入進行一些計算，並返回計算結果。 從本質上講，該「系統」與「環境」分離，從而將「輸入」和「輸出」定義為一個統一的機制。  系統將工作分為輸入, 處理, 輸出三個部分:   輸入: 從「環境(Environment)」的要求. A requirement from the environment (input) 處理: 基於要求的「計算(Computation). A computation based on the requirement (process) 輸出: 提供「環境」結果. A provision for the environment (output)  系統思維: 可靠的推論:   系統思維是透過對基本結構的理解日益深入地理解，對行為進行可靠推斷的藝術和科學 Systems thinking is the art and science of making reliable inferences about behaviour by developing an increasingly deep understanding of the understanding of the underlying structure  作為思維模型的IPO IPO實例:   工業: 原材料\u0026ndash;\u0026gt;工廠加工\u0026ndash;\u0026gt;工業產品 考核: 員工工作內容\u0026ndash;\u0026gt;考核規則\u0026ndash;\u0026gt;員工考核結果 認知: 對環境的觀察, 欣賞, 交流, 閱讀\u0026ndash;\u0026gt;大腦進行類比, 歸納, 演繹等抽象思考\u0026ndash;\u0026gt; 解釋現象的思維模型. 數據分析: 大量數據\u0026ndash;\u0026gt;統計分析處理\u0026ndash;\u0026gt;有用信息, 結論. 計算機: 輸入指令\u0026ndash;\u0026gt;運算處理\u0026ndash;\u0026gt;輸出計算結果 造筆記: 心法\u0026ndash;\u0026gt;技法\u0026ndash;\u0026gt;用法  輸入要正確, 處理要合理, 輸出有價值 輸入要正確:   持續學習，擴大自己知識的深度和廣度，從而不斷提升自己有效過濾錯誤輸入信息能力。 管理者要有能力甄別下屬所反饋信息正確性，帶主觀情緒的負面信息只會讓管理者做出錯誤的決策 在交流學習中不能聽啥是啥，要有個人客觀辯證的能力，否則會很容易學到錯誤的知識。  處理要合理:   要努力學習掌握更多已經被證明是有效的思維模型 這樣我們在解決問題或做決策的時候才有更合適的選擇，而不是拿著錘子看什麼都是釘子。  輸出有價值:   要掌握各種標準化的模板, 穩定有效的輸出, 去看見更深一層的資訊價值.  人生的IPO模型 人生的IPO:   人生: 信息輸入\u0026ndash;\u0026gt;大腦處理\u0026ndash;\u0026gt;總結輸出  信息輸入\u0026ndash;大腦處理\u0026ndash;總結輸出 信息輸入:兩種途徑   一手信息: 與人聊天, 親手動手實踐 二手信息: 從他人總結出的經驗, 看書, 看部落格文章   看書是得到二手信息; 家教是得到一手信息\n 大腦處理:不斷收集各種模型   理解世界, 要找到自己底層的思維模型(Mental Models) 思維模型來自不同學科不同方面, 80~90個模型能處理90%以上的問題.  總結輸出:持續的大量的輸出   總結和分享為目標, 進行整理產出. 寫文章, 文件, 根據聽眾觀眾的需求, 練習精進各種體裁的寫作.  結果驅動的IPO模型 定義IPO: 系統思考, 系統設計的底層原理. 由輸入, 加工, 輸出三部分組成的閉環.   I \u0026ndash; Inputs (輸入): 數據, 人力, 物質等等 P \u0026ndash; Processes (工序): 對輸入加工處理的方法流程 O \u0026ndash; Outputs (輸出): 工序的輸出.  閉環思維:   我們先收集信息，再根據收集到的信息（輸入），決定下一步的決策和行動（工序），若結果是我們期望的（輸出），就重復這一過程，構成了一個單環迴路。 單環思維是人類的本能思維，我們餓了，點了一個外賣，味道不錯，下一次還繼續點這個外賣。我們越來越熟練，也越來越依賴這個閉環，迴路就越難打破。 這個循環迴路會逐漸固化為一種心智模式——決定我們的思考方式，成為指導我們認知和行動的規則，並以此來理解他人行為。  單環思維陷阱:   一定程度上，越成功的人，他的模式越有效，越容易陷入單環思維陷阱。一旦環境發生變化，他的策略失靈時，會產生強烈的自我防衛意識，拒絕接受批評，將問題歸罪於環境或第三者，堅稱自己並沒有過錯（維護自己的模式）。 策略失靈，本提供了一個好的學習機會，但在這種單環的防衛心理機制下，他喪失了學習能力。  三果思維: 結果, 效果, 成果  三果導向思考: 增加Outcomes(三果)來評估結果, 質疑過程, 反思我們的信念與假設(模式).\n  結果, 效果, 成果:技術上Outcomes有三個層次\n   結果: 完成任務. 效果: 比上次做有什麼改進, 提升幅度. 成果: 個人有無成長, 通過做這件事取得什麼進步.  每日寫文章的三果: 寫作習慣的三果分析   結果: 完成篇500字以上的文章 效果: 這次比之前是否更清晰,更易懂, 格局變大? 成果: 經過寫這篇文章後, 獲得什麼進步, 解決什麼問題, 對某事是否有更深的理解, 寫作的目的是什麼, 是否是當前最重要的事.  三果核查輸出:   利用三果核查, 跳出系統本本身, 在「元系統」的更高層次觀察系統.  商業三效:   效率 (Efficient): doing something in a good, careful and complete way with no waste of time, money or energy 把事情做得好, 謹慎, 完整, 沒有額外浪費時間, 金錢, 能源. 效果 (Effective): producing the result that is wanted or intended; producing a successful result 生產想要或預期的成功結果 效益 (Economic): (of a process, a business or an activity) producing enough profit to continue 產生足夠利潤以繼續  工序即模式:   模式: 事物重複, 有規律地發生與完成. 工序自動化: 自動化模式, 成為我們行為模式與思維方式. 雙環學習: 三果能將自動化的模式轉為雙環學習, 將原本封閉的IPO系統轉化為開放的IPOO系統.  學習的IPO模型 信息輸入(Input):     每週的學習精力如何分配?    是否真的善用所有的學習渠道?    可否大幅提升現在的學習內容質量?    學習渠道實務:   做業務復盤, 紀錄實際工作中的領悟 閱讀學習, 讀書, 碎片化知識, 課程 行業交流諮詢, 高水平交流 產品學習, 頂級產品, 競爭對手, 行業的經典打法  學習質量提升實務:   閱讀的信息質量愈高, 學習效率愈高 高價值的真實信息鳳毛麟角, 定期更新閱讀源, 提升信息質量. 過去99%的PR文, 記者觀點; 只閱讀優秀創業者的業務復盤, 演講, 採訪.  信息處理(Process):     有不斷追問事物本質的習慣媽?    掌握了多少思維模型?    面對創業難題, 是否有自己的完備思考框架?    事物本質實務:   產品的本質, 營運的本質, 融資的本質, 品牌的本質 花時間討論本質問題, 討論清楚, 達成共識, 會提升決策質量. 如何一層層深度思考, 逐步挖掘深層次含義, 最終接近事情的本質.  思維模型實務:   數學, 工程學, 物理學, 生物學, 化學, 統計學, 社會學, 心理學, 經濟學. 經典思考框架, 對90%的問題都可做出優秀決策.  自我框架實務:   完整思考, 理性選擇.  信息輸出:     在用的是哪些輸出方式?    目前的學習深度真的足夠媽?    輸出方式實務:   與高段位的人聊天 被動學習: 聽講, 朗讀, 聽與看, 示範展示 主動學習: 小組討論, 實作演練, 轉教別人, 立即應用 聆聽\u0026ndash;\u0026gt;沈澱\u0026ndash;\u0026gt;傳播  學習深度實務:   被動學習留存率低, 應避免. 主動學習留存率高, 透過討論分享實踐, 達到學習深度. 自我探討, 自我復盤, 自我反思.  後記  到此摘錄了各種IPO模型的實踐. 從輸入, 處理, 輸出, 所搭建起的系統, 與環境互動, 得到反饋, 是我們在增強學習中對「認知能力」的建模. 藉由標準化這些建模, 許多部分得到了自動化, 也因此可以被逐漸優化, 達到更好的表現.\n  另外, 透過三果思維, 去看行動的結果, 效果, 成果, 於是就能夠fine tune系統的各種變數. 機器學習系統也是類似的邏輯, 能夠人工找出表現好的機器狀態. 這類的邏輯都考慮進去, 就能累積工程思維, 做出來的東西也就更有價值, 因為能解決更具體的問題.\n  透過這次的寫作, 對系統思維的掌握感覺又更確實了! 之後可以多思考將此IPO系統模型, 與Python的編程思維做搭配, 來迭代自己的認知模型！天天向上, 共勉之!\n  2022.01.18. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-18","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur020-%E6%B2%88%E6%80%9D%E8%BC%B8%E5%85%A5-%E8%99%95%E7%90%86-%E8%BC%B8%E5%87%BA%E6%A8%A1%E5%9E%8B/","series":["每日文章","編程思維"],"tags":["Python"],"title":"MUR020 沈思輸入-處理-輸出模型"},{"categories":[],"content":"從幼女戰記思考蘇德戰爭  紫式晦澀每日一篇文章第19天\n 前言   今天是2022年第17天, 全年第3週, 一月的第三個週一. 昨晚與今天一整天都在看「幼女戰記」, 看了動畫漫畫與電影版. 非常好看, 以前對戰爭的歷史沒這麼有興趣, 現在多了很多角度可以思考, 用今天的文章來研究看看「蘇德戰爭」.\n  今天的素材主要來自網路關於「蘇德戰爭」的種種資訊. 對歷史類的東西沒研究過, 來學習學習相關的文化.\n  蘇德戰爭背景與細節  定義: 蘇德戰爭（德國方面稱為東方戰線，德語：die Ostfront；蘇聯方面稱為偉大的衛國戰爭，俄語：Великая Отечественная Война，而「衛國戰爭」本指1812年的俄法戰爭）是第二次世界大戰期間蘇聯與納粹德國及雙方盟國之間發生的戰爭，時間從1941年6月22日德國開始進攻蘇聯、至1945年5月9日德國向蘇聯無條件投降為止。   戰前背景:\n   在20世紀30年代阿道夫·希特勒領導的納粹黨在德國取得執政，而英國、法國等西方國家採取的綏靖政策則更加助長了納粹德國的氣勢。 而此時的蘇聯總書記約瑟夫·史達林為實現自己的利益和德國簽署了「蘇德侵犯條約」。 該條約中包括了「秘密附屬議定書」，將波蘭、波羅的海三國、芬蘭以及東歐等國劃分為兩國的勢力範圍。  德國與蘇聯合作, 瓜分歐洲大陸:   1939年9月，德國與蘇聯聯合發動的波蘭戰役以及1940年的法國戰役之後，德國和義大利在1940年就很快就占領了中歐、西歐大陸、北歐和巴爾幹半島. 1941年6月初，德國控制了歐洲包括法國、波蘭西部、荷蘭、挪威、丹麥、比利時等16個國家的人力、物力資源，並且盟國除了義大利外有匈牙利、羅馬尼亞和保加利亞。 而這時的全世界，只有英國在獨自和德國、義大利作戰。 而此時，蘇聯在1939年9月蘇德瓜分波蘭之後從波蘭得到了它51%的領土，從羅馬尼亞拿走了南比薩拉亞和北布科維納，從芬蘭拿走了卡累利阿、薩拉、雷巴奇半島等領土。  蘇聯方面 德國發展更好, 蘇聯開始防範德國:   1930年代末的蘇聯經過十餘年的大規模經濟建設，工業產值已經躍居歐洲第一，但是生產品質和人民生活水準則遠不如納粹德國。 另外，蘇聯全國也剛從瘋狂的大清洗肅反運動中慢慢恢復正常的秩序。 在德國橫掃歐洲之時，史達林看到如此強大的德國必然會威脅到蘇聯的國家安全，便開始著手採取防範德國的系列措施   將蘇聯的重工業和軍工工業有計劃地遷移到烏拉爾山以東；    對德國和談，避免刺激德國；    穩住東方的大日本帝國，先是通過援助中國的抗日戰爭以牽制日本的軍隊，之後又轉變態度，於1941年4月和日本簽署《蘇日中立條約》；    建立「東方戰線」，增加戰略縱深等等。    蘇聯的東方戰線(Восточный фронт):侵略東歐:   在蘇聯的系列準備措施中，建立東方戰線是最具爭議的，在建立東方戰線的進程中，蘇聯根據《蘇德互不侵犯條約》中的秘密條款占領了約60萬平方公里的土地，將芬蘭、羅馬尼亞等國迅速地推向自己的對立面，還遭受到世界各國的普遍譴責。 且事實上東方戰線在戰爭中起到的作用也極其有限，蘇聯吞併的地區的人民大多反對蘇聯的統治，在德國入侵的時候有很多民眾都發起暴動趕走蘇軍並歡迎德軍的到來。  德國方面 德國西線戰事順利, 著手入侵蘇聯:   德國方面，西線戰事的順利進行，只有英國憑藉海洋的優勢持續抵抗（德國的入侵），讓希特勒感到有機會騰出手來對付東邊的蘇聯了. 而德國的情報機構對蘇聯的大大低估的錯誤判斷，也讓希特勒認為蘇聯是非常容易對付的國家，便開始著手制定入侵蘇聯的計劃，計劃代號：「巴巴羅薩計劃」。  巴巴羅薩計劃(Unternehmen Barbarossa):   巴巴羅薩計劃，德軍計畫以集中380萬兵力，以「閃電戰」分別從北、中、南3個方向實施迅猛而深遠的突擊，占領蘇聯的首都莫斯科、蘇聯第2和第3大城市列寧格勒和基輔等城市 在蘇聯西部地區把蘇軍的主力消滅，接著向蘇聯腹地長驅直入，進抵阿爾漢格爾斯克、伏爾加河、阿斯特拉罕一線，並用空軍摧毀烏拉爾工業區，從而擊敗蘇聯。  蘇聯對巴巴羅薩計畫的無防備:   擬定巴巴羅薩計畫半年後，蘇聯渾然不覺，還於1941年5月1日莫斯科紅場舉行閱兵式紀念國際勞動節 雖然英國等國和蘇聯間諜多次向史達林匯報「德國將在1941年夏進攻蘇聯」，但史達林認為英國和德國的戰爭分勝負之前，德軍還沒有能力進犯蘇聯。他甚至懷疑這是英國間諜為了將蘇聯拉入對德作戰的陷阱而捏造的。 而且蘇聯此時也因為正在準備進攻德國的「大雷雨計劃」，而沒有料到德軍即將進攻蘇聯。 史達林有一個進攻德國的「大雷雨計劃」，該計劃還比巴巴羅薩計劃更早策劃，只是由於蘇聯的戰爭準備還沒有達到進攻的要求而被德國的巴巴羅薩計劃先行一步。可見蘇聯對德國的進攻計劃是完全沒有防備的。  颱風行動 德軍進攻莫斯科:   9月30日，德軍中央軍團發動「颱風行動」，大舉進攻莫斯科。 德軍用於颱風行動莫斯科方向的兵力達到100萬、75個師、1,700輛坦克、1.4萬門火炮、1,400架飛機。 蘇軍在莫斯科以西300餘公里縱深內，建立了梯次配置的多道防禦地帶，有3個方面軍和一個戰役集群，總共125萬名兵力、共75個師、990輛坦克、7,600門火炮、677架作戰飛機擔任防禦，蘇共中央政治局、國防委員會、最高統帥部和總參謀部作戰組一概不離開莫斯科。 德軍陸軍兵分3路在沿途突破蘇軍的防禦線後逐步向莫斯科挺進，但是最猛烈的進攻遇到了最頑強的抵抗，進入10月2日之後莫斯科戰役正式打響，但此時蘇聯進入了秋冬時節，而這對蘇軍則非常有利。 11月7日冒著德軍的炮火，蘇聯在紅場舉行紀念十月革命勝利24周年的閱兵式，閱兵式後的蘇軍直接開赴前線。  蘇軍頂住德軍進攻:   經過浴血奮戰，蘇軍憑藉極其堅強的抵抗和熟悉的自然條件，蘇軍最終頂住了德軍進攻，並在11月30日將德軍中央集團軍的進攻成功阻止在北線到卡林努、中線在納羅—福明斯克和莫扎伊斯克、南線到圖拉城下並迫使中央集團軍停下來建造了幾道臨時防線， 於1941年12月5日之後轉入125萬人的大反攻，至1942年1月7日將參與颱風行動的德軍向西擊退至距離莫斯科250公里至320公里的地方，並使得德軍中央集團軍在1941年10月2日－1941年12月1日－1942年1月7日之間付出了35,757人陣亡、9,721人失蹤，共計45,478人陣亡和失蹤、總傷亡17.3萬人的代價。  幼女戰記中的相關資訊 帝國／萊希（帝国／ライヒ，Empire/ Reich）:   歷史定位從地理上來說類似是德意志帝國及奧匈帝國兩國合併成的國家，但主要影射的是德國。 帝國國土坐落於大陸中央四戰之地位置的新興工業大國，首都名為柏盧。 其在建國期間攻佔周遭國家的接鄰領土而埋下仇恨，並因國際情勢演變出強烈的民族主義和軍國主義，以及因地緣政治學而生的國防政策，擁有相當於軍事及工業命脈的西方工業地帶及低地工業地帶，並在大戰開始前與盧斯聯邦和義魯朵雅王國各自簽署互不侵犯條約及同盟協定（這裡參考了二戰時納粹德國的外交）。 魔導師大多為少數菁英風格，帝國一開始以陸軍的支援角色作為運用魔導戰力的方針，但隨著大戰逐漸激烈化而漸漸朝向獨立運用的菁英部隊發展。飛行魔導具的技術居於世界領先地位，動畫中的造型為類似傘兵副傘的前背包搭配裝在單腳的推進裝置。 漫畫的木偶劇中使用德國狼犬的形象。    與歷史上的德國「軍國主義」相符合; 這不是有軍隊的國家, 而是有國家的軍隊.\n   盧斯聯邦（ルーシー連邦，Russy Federation）:\n   歷史地位類似於蘇聯，與帝國東方接壤的共產主義國家，和帝國擁有帝國領奧斯特倫德的領土紛爭地區，首都名為莫斯科，為帝國東部方面軍和北方方面軍的主要假想敵。 並未簽署部分的國際法，理論上可隨意處置俘虜，前皇國政府和共和國有悠久的同盟傳統，並與列強共同維繫箝制帝國擴張的包圍網，但因近年來國內共產政府掌權而與共和國產生嫌隙，進而和帝國簽署互不侵犯條約，同時國內包含絕大多數魔導師和幹練軍官的保皇派因遭到新政府逮捕管制，所以軍隊嚴重缺乏魔導部隊及老練將校。 和帝國之間的東方戰場號稱為本次大戰死傷最為慘烈的戰區，戰後以五月九日作為偉大衛國戰爭紀念日。 漫畫的木偶劇中使用狐狸的形象。    歷史上5月9號是德國投降, 作品中是用了俄國的「偉大衛國戰爭」的名稱當紀念日.\n   後記  到此稍微比較了歷史上的蘇德戰爭與幼女戰記中的盧斯聯邦與帝國萊希. 有很多地方兩者都雷同, 之後可以慢慢用寫文章的方式思考學習.\n  以前對歷史不是很有興趣, 但看了幼女戰記之後, 開始想要了解各種喜歡的輕小說背後的文化基礎. 持續累積, 搬運資訊, 總結思考, 共勉之！\n  2022.01.17. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-17","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur019-%E5%BE%9E%E5%B9%BC%E5%A5%B3%E6%88%B0%E8%A8%98%E6%80%9D%E8%80%83%E8%98%87%E5%BE%B7%E6%88%B0%E7%88%AD/","series":["每日文章"],"tags":["幼女戰記"],"title":"MUR019 從幼女戰記思考蘇德戰爭"},{"categories":["物件導向編程"],"content":"沈思Python建構式  紫式晦澀每日一篇文章第18天\n 前言   今天是2022年第16天, 全年第3週, 一月的第三個週六. 今天來深思「構造Class」的細節思想, 讓思考昇華到更根本的原因上.\n  今天的素材主要來自各篇關於Python 建構式(constructor)的文章\n  Python構造式 構造式(Constructor)簡介:   在創建類的對象時, 使用 Python 編程語言中的構造函數(Constructor)，同時驗證對像是否有足夠數量的資源來執行任何類型的啟動任務。 一種特殊類型的函數或方法，其目的是初始化類的成員，通常分為兩種類型。 這些函數基本上是「參數化構造函數」和「非參數化構造函數」 。前者致力於考慮參數的想法，後者認為它不需要任何參數。  技法: 預設構造式\u0026參數化構造式 預設構造式(Default Constructor):    不接受任何參數。\n  在稱為 self 的默認構造函數的情況下存在默認參數。此參數指的是為此類創建的對象。\n  基本代碼\n  1class sample: 2 3 # default constructor 4 def __init__(self): 5 # initializing variable instance 6 self.number_variable=1001 7 8 # a method 9 def print_method(self): 10 print(“number variable : “,self.number_variable)  執行  1obj=sample() 2obj.print_method() 參數化構造式(Parameterized Constructor):   參數化構造函數接受其中的參數。 就像這裡的默認構造函數一樣，創建的第一個參數引用這個類的實例。 程序員需要並定義其餘參數以引用實例變量。 基本代碼  1class sample: 2 # parameterized constructor 3 def __init__(self , id , name , age , gender, doj , dob ): 4 self.id_value = id 5 self.name_value = name 6 self.age_value = age 7 self.gender_value = gender 8 self.doj_value = doj 9 self.dob_value = dob 10 # method to output attributes  11 def print_output(self): 12 print(“Id value :”, self.id_value) 13 print(“name_value :”, self.name_value) 14 print(“age_value :”, self.age_value) 15 print(“gender_value :”, self.gender_value) 16 print(“doj_value :”, self.doj_value) 17 print(“dob_value :”, self.dob_value)  執行  1obj1=sample(101,’Terry’,27,’male’,10072015,10071993) 2obj1.print_output() 心法: 構造式的工作原理-__init__()函數與自引用 __init__()函數:   該函數在相應類的對象實例創建時調用。 這個構造函數是使用 def 關鍵字來確認的，這與所有其他函數聲明非常相似。 在函數中的這些初始化函數聲明中另一個值得注意的事情將是前面和後綴的雙下劃線。  1def __init__(self,salary_arg) 自引用(Self Reference):   self 引用了物件本身(Object itself)。 self 可以指代與其所涉及的類相關的函數和變量。 必須是構造函數聲明中最重要的參數。 它表示期望指數與該對象的屬性一起使用。  1def __init__(self,Employee_name,Employee_id, Employee_age): 2 self.Employee_name = name; 3 self.Employee_id = id; 4 self.Employee_age = age 用法: 初始化, 實例變數 構造式的優點:   主優點: 在「初始化(Initialize)」有莫大幫助 實例變數(Instance variables): 使用建構式可以初始化的最後狀態 可以使用構造函數省略默認值初始化. 當 python 類創建物件時。, 造函數將是第一個開始執行的代碼段，這使得所有初始化都作為程序的第一個工作實例發生。 可以使用和不使用參數來啟動構造函數.  概覽__init__()方法  __init__()方法創造物件: __init__()方法是物件導向編程的核心, 因為其能「創造物件(create objects)」\n  物件導向:\n   面向物件寫程式（OOP）是一種編程範式，包括定義物件並與它們互動。 物件是複雜變數和函式的集合，可用於表示按鈕、飛機或人等真實實體。 要在Python中宣告、初始化和操作物件，我們使用類。 它們作為建立物件的模板。 下圖說明了這個想法：  類別是物件的克隆藍圖:   類(Class)定義並構建從中建立的所有物件。 類為物件(Object)工廠。 類使用方法(Method)和建構式(Constructor)來建立和定義物件。  特殊方法(Special methods):   特殊方法透過其名稱兩側的雙下劃線標識，例如__init__。 Python使用特殊方法來增強類的功能。 他們中的大多數人在後臺工作，並在程式需要時自動呼叫。 您不能顯式呼叫他們。 例如，當您建立新物件時，Python會自動呼叫__new__方法，而該方法又會呼叫__init__方法。 當您列印（）物件時，會呼叫__str__方法。 另一方面，使用者定義的方法，如stefi.run()，被顯式呼叫。  構造式(Constructor):   建構函式是程式呼叫物件建立的特殊方法。 建構函式在類中用於將資料成員初始化到物件。 特殊方法__init__是Python建構函式。  類別物件兩大功能: 屬性引用, 實例化  官方文檔的「類別物件(Class Object)」\n  類別物件核心功能: 類別物件的兩種核心操作, 為「屬性引用(Attribute References)」與「實例化(Instantiation)」 .\n  屬性引用(Attribute References):\n   使用Python中所有屬性引用的標準語法：obj.name 有效屬性名稱(Valid attribute names): 建立類物件時「類名稱空間(Class\u0026rsquo;s namespace)」中的所有名稱。 定義實例:  1class MyClass: 2 \u0026#34;\u0026#34;\u0026#34;A simple example class\u0026#34;\u0026#34;\u0026#34; 3 i = 12345 4 5 def f(self): 6 return \u0026#39;hello world\u0026#39;  這裡的MyClass.i(類屬性)與MyClass.f(類方法)是有效的屬性引用. MyClass.i(類屬性) 返回「正整數物件(Integer Object)」 MyClass.f(類方法) 返回「函數物件(Function Object)」 可以透過指定, 來感變MyClass.i的數值 __doc__也是有效特徵, 返回文檔\u0026quot;\u0026quot;\u0026quot;A simple example class\u0026quot;\u0026quot;\u0026quot;  實例化(Instantiation):   把「類別物件」視為「無參數函數」來產生「新的類別實例(new instance of the class)」. 實例:  1x = MyClass()  實例化(Instantiation): 實例化操作創建空物件. 類別創造「具有特定初始值的實例物件」, 所以需要定義特殊方法__init__() 實例:  1def __init__(self): 2 self.data = []  如果一個類別有定義__init__()方法, 實例化就會自動啟動. 也就是x = MyClass()在操作的事情.  建構式實例化實務 例子: 「學生」類別, 「學生成績」類別:   所謂的「類別」, 可以很融通. 例如「學生」類別包含一個學生的「個人資訊」; 「學生成績」類別包含了一個學生的「課業表現」. 實例  1# 定義「學生」類別, 屬性為「個人資訊」 2class student: 3 def __init__(self,name,age): 4 self.name=name 5 self.age=age 6 7# 定義「學生成績」類別, 屬性為「課業表現」, 方法有「計算平均成績」 8class student_marks: 9 def __init__(self,name,english,maths,science,sst): 10 self.name=name 11 self.english=english 12 self.maths=maths 13 self.science=science 14 self.sst=sst 15 16 def calculate_avg_marks(self): 17 total_marks=self.english+self.maths+self.science+self.sst 18 avg_marks=total_marks/4 19 return avg_marks 20 21# 實例化四個「學生成績」物件 22student1=student_marks(\u0026#34;Ashwini\u0026#34;,20,12,14,15) 23student2=student_marks(\u0026#34;Ashu\u0026#34;,10,18,16,9) 24student3=student_marks(\u0026#34;Sonu\u0026#34;,16,14,20,11) 25student4=student_marks(\u0026#34;Sushant\u0026#34;,20,20,20,20) 26 27# 計算四個「學生成績物件」的平均分數 28print(student1.calculate_avg_marks()) 29print(student2.calculate_avg_marks()) 30print(student3.calculate_avg_marks()) 31print(student4.calculate_avg_marks())   這個點很有趣, 適當的構造「類別」可以有效管理複雜度. 任何東西不要超過3個點, 超過我們就分解, 也是管理複雜系統的重要心法.\n   例子:「學生」, 「員工」都是「人」:\n   活用「繼承」的概念, 可以實踐「學生」, 「員工」都是「人」這句大白話 實例  1### 定義「人」類別 2class Person(): 3 def __init__(self,name): 4 print(\u0026#34;Person Init called.\u0026#34;) 5 self.name = name 6 7### 定義「學生」類別, 基於「人」類別 8class Student(Person): 9 def __init__(self, name): 10 Person.__init__(self, name) 11 print(\u0026#34;Student Init called.\u0026#34;) 12 self.name = name 13 def display(self): 14 print(\u0026#34;Student\u0026#34;,self.name) 15 16### 定義「員工」類別, 基於「人」類別  17class Employee(Person): 18 def __init__(self, name): 19 Person.__init__(self, name) 20 print(\u0026#34;Employee Init called.\u0026#34;) 21 self.name = name 22 def display(self): 23 print(\u0026#34;Employee\u0026#34;,self.name) 24 25### 實例化Vicky學生 26student = Student(\u0026#39;Vicky\u0026#39;) 27student.display() 28 29### 實例化Vikas員工 30employee= Employee(\u0026#39;Vikas\u0026#39;) 31employee.display()  更好的實例:  1### 定義「人」類別 2class Person(): 3 def __init__(self,name): 4 print(\u0026#34;Person Init called.\u0026#34;) 5 self.name = name 6 7### 定義「學生」類別, 繼承「人」類別 8### 利用「父類別」的「構造式 `__init__()` 9class Student(Person): 10 def __init__(self, name): 11 print(\u0026#34;Student Init called.\u0026#34;) 12 self.name = name 13 Person.__init__(self, name) 14 15 def display(self): 16 print(\u0026#34;Student\u0026#34;,self.name) 17 18student = Student(\u0026#39;Vicky\u0026#39;) 19student.display()   或用這種繼承思想, code可以更簡潔, 管理複雜度.\n   感覺寫論文是不是也能這樣呢? 要把哪些sentence pattern 給抽象畫出來呢? 如何建立資料庫呢? Python for Latex之類的呢?\n  Super()方法:\n   在「子類別(Subclass)」裡面使用方法super()可以得到「超類別(Superclass)」裡面的「暫時物件(temporary)」. 這個暫時物件的方法, 可以被使用. 實例:  1### 定義「長方體」類別 2class Cuboid: 3 ### 構造「長寬高」屬性 4 def __init__(self, length, width,height): 5 self.length = length 6 self.width = width 7 self.height=height 8 ### 方法計算表面積 9 def total_surface_area(self): 10 return 2*(self.length * self.width +self.width*self.height+self.length*self.height) 11 12### 「立方體」類別, 寫法一: 不使用繼承  13class Cube: 14 def __init__(self, length): 15 self.length = length 16 17 def total_surface_area(self): 18 return 6*self.length * self.length 19 20### 「立方體」類別, 寫法二: 使用繼承 21class Cube(Cuboid): 22 def __init__(self, length): 23 super().__init__(length,length,length) 24 25cuboid=Cuboid(5,4,3) 26print(cuboid.total_surface_area()) 27cube=Cube(5) 28print(cube.total_surface_area()) 後記  到此回顧了一些__init__()的技術細節, 以及實例思考「繼承」的一些用法.\n  感覺寫著寫著偏離了「構造式」的原主題, 但因為這邊的水很深, 所以100分鐘大概就cover這些內容. 我們寫作也要懂得取捨, 一步一腳印慢慢往前. 天天向上, 共勉之！\n  2022.01.16. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-16","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur018-%E6%B2%88%E6%80%9Dpython%E5%BB%BA%E6%A7%8B%E5%BC%8F/","series":["每日文章","編程思維"],"tags":["Python"],"title":"MUR018 沈思Python建構式"},{"categories":[],"content":"思考音程與音階  紫式晦澀每日一篇文章第17天\n 前言   今天是2022年第15天, 全年第2週, 一月的第三個週六. 今天要上音樂課, 把音程與音階的概念仔細想一次.\n  今天的素材主要網路搜尋各種關於音程與音階的資訊.\n   什么是音程？  音程之定義與各類音程的判別方式  維基百科  兩個基礎: 五線譜與十二平均律. 五線譜  五線譜 (Staff): 五線譜是一組, 由五條線與四個間組成的, 每個代表不同「音高 (Pitch)」.   組合五線譜 (System):\n   多個五線譜, 可以組成一個系統(System), 表達每個五線譜的樂器同時眼咒. 中括號(Bracket)代表同個樂器不同的五線譜的系統. 大括號(Brace)代表不同樂器組合成的系統.  總譜 (Grand Staff): 利用中括號聯合的兩個五線譜, 稱為「總譜」.  十二平均律 十二平均律(Equal temperament):音樂律式的一種，也是當今最主流的律式。   將一個八度平均分成十二等份，每等分稱為半音，音高八度音指的是頻率乘上二倍。 八度音的頻率分為十二等分，即是分為十二項的等比數列，也就是每個音的頻率為前一個音的2的12次方根   十二平均律表: 將主音設為a1(440Hz)，來計算所有音的頻率\n   音程名稱 間隔半音數 音程品質     純一度($A^1$) 0 極完全協和音程   增一度/小 二度 ($A\\sharp^{1}$//$B\\flat^{1}$) 1 不協和音程   大二度 (B^(1)) 2 不協和音程   小三度(C) 3 不完全協和音程   大三度 ($C\\sharp$) 4 不完全協和音程   純四度(D) 5 完全協和音程   增四度/減五度 ($D\\sharp^{1}$//$E\\flat^{1}$) 6 不協和音程   純五度(E) 7 完全協和音程   小六度(F) 8 不完全協和音程   大六度 ($F\\sharp$) 9 不完全協和音程   小七度(G) 10 不協和音程   大七度 ($G\\sharp$) 11 不協和音程   純八度(A) 12 極完全協和音程      從五線譜看十二平均律表\n   1）在度數為一的情況下，顯然，兩音的音高是一樣的，相隔的音數自然為零。最後將度數和音數一起考慮，我們把度數為一，音數為零的情況稱為：純一度。 2）在度數為二的情況下，兩音音高不同，相隔的音數有兩種情況。如果兩音是C和D，那麼它們相隔一個全音，我們稱音數為一；如果兩音是E和F，那麼它們相隔一個半音，我們稱音數為二分之一。最後將度數和音數一起考慮，我們把度數為二，音數為一的情況稱為：大二度。而把度數為二，音數為二分之一的情況稱為：小二度。 3）三度的情況跟二度是一致的。有大三度和小三度。 4）但四度與三度就不一致了！深層次的原因還是在於七個音名在十二平均律中分布的「不均勻」性。四度的分類不是大四度和小四度；而是純四度和增四度。 如果兩音是F和B，那麼它們相隔三個全音，我們稱音數為三。如果兩音是C和F，那麼它們相隔二又二分之一個全音，我們稱音數為二又二分之一。前者稱為增四度，後者稱為純四度。 為什麼不叫大四度和小四度呢？我覺得可以這麼理解。四度的情況下，音數為三的情況只有一種，即F和B。而音數為二又二分之一的情況則有六種（C和F；D和G等等）。前者很特別，後者很常見，所以把後者稱為是「純」的。從統計上講，由於七音在十二平均律上分布的「不均勻性」，導致後者出現的概率大得多（度數為四，音數為二又二分之一），故命名為「純」。那麼前者由於音數比「純」多了，所以命名為「增」。這只是一種「可能」的理解方式。由於我不是音樂專業出身，這種理解方法也許是不正確的，我並不知道在音樂專業上是如何理解這件事情的。 5）五度的話，分為減五度（音數為三）和純五度（音數為三又二分之一）。與四度的分法類似。 6）六度的話，則分為小六度（音數為四）和大六度（音數為四又二分之一）。與二度、三度的分法一致。 7）七度與六度類似，分小七度和大七度。 8）八度只有純八度，與一度的分法類似。  音程的大小與性質 音程大小: 音程, 音級, 音數  定義音程(Interval): 音程（interval），指的是一個特定樂音體系中，兩个音之間音高的距離。\n  定義音級(來自五線譜): 音級(Interval Number)是指兩音之間包括幾個基本音。在樂理上，習慣以「度」來表示兩音之間的距離。 The number of an interval is the number of letter names or staff positions (lines and spaces) it encompasses, including the positions of both notes forming the interval.\n  五線譜上的音級: 在五線譜(Staff)中，每個「線(Lines)」、每個「間(Spaces)」都代表一個音級，當兩個音在同一個「線」或「間」時，稱為一度。若兩個音是在相鄰的「間」與「線」上，就稱為二度。其它度數以此類推。   定義音數(來自十二平均律): 半音數目是指音程內所含半音(Semitone)的個數。西方樂理中雖然有全音和半音的概念，但計算音程時，一直來都只採用單一的量度單位－半音。\n  比較音級與音數:\n   描述「音程」共有兩個維度：度數和音數。 度數是指兩個音在五線譜上的距離（度量單位為：線、間）。 音數是指兩個音在十二平均律上的距離（度量單位為：全音、半音）。  音程性質: 完全, 大, 小, 增, 倍增, 減, 倍減 音程性質(Interval Quality): 音程的性質, 有七種:     Perfect(P) : 完全音程    major(M): 大音程    minor(m): 小音程    augmented(A): 增音程    double augmented: 倍增音程    diminished(d): 減音程    double diminished: 倍減音程    完全音程 (Perfect Interval):   完全音程（Perfect Intervals）：完全音程只有四種，包括：同度（Unison）、完全四度（Perfect Forth）、完全五度（Perfect Fifth）與完全八度（Perfect Octave）。 「完全音程」之所以稱作完全音程，是因為在中世紀與文藝復興時期的音樂中，這四個音程最適合用於一樂曲的終止式，這包含了一樂句裡頭較小型的與樂段中較規模的終止式。   大音程(Major Interval):   大音程只有四種，包括：大二度、大三度、大六度與大七度。 「大音程」之所以稱作「大音程」是因為它們來自於一大調音階中，各個音（扣掉上述四個完全音程之後）與主音所形成的關係。   小音程(Minor Interval):   小音程：小音程只有四種，包括：小二度、小三度、小六度與小七度。 「小音程」之所以稱作「小音程」是因為它們來自於一小調音階中，各個音（扣掉上述四個完全音程之後）與主音所形成的關係。   增音程(Augmented Interval):   增音程：比完全音程以及大音程寬一個半音的音程，稱為「增音程」。 增音程共有八種。 「增音程」之所以稱作增音程，係由於它們是以上述之完全音程與大音程作為基礎，擴大一個半音後所得之音程。  倍增音程(Double Augmented Interval):   倍增音程：比增音程再寬一個半音的音程，稱為「倍增音程」。 倍增音程共有八種。 「倍增音程」之所以稱作倍增音程，係由於它們是以上述之完全音程與大音程作為基礎，擴大兩個半音後所得之音程。  減音程(Diminish Interval):   減音程：比完全音程以及小音程窄一個半音的音程。 「減音程」之所以稱作減音程，係由於它們是以上述之完全音程與小音程作為基礎，減小一個半音後所得之音程。  倍減音程(Double Diminish Interval):   倍減音程：比減音程再窄一個半音的音程。 「倍減音程」之所以稱作倍減音程，係由於它們是以上述之完全音程與大音程作為基礎，擴大兩個半音後所得之音程。  協和音程與不協和音程:   1）極完全協和音程：純一、八度。就是聽起來就像聽同一個音一樣。 2）完全協和音程：純四、五度。 3）不完全協和音程：大、小三度，大、小六度。聽起來感覺還不錯，悅耳。 4）不協和音程：聽起來不和諧。大、小二度，大、小七度，所有的增、減、倍增、倍減音程。 前三類聽起來都悅耳，統稱協和音程。 這四類音程是「和聲學」的基石，判斷音程的協和性是非常重要的。  音階: 自然音階與半音音階 自然音階  音階(Music Scale): 按照泛音列(Harmonic series)或音高(Pitch)排列，並因其規律形成循環的一系列音符。一個音階中的音符按照音高的上升而排列便是上行音階，若音符按照音高的下降而排列則是下行音階。\n  自然音階(Diatonic scale): 自然音階, 是任何包含五個「全音(Whole tones)」與兩個「半音(Semitones)」, 共十二個半音, 的「七音音階 (Heptatonic scale)」.\n   唱名: C–D–E–F–G–A–B\n 七音高 (Seven pitches):自然音階的七音高, 可以由「一串六個完全五度 (chain of six perfect fifths)」來得到   隔五度: F–C–G–D–A–E–B\nFa-Do-So-Re-La-Mi-Si\n  倒轉格五度: B-E-A-D-G-C-F\nSi-Mi-La-Re-So-Do-Fa\n  完全五度圈\n 半音音階 半音音階(Chromatic Scale): 有12個不同音高順序排列的音階. 稱為十二音音階(twelve-tone scale), 每個差半音.  鋼琴12健 七個白鍵, 五個黑鍵.  後記  到此整理思考了音程與音階的相關資訊. 從五線譜與十二平均律, 對應了自然音階的七音音階與半音音階的十二音音階. 在音程的大小與性質方面, 音程是兩個音之間的音高, 音級是兩個音之間包含多少個自然音(七音系統), 音數則是中間包含多少個半音(十二音系統).\n  對於大音程與小音程, 之前有口訣是「全全半全全全半」與「全半全全半全全」兩種. 據說是對應大調音階與小調音階. 這個部分還很陌生, 之後與老師討論增強理解.\n  這篇文章把各種複雜的音樂基礎知識做了爬梳, 非常好！維持寫文章整理資訊的習慣, 可以有效地加深每次問題的深度！共勉之！\n  2022.01.15. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-15","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur017-%E6%80%9D%E8%80%83%E9%9F%B3%E7%A8%8B%E8%88%87%E9%9F%B3%E9%9A%8E/","series":["每日文章"],"tags":["音樂"],"title":"MUR017 思考音程與音階"},{"categories":["合成數據","可信任的AI"],"content":"思考Chase合成數據技術細節  紫式晦澀每日一篇文章第16天\n 前言   今天是2022年第14天, 全年第2週, 一月的第二個週五. 一月也過了一半, 時間真快！\n  今天的素材主要來自Chase關於synthetic data的技術介紹 . 藉由學習裡面做事邏輯來思考合成數據的技術細節.\n  大框架與工作流: 七步從真實到合成  背景: 研究開發演算法, 以合成逼真的合成數據集. 有四類金融任務: 反洗錢, 客戶旅程事件, 市場執行資料, 付款與詐欺偵測.\n  技術文章: 文章為Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls . 共十頁, 可系統讀讀學習.\n  工作流: 共有七步\n   a. 計算「真實數據的度量(metrics for the real data)」 b. 「發展合成器 (Develop a Generator)」, 可基於統計方法或基於代理人的模擬. c. 使用真實數據「校準合成器 (Calibrate the Generator) d. 「跑合成器(Run the Generator)」來生成合成數據 e. 計算「合成數據的度量(metrics for the synthetic data)」 f. 真實數據與合成數據「比較度量(Compare the metrics)」 g. 「改良合成器 (Refine the Generator)」來改進比較的度量  實際金融場景: 反洗錢, 客戶旅程, 市場執行, 付款資料詐欺偵測 反洗錢（AML）: 洗錢是將來自非法活動的資金引入金融系統以將其用於合法或非法目的的過程。 這些資料代表了合法客戶和從事洗錢活動的客戶與金融機構的高級別互動的順序。 當前資料包含銀行客戶相關活動的狀態和行動對。 例如開立賬戶、進行交易、付款、提款、購買等。 資料是透過執行人工智慧規劃-執行模擬器生成的。   References\n Generating Synthetic Data in Finance: Opportunities, challenges and pitfalls. S Assefa, D Dervovic, M Mahfouz, R Tillman, P Reddy, T Balch and M Veloso. Proceedings of the 1st International Conference on AI in Finance (ICAIF), 2020. Also in NeurIPS 2019 Workshop on AI in Financial Services Simulating and classifying behavior in adversarial environments based on action-state traces: An application to money laundering, D Borrajo, M Veloso, S Shah. Proceedings of the 1st International Conference on AI in Finance (ICAIF), 2020. Also in arXiv preprint arXiv:2011.01826, 2020   客戶旅程活動: 客戶旅程事件代表了低階零售銀行客戶與銀行互動的順序。 事件的示例型別包括登入Web應用程式、付款、從自動取款機取款。 資料是透過執行人工智慧規劃執行模擬器並將輸出規劃跟蹤轉換為表格格式生成的。   References\n Generating Synthetic Data in Finance: Opportunities, challenges and pitfalls. S Assefa, D Dervovic, M Mahfouz, R Tillman, P Reddy, T Balch and M Veloso. Proceedings of the 1st International Conference on AI in Finance (ICAIF), 2020. Also in NeurIPS 2019 Workshop on AI in Financial Services Domain-independent generation and classification of behavior traces. D Borrajo and M Veloso. arXiv preprint arXiv:2011.02918.   市場執行資料: 綜合限額訂單簿資料，描述公共證券交易所各種市場參與者的一系列金融工具（股票）買賣訂單。 具體來說，這些資料將包含一段時間內訂單的訊息和快照。 這些資料代表了不同市場制度下高流動性股票模擬資料的N個交易日（例如，趨勢上升/下降，高/低波動）。   References\n Generating Synthetic Data in Finance: Opportunities, challenges and pitfalls. S Assefa, D Dervovic, M Mahfouz, R Tillman, P Reddy, T Balch and M Veloso. Proceedings of the 1st International Conference on AI in Finance (ICAIF), 2020. Also in NeurIPS 2019 Workshop on AI in Financial Services Get Real: Realism Metrics for Robust Limit Order Book Market Simulations. S. Vyetrenko et al. Proceedings of the 1st International Conference on AI in Finance (ICAIF), 2020.   用於欺詐檢測的付款資料: 從以主題為中心的角度代表交易的資料，目的是識別欺詐交易。 這些資料包含大量交易型別，代表正常活動以及以預定義概率引入的異常/欺詐活動。 資料是透過執行人工智慧規劃執行模擬器並將輸出規劃跟蹤轉換為表格格式生成的。 資料生成模型的引數包括客戶端數量、時間持續時間和欺詐概率。   References\n Generating Synthetic Data in Finance: Opportunities, challenges and pitfalls. S Assefa, D Dervovic, M Mahfouz, R Tillman, P Reddy, T Balch and M Veloso. Proceedings of the 1st International Conference on AI in Finance (ICAIF), 2020. Also in NeurIPS 2019 Workshop on AI in Financial Services Domain-independent generation and classification of behavior traces. D Borrajo and M Veloso. arXiv preprint arXiv:2011.02918.   合成器技術: 表格數據, 流數據, 金融時間序列, 非結構化圖片與聲音. 生成的資料種類: 主要需要合成的數據具有「隱私保證(Privacy guarantees)」. 產生的數據類型有:     表格數據 (Tabular Data)    合成金融時間序列 (Synthetic financial time series)    具有隱私保證的流數據 (Stream data with privacy guarantees)    非結構化數據(Unstructured Data)    表格數據 (Tabular Data): 有很多被研究的方法.     合成數據定義語言(SDDL; Synthetic data definition language). 缺點: 無隱私保證.    經典機器學習分類器 (Classical machine learning classifiers): 支撐向量機, 隨機森林. 缺點: 準確度高的模型, 容易洩露隱私, 也無法調整隱私參數.    貝氏差分隱私合成數據(Bayesian differential privacy synthetic data). 有效. 缺點: 不易規模化    耦合(Copula): 用直方圖產生分佈相同的合成數據. 缺點: 不易規模化.    Gibbs取樣 (Gibbs sampling): 強隱私保證, 容易規模化. 缺點: 只限定於類別變數.    隱私保護生成數據的共通限制:   難以規模化: 大多數差分隱私框架將給定表的一行表示為長度等於域大小的位串，它在表的列數中呈指數增長。 這種表示很快變得不實用。 高維度資料集很稀疏, 加了噪聲加密後就喪失了訊號: 這種表示的第二個限制是大多數高維數據集非常稀疏，導致添加噪聲以生成隱私完全沖刷了真實數據，使得發布的數據集不適合作為真實數據集的近似值。 在 Zhang ([25] Jun Zhang. Algorithms for Synthetic Data Release under Differential Privacy. PhD thesis, Nanyang Technological University, 2016.)中可以找到對這些限制的更徹底的討論。   基於代理人建模(ABM; Agent-based modelling): 建模「銀行付款處理系統(bank\u0026rsquo;s payment processing system), 調查惡意事件的宏觀影響. 要校準模型來滿足隱私限制, 但還沒有被廣泛研究.\n  合成金融時間序列 (Synthetic financial time series):\n   有很多於差分隱私資料流合成金融時間序列的研究, 但很少有隱私保護. 一般是用時間序列模型, 最大似然方法來做. 容易做, 好解釋, 但需要強假設, 無法再產生各種金融時間序列的統計特徵. 近代的方法有QuantGAN來合成股票的對數利潤. 但沒有提供隱私保證, 無法記得資料的特徵. 基於代理人模擬, 會重現金融市場的動態, 以導出金融時間序列.  具有隱私保證的流數據 (Stream data with privacy guarantees):   流數據: bit string. 有很多細節, 可以使用到隱私. 可以跟之前洗牌的隱私工作比較, 來做各種研究.  非結構化數據(Unstructured Data):   產生「圖片(images)」與「聲音(audio)」的差分隱私合成數據. 這些方法還有很多神經網路的問題未突破. 這些方法是在個人層級保護資料, 但這種隱私保護不夠強.  噪音與隱私權衡: 如何以最優的方式權衡噪音與隱私? ([9] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Shai Halevi and Tal Rabin, editors, Theory of Cryptography, pages 265–284, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.)  後記  延伸閱讀: 在MUR012 合成數據於Chase銀行的金融應用 我們有討論過金融科技的應用. 當初的場景洞察沒有提供太多詳細解釋, 今天的文章補齊了這個方面的缺漏.\n  到此翻譯思考了Chase關於synthetic data的技術介紹 中的技術細節. 這次寫完文章覺得對具體場景有更加認識. 其實, 現在的元宇宙中的金融行為非常多, 而各種金融機構也都有記錄各種數據. 在隱私保護的驅動下, 如何做新一個時代的資料科學, 這是一個重要值得研究的問題, 我覺得很有意思.\n  之後寫文章可以往更細緻的論文去寫. 現在覺得對「形式化描述與結果」做「知識卡片」就可以, 對「文字化邏輯與論證」做「豆腐塊文章」的效果會更大. 的確, 我們要根據論據的不同做不同的處理, 才會有更好的工作流.\n  寫文章真的是思考最好的方式. 每一次的輸入, 範式處理,轉成輸出, 都讓我們更具體的將資訊以自己最適合的形式儲備為知識. 三十代的結構與價值之魅, 共勉之！\n  2022.01.14. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-14","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur016%E6%80%9D%E8%80%83chase%E5%90%88%E6%88%90%E6%95%B8%E6%93%9A%E6%8A%80%E8%A1%93%E7%B4%B0%E7%AF%80/","series":["每日文章","編程思維"],"tags":["金融科技"],"title":"MUR016 思考Chase合成數據技術細節"},{"categories":["物件導向編程"],"content":"從物件導向思考線性迴歸分析  紫式晦澀每日一篇文章第15天\n 前言   今天是2022年第14天, 全年第2週, 一月的第二個週四. 昨日生成自己的「元工作流」以後, 感覺安排任務好很多, 可以更有精力去規劃任務!\n  今天的素材主要來自Datacamp課程-Object-Oriented Programming in Python 結合之前的研究經驗. 目標是用物件導向編程的角度來重新思考線性回歸方法.\n  物件導向基礎: 克隆的藍圖 編程範式: 從單純系統到複雜系統 過程式編程: 我們那個年代一開始程式教育的編程模式.   過程式程序設計（英語：Procedural programming），又稱過程化編程，一種編程典範，衍生自指令式編程，有時會被視為是同義語。 主要要採取過程調用或函數調用的方式來進行流程控制。  過程式編程的好處: 比較符合我們傳統的數學教育, 但對現代的Python編程就產生gap了.   Code as a sequence of steps 代碼=一串動作 Great for data analysis 適合資料分析   物件導向編程: 在物件導向程式編程裡，電腦程式會被設計成彼此相關的物件。 面向對象程序設計可以看作一種在程序中包含各種獨立而又互相調用的對象的思想.\n  物件導向編程的好處: 我們這個時代的編程思維.\n   Code as interactions of objects 代碼=物件之間的互動 Great for building frameworks and tools 建立框架與工具 Maintainable and reusable code! 好維護, 好重複使用   單純系統: 過程式編程 複雜系統: 物件導向編程\n 物件 as 資料結構; 類 as 藍圖  原則: 封裝 (Encapsulation): bundling data with code operating on it. 將「資料」與「作用他的代碼」捆綁在一起.\n  物件(Object): 定義物件, 思考其會有的狀態, 以及他需要的行為.\n   物件 = 狀態 + 行為 Object = State + Behavior\n 類(Class): 物件可能的「狀態」與「行為」的藍圖. Blueprint for objects outlining possible states and behaviors.  物件導向Python實踐: 狀態, 屬性, 變數; 行為, 方法, 函數. Python中的物件: 找到任何看到的Python code背後的藍圖.   Python中什麼都是「物件」. Everything in Python is an object. 所有物件背後都有「類」. Every object has a class. 用內建函數type()可以找到「物件背後的類」. Use type() to find the class  物件=狀態+行為; 狀態 as 物件屬性; 行為 as 物件方法 召喚Python物件資訊: 利用obj.可以讀取「屬性(attributes)」與「方法 (methods)」.  物件=狀態+行為; 數學物件=變數+函數; 編程物件=屬性+方法 類比物件導向編程與數學: 屬性對應變數; 方法對應函數   attribute 對應 variables 對應 obj.my_attribute method 對應 function() 對應 obj.my_method() 用內建函數dir()查詢所有的屬性與方法.   練習使用type()與dir()\n 以物件導向理解線性迴歸模組: type(), dir()與help() 實踐物件導向思維於LinearRegression模組:  1# Program 01: Run linear regression. 2import numpy as np 3from sklearn.linear_model import LinearRegression 用type(LinearRegression)找到線性模型的藍圖(狀態與行為的封裝) 執行type(LinearRegression): 會跑出很簡潔的abc.ABCMeta  1type(LinearRegression) 2abc.ABCMeta 用dir(LinearRegression)找到線性模型的「狀態」與「行為」 執行dir(LinearRegression): 會跑出一大串類方法(class methods)! 其中有私有方法(Private method)也有公用方法(Public method).  1dir(LinearRegression) 2[\u0026#39;__abstractmethods__\u0026#39;, 3 \u0026#39;__class__\u0026#39;, 4 \u0026#39;__delattr__\u0026#39;, 5 \u0026#39;__dict__\u0026#39;, 6 \u0026#39;__dir__\u0026#39;, 7 \u0026#39;__doc__\u0026#39;, 8 \u0026#39;__eq__\u0026#39;, 9 \u0026#39;__format__\u0026#39;, 10 \u0026#39;__ge__\u0026#39;, 11 \u0026#39;__getattribute__\u0026#39;, 12 \u0026#39;__getstate__\u0026#39;, 13 \u0026#39;__gt__\u0026#39;, 14 \u0026#39;__hash__\u0026#39;, 15 \u0026#39;__init__\u0026#39;, 16 \u0026#39;__init_subclass__\u0026#39;, 17 \u0026#39;__le__\u0026#39;, 18 \u0026#39;__lt__\u0026#39;, 19 \u0026#39;__module__\u0026#39;, 20 \u0026#39;__ne__\u0026#39;, 21 \u0026#39;__new__\u0026#39;, 22 \u0026#39;__reduce__\u0026#39;, 23 \u0026#39;__reduce_ex__\u0026#39;, 24 \u0026#39;__repr__\u0026#39;, 25 \u0026#39;__setattr__\u0026#39;, 26 \u0026#39;__setstate__\u0026#39;, 27 \u0026#39;__sizeof__\u0026#39;, 28 \u0026#39;__str__\u0026#39;, 29 \u0026#39;__subclasshook__\u0026#39;, 30 \u0026#39;__weakref__\u0026#39;, 31 \u0026#39;_abc_impl\u0026#39;, 32 \u0026#39;_check_feature_names\u0026#39;, 33 \u0026#39;_check_n_features\u0026#39;, 34 \u0026#39;_decision_function\u0026#39;, 35 \u0026#39;_estimator_type\u0026#39;, 36 \u0026#39;_get_param_names\u0026#39;, 37 \u0026#39;_get_tags\u0026#39;, 38 \u0026#39;_more_tags\u0026#39;, 39 \u0026#39;_preprocess_data\u0026#39;, 40 \u0026#39;_repr_html_\u0026#39;, 41 \u0026#39;_repr_html_inner\u0026#39;, 42 \u0026#39;_repr_mimebundle_\u0026#39;, 43 \u0026#39;_set_intercept\u0026#39;, 44 \u0026#39;_validate_data\u0026#39;, 45 \u0026#39;fit\u0026#39;, 46 \u0026#39;get_params\u0026#39;, 47 \u0026#39;predict\u0026#39;, 48 \u0026#39;score\u0026#39;, 49 \u0026#39;set_params\u0026#39;] 用help(LinearRegression)找到「公用」的屬性與方法 公用方法(Public Method): 注意沒加底線的那些方法, 就是我們讀官方指南會先出現的, 給「用戶」所使用的方法\n1 \u0026#39;fit\u0026#39;, 2 \u0026#39;get_params\u0026#39;, 3 \u0026#39;predict\u0026#39;, 4 \u0026#39;score\u0026#39;, 5 \u0026#39;set_params\u0026#39; 這些與官方API解釋的方法一致: 執行help(LinearRegression): 會出現詳細的文檔, 對公用的屬性與方法做詳細解釋  1HelponclassLinearRegressioninmodulesklearn.linear_model._base:23classLinearRegression(sklearn.base.MultiOutputMixin,sklearn.base.RegressorMixin,LinearModel)4|LinearRegression(*,fit_intercept=True,normalize=\u0026#39;deprecated\u0026#39;,copy_X=True,n_jobs=None,positive=False)5|6|OrdinaryleastsquaresLinearRegression.7|8|LinearRegressionfitsalinearmodelwithcoefficientsw=(w1,...,wp)9|tominimizetheresidualsumofsquaresbetweentheobservedtargetsin10|thedataset,andthetargetspredictedbythelinearapproximation.11|12|Parameters13|---------- 14| fit_intercept : bool, default=True 15|Whethertocalculatetheinterceptforthismodel.Ifset16|toFalse,nointerceptwillbeusedincalculations17|(i.e.dataisexpectedtobecentered).18|19|normalize:bool,default=False20|Thisparameterisignoredwhen``fit_intercept``issettoFalse.21|IfTrue,theregressorsXwillbenormalizedbeforeregressionby22|subtractingthemeananddividingbythel2-norm.23|Ifyouwishtostandardize,pleaseuse24|:class:`~sklearn.preprocessing.StandardScaler`beforecalling``fit``25|onanestimatorwith``normalize=False``.26|27|..deprecated::1.028|`normalize`wasdeprecatedinversion1.0andwillbe29|removedin1.2.30|31|copy_X:bool,default=True32|IfTrue,Xwillbecopied;else,itmaybeoverwritten.33|34|n_jobs:int,default=None35|Thenumberofjobstouseforthecomputation.Thiswillonlyprovide36|speedupincaseofsufficientlylargeproblems,thatisiffirstly37|`n_targets\u0026gt;1`andsecondly`X`issparseorif`positive`isset38|to`True`.``None``means1unlessina39|:obj:`joblib.parallel_backend`context.``-1``meansusingall40|processors.See:term:`Glossary\u0026lt;n_jobs\u0026gt;`formoredetails.41|42|positive:bool,default=False43|Whensetto``True``,forcesthecoefficientstobepositive.This44|optionisonlysupportedfordensearrays.45|46|..versionadded::0.2447|48|Attributes49|---------- 50| coef_ : array of shape (n_features, ) or (n_targets, n_features) 51|Estimatedcoefficientsforthelinearregressionproblem.52|Ifmultipletargetsarepassedduringthefit(y2D),this53|isa2Darrayofshape(n_targets,n_features),whileifonly54|onetargetispassed,thisisa1Darrayoflengthn_features.55|56|rank_:int57|Rankofmatrix`X`.Onlyavailablewhen`X`isdense.58|59|singular_:arrayofshape(min(X,y),)60|Singularvaluesof`X`.Onlyavailablewhen`X`isdense.61|62|intercept_:floatorarrayofshape(n_targets,)63|Independentterminthelinearmodel.Setto0.0if64|`fit_intercept=False`.65|66|n_features_in_:int67|Numberoffeaturesseenduring:term:`fit`.68|69|..versionadded::0.2470|71|feature_names_in_:ndarrayofshape(`n_features_in_`,)72|Namesoffeaturesseenduring:term:`fit`.Definedonlywhen`X`73|hasfeaturenamesthatareallstrings.74|75|..versionadded::1.076|77|SeeAlso78|-------- 79| Ridge : Ridge regression addresses some of the 80|problemsofOrdinaryLeastSquaresbyimposingapenaltyonthe81|sizeofthecoefficientswithl2regularization.82|Lasso:TheLassoisalinearmodelthatestimates83|sparsecoefficientswithl1regularization.84|ElasticNet:Elastic-Netisalinearregression85|modeltrainedwithbothl1andl2-normregularizationofthe86|coefficients.87|88|Notes89|----- 90| From the implementation point of view, this is just plain Ordinary 91|LeastSquares(scipy.linalg.lstsq)orNonNegativeLeastSquares92|(scipy.optimize.nnls)wrappedasapredictorobject.93|94|Examples95|-------- 96| \u0026gt;\u0026gt;\u0026gt; import numpy as np 97|\u0026gt;\u0026gt;\u0026gt;fromsklearn.linear_modelimportLinearRegression98|\u0026gt;\u0026gt;\u0026gt;X=np.array([[1,1],[1,2],[2,2],[2,3]])99|\u0026gt;\u0026gt;\u0026gt;# y = 1 * x_0 + 2 * x_1 + 3 100|\u0026gt;\u0026gt;\u0026gt;y=np.dot(X,np.array([1,2]))+3101|\u0026gt;\u0026gt;\u0026gt;reg=LinearRegression().fit(X,y)102|\u0026gt;\u0026gt;\u0026gt;reg.score(X,y)103|1.0104|\u0026gt;\u0026gt;\u0026gt;reg.coef_105|array([1.,2.])106|\u0026gt;\u0026gt;\u0026gt;reg.intercept_107|3.0...108|\u0026gt;\u0026gt;\u0026gt;reg.predict(np.array([[3,5]]))109|array([16.])110|111|Methodresolutionorder:112|LinearRegression113|sklearn.base.MultiOutputMixin114|sklearn.base.RegressorMixin115|LinearModel116|sklearn.base.BaseEstimator117|builtins.object118|119|Methodsdefinedhere:120|121|__init__(self,*,fit_intercept=True,normalize=\u0026#39;deprecated\u0026#39;,copy_X=True,n_jobs=None,positive=False)122|Initializeself.Seehelp(type(self))foraccuratesignature.123|124|fit(self,X,y,sample_weight=None)125|Fitlinearmodel.126|127|Parameters128|---------- 129| X : {array-like, sparse matrix} of shape (n_samples, n_features) 130|Trainingdata.131|132|y:array-likeofshape(n_samples,)or(n_samples,n_targets)133|Targetvalues.WillbecasttoX\u0026#39;s dtype if necessary. 134| 135| sample_weight : array-like of shape (n_samples,), default=None 136| Individual weights for each sample. 137| 138| .. versionadded:: 0.17 139| parameter *sample_weight* support to LinearRegression. 140| 141| Returns 142| ------- 143| self : object 144| Fitted Estimator. 145| 146| ---------------------------------------------------------------------- 147| Data and other attributes defined here: 148| 149| __abstractmethods__ = frozenset() 150| 151| ---------------------------------------------------------------------- 152| Data descriptors inherited from sklearn.base.MultiOutputMixin: 153| 154| __dict__ 155| dictionary for instance variables (if defined) 156| 157| __weakref__ 158| list of weak references to the object (if defined) 159| 160| ---------------------------------------------------------------------- 161| Methods inherited from sklearn.base.RegressorMixin: 162| 163| score(self, X, y, sample_weight=None) 164| Return the coefficient of determination of the prediction. 165| 166| The coefficient of determination :math:`R^2` is defined as 167| :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual 168| sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v` 169| is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``. 170| The best possible score is 1.0 and it can be negative (because the 171| model can be arbitrarily worse). A constant model that always predicts 172| the expected value of `y`, disregarding the input features, would get 173| a :math:`R^2` score of 0.0. 174| 175| Parameters 176| ---------- 177| X : array-like of shape (n_samples, n_features) 178| Test samples. For some estimators this may be a precomputed 179| kernel matrix or a list of generic objects instead with shape 180| ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted`` 181| is the number of samples used in the fitting for the estimator. 182| 183| y : array-like of shape (n_samples,) or (n_samples, n_outputs) 184| True values for `X`. 185| 186| sample_weight : array-like of shape (n_samples,), default=None 187| Sample weights. 188| 189| Returns 190| ------- 191| score : float 192| :math:`R^2` of ``self.predict(X)`` wrt. `y`. 193| 194| Notes 195| ----- 196| The :math:`R^2` score used when calling ``score`` on a regressor uses 197| ``multioutput=\u0026#39;uniform_average\u0026#39;`` from version 0.23 to keep consistent 198| with default value of :func:`~sklearn.metrics.r2_score`. 199| This influences the ``score`` method of all the multioutput 200| regressors (except for 201| :class:`~sklearn.multioutput.MultiOutputRegressor`). 202| 203| ---------------------------------------------------------------------- 204| Methods inherited from LinearModel: 205| 206| predict(self, X) 207| Predict using the linear model. 208| 209| Parameters 210| ---------- 211| X : array-like or sparse matrix, shape (n_samples, n_features) 212| Samples. 213| 214| Returns 215| ------- 216| C : array, shape (n_samples,) 217| Returns predicted values. 218| 219| ---------------------------------------------------------------------- 220| Methods inherited from sklearn.base.BaseEstimator: 221| 222| __getstate__(self) 223| 224| __repr__(self, N_CHAR_MAX=700) 225| Return repr(self). 226| 227| __setstate__(self, state) 228| 229| get_params(self, deep=True) 230| Get parameters for this estimator. 231| 232| Parameters 233| ---------- 234| deep : bool, default=True 235| If True, will return the parameters for this estimator and 236| contained subobjects that are estimators. 237| 238| Returns 239| ------- 240| params : dict 241| Parameter names mapped to their values. 242| 243| set_params(self, **params) 244| Set the parameters of this estimator. 245| 246| The method works on simple estimators as well as on nested objects 247| (such as :class:`~sklearn.pipeline.Pipeline`). The latter have 248| parameters of the form ``\u0026lt;component\u0026gt;__\u0026lt;parameter\u0026gt;`` so that it\u0026#39;s249|possibletoupdateeachcomponentofanestedobject.250|251|Parameters252|---------- 253| **params : dict 254|Estimatorparameters.255|256|Returns257|------- 258| self : estimator instance 259|Estimatorinstance.後記  延伸閱讀: 在MUR009 思辨註釋與文檔 我們有討論過註釋與文檔的差異. 現在看起來, 可以把「文檔(documentation)」理解為「面向用戶, 對公用方法的解釋」; 而「註釋(comment)理解為「面向開發者, 對私有方法的技術註解」.\n  到此整合了Datacamp課程-Object-Oriented Programming in Python 的邏輯, 結合之前研究經驗, 用物件導向的世界觀來理解線性回歸模組.\n  本文章分了三個面向. 從抽象逐漸具體, 涵蓋心法, 技法, 用法三個層次.\n   第一個面向為「物件導向基礎: 克隆的藍圖」: 扮演心法的角色 第二個面向為「物件導向Python實踐: 狀態, 屬性, 變數; 行為, 方法, 函數.」: 扮演技法的角色 第三個面向為「以物件導向理解線性迴歸模組: type(), dir()與help(): 扮演用法的角色.  感覺寫文章真的是思考最好的方式, 每天不斷的輸出, 讓各種文章寫作過程shape我們的思維. 不斷迭代往前, 逐漸專業, 用自己的宇宙觸碰物理宇宙或元宇宙. 天天向上, 共勉之！  2022.01.13. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-13","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur015%E5%BE%9E%E7%89%A9%E4%BB%B6%E5%B0%8E%E5%90%91%E6%80%9D%E8%80%83%E7%B7%9A%E6%80%A7%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90/","series":["每日文章","編程思維"],"tags":["Python"],"title":"MUR015 從物件導向思考線性迴歸分析"},{"categories":[],"content":"從學習2Do軟體反思個人學術工作流  紫式晦澀每日一篇文章第14天\n 前言   今天是2022年第12天, 全年第2週, 一月的第二個週三. 昨日休息後, 今天精力旺盛. 早上思考2Do代辦事項軟體的思維邏輯, 獲得很大的啟發, 今天趕緊寫文章來記錄所思所想.\n  今天的素材主要來自2do的官方文檔 以及個人之前分享的學術工作流. 對於做事的方法, 在今天得到很大的突破！非常棒.\n  重新思考學術工作流: 目標, 計畫, 列表, 任務  學術工作流: 目標, 計畫, 列表, 任務: 從各種職場書籍裡面可以看到各式各樣的管理; 其核心差別其實是看事情的「格局(Scope)」, 也就是要達成的事情的複雜程度. 目前能夠理解到有四個階段: 目標, 計畫, 列表, 任務; 其時間的分佈大概為 3個月, 1個月, 1個禮拜, 1天. 掌握時間的顆粒度顯得十分十分的重要\n  目標, 計畫, 列表, 任務的心法: 感覺還差一步就可以上升到職業發展人生規劃的層次！\n   目標: 從「中期願景」到「短期目標」—\u0026gt;目標管理 計畫: 從「短期目標」到「抽象該做的事」\u0026ndash;\u0026gt; 項目管理 列表: 從「抽象該做的事」到「具體可做的事」—\u0026gt; 任務管理 任務: 從「具體可做的事」到「實際完成的事」—\u0026gt; 精力管理  2Do使用手冊組成邏輯: Meta-work 與Work; 管理列表(Lists)與 管理任務(Tasks)  列表是Meta-Work; 任務是Work: 這個是思維重大突破點; 當我們在安排一天的事情時, 就類似煮菜前的「備料」. 如果把「工作」比喻成「煮菜」, 那麼「元工作」就是「備料」. 當我們把元工作做好, 那實際在工作就可以減少精力磨損. 這個秘笈是「全神貫注」的關鍵.\n  管理列表: 做Pre-Work. 先把「抽象該做的事」切成「具體可做的事」.\n  管理任務: 做Work. 用「精力」完成「具體可做的事」.\n  元工作流(Meta-Workflow): 主要文件, 程式試錯, 知識生產, 每日文章  元工作流(Meta-Workflow): 之前分享學術工作流時, 說到的庫柏學習圈理論的四個階段; 這正好就可以成為「元工作」的四種類型, 藉此驅動「每日工作」; 而「每日工作」在具現化為「列表」, 結構化實際要執行的「任務」.\n  抽象概念化(思考; Abstract Conceptualization): 主要文件; 與「職涯發展」直接相關的, 工作的重要文件. 寫論文, 寫研究報告都屬於這個層次\n  具體經驗(感覺; Concrete Experience): 程式試錯; 任何「論證」類型的工作, 實際自己做一趟, 掃雷可能會卡住的地方, 排除未知的風險。復現他人程序的代碼, 復現他人文章的引理證明, 都屬於這個類型.\n  反思觀察(觀看; Reflective Observation):知識生產; 任何「獲取新知」類型的工作. 學習新的概念, 擴增個人知識庫, 同步最新的科學與技術. 讀文獻, 學課程, 接觸新的mindset, 都屬於這個類型.\n  主動驗證(執行; Active Experimentation):每日文章; 任何「快速輸出」類型的工作. 從心血來潮開始, 把收集到的資訊做「總結」, 「重排」, 「再組織」, 發現資訊後面更深一層的意義. 每日文章, 小總結, 科普文, 談話, 給課程, 都是屬於這個類型.\n  2Do待辦清單軟體細節: 用戶介面, User interface有五個成分: Toolbar, Lists Panel, Tasks list, Utility Panel, Calendar  Toolbar: 創造新的task, 新的project或sub-task; 搜尋; 查看Utility Panel\n  Lists Panel: 三個環節-Focus, Personal, Smart Lists\n  Tasks list: 展現各種輸入的Tasks\n  Utility Panel: 以Tags與Locations來過濾Tasks**\n  Calendar: 展示目前的時間\n  Lists Panel的三個環節: Focus, Personal, Smart Lists  Focus: 有「內建lists」來做filter. 分別有All, Today, Starred, Scheduled, Done.\n  Personal: 塗色以做collection. 可以收集 Tasks, Projects或Checklists.\n  Smart Lists: 一言以蔽之是「搜尋儲存(Saved searches)」. 可使用地點, 動態日期範圍, 固定日期, Tags, 關鍵字等等.\n  五種物件: Lists, List group, Tasks, Projects, Checklists  Lists: 列表是任務, 項目, 核對表的「收集」. 可以被排序, 過濾, 密碼保護, 塗色. A list is a collection of simple tasks, projects and checklists. Lists can be sorted independently, filtered, password protected and colored.\n  List group: 列表群是列表的「收集」. 可以用來組織與重整列表. A list group is a collection of lists. You can create multiple list groups and use them to organize and rearrange your lists.\n  Tasks: 任務是「代辦事項」且可以擁有「1.標題 2.備忘 3.優先級4. 起始日期 5.結束日期6. 語音備忘 7. 圖片. 8. 多重提醒 」. 任務可以「規律重複」. A task represents a to-do item and can have its own title, a note, priority, start and due dates, an embedded audio note, an embedded picture, and multiple alarms. A task can also be set to repeat periodically upon completion based on due date or completion date.\n  Projects: 項目也可以擁有「1.標題 2.備忘 3.優先級4. 起始日期 5.結束日期6. 語音備忘 7. 圖片. 8. 多重提醒 」另外, 項目可以包含「簡單任務」與「子任務」A project has all the properties of a simple task. Additionally, a project can contain other simple tasks as sub-tasks.\n  Checklists: 核對表是「單一執行的項目」: 子任務無法指定「到期/起始日」, 其「到期/起始日」繼承自核對表\n  後記  再見了，20代的資訊與形式之蠱\n歡迎來到30代的結構與價值之魅\n  到此思考整理了2do的官方文檔 以及個人之前分享的學術工作流, 所帶來的工作流思維重大突破.\n  寫作後寫出了「元工作流」這個系統, 非常酷而好用. 這個系統可以是學術工作流的2.0版本, 能夠自驅動任務產生與管理! 天天向上, 共勉之！\n  2022.01.12. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-12","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur014%E5%BE%9E%E5%AD%B8%E7%BF%922do%E8%BB%9F%E9%AB%94%E5%8F%8D%E6%80%9D%E5%80%8B%E4%BA%BA%E5%AD%B8%E8%A1%93%E5%B7%A5%E4%BD%9C%E6%B5%81/","series":["每日文章","學術工作流"],"tags":[],"title":"MUR014 從學習2Do軟體反思個人學術工作流"},{"categories":[],"content":"思考Obsidian外掛templater  紫式晦澀每日一篇文章第13天\n 前言   今天是2022年第11天, 全年第2週, 一月的第二個週二. 今天狀態不好, 臨時day off一天, 讓自己進入發散思維. 研究了Obsidian的外掛templater, 對「範本建立」的技術精進.\n  今天的素材主要來自templator的官方文檔 , 翻譯內文, 閱讀思考的紀錄.\n  簡介Templater  建立範本: 利用Templater可以建立範本, 快速帶入筆記的格式. 很適合執行一些常用的文本分析.\n  使用用戶自訂/內建的變數與函數: 妥善使用內建的變數與函數, 可以讀取很多檔案上的各種性質.\n  使用Eta範本引擎: 這個引擎不太熟悉, 之後有機會研究研究.\n  簡介入門  定義Templater: 一種「範本語言」讓我們能使用變數與函數帶入筆記. 可執行JavaScript代碼來操作這些變數與函數.\n  具體實例:從實例代碼來學習\n   \u0026lt;% tp.file.creation_date() %\u0026gt;: 檔案製造的日期 \u0026lt;% tp.file.last_modified_date(\u0026quot;dddd Do MMMM YYYY HH:mm:ss”): 檔案更新的日期 [[\u0026lt;% tp.date.now(\u0026quot;YYYY-MM-DD\u0026quot;, -1) %\u0026gt;]]: 現在前一天 [[\u0026lt;% tp.date.now(\u0026quot;YYYY-MM-DD\u0026quot;, 1) %\u0026gt;]]: 現在後一天 \u0026lt;% tp.file.title %\u0026gt;: 檔案的名字  術語入門  模板: 包含命令的檔案: A template is a file that contains commands.\n  命令: 被\u0026lt;% %\u0026gt;給包裝的變數與函數: A text snippet that starts with an opening tag \u0026lt;%, ends with a closing tag %\u0026gt; and that contains some variable / function is what we will call a command.\n  變數與函數: 可使用「命令」的物件, 能回傳「值」. 分別有「內建變數與函數」和「用戶定義函數」\n  內建變數與函數: 包已經定義好的變數與函數: They are predefined variables / functions that are built within the plugin. As an example, tp.file.title is an internal variable that will return the name of the file.\n  用戶定義函數: 看創意: Users can define their own functions in the plugin settings. They are either system command user functions or script user functions.\n  內建模組 內建模組: 有七種內建模組(Internal modules)   Config module: tp.config Date module: tp.date File module: tp.file Frontmatter module: tp.frontmatter Obsidian module: tp.obsidian System module: tp.system Web module: tp.web  學習object hierarchy: tp是一個物件. 命令的公式為\u0026lt;% tp.\u0026lt;module_name\u0026gt;.\u0026lt;internal_variable_or_function_name\u0026gt; %\u0026gt;  Config Module Config Module : 裡面的東西感覺暫時都用不到  Date Module Date Module : 昨天, 今天, 明天  File Module File Module : 創造檔案相關的東西, 感覺蠻有用的   tp.file.creation_date(format: string = \u0026quot;YYYY-MM-DD HH:mm\u0026quot;): Retrieves the file\u0026rsquo;s creation date. tp.file.last_modified_date(format: string = \u0026quot;YYYY-MM-DD HH:mm\u0026quot;) : Retrieves the file\u0026rsquo;s last modification date. tp.file.title: Retrieves the file\u0026rsquo;s title. tp.file.tags: Retrieves the file\u0026rsquo;s tags (array of string, comma separated)  Frontmatter Module Frontmatter Module : 可以抽取note的meta data! 所以很融通！   tp.frontmatter.\u0026lt;frontmatter_variable_name\u0026gt;: Retrieves the file\u0026rsquo;s frontmatter variable value.  1\u0026lt;% tp.frontmatter[\u0026#34;variable name with spaces\u0026#34;] %\u0026gt; 2 3--- 4alias: myfile 5note type: seedling 6--- 7 8file content 9 10File\u0026#39;s metadata alias: \u0026lt;% tp.frontmatter.alias %\u0026gt; 11Note\u0026#39;s type: \u0026lt;% tp.frontmatter[\u0026#34;note type\u0026#34;] %\u0026gt; Obsidian Module Obsidian Module : 沒有具體例子, 要回到Obsidian的API. 之後再研究如何使用.  System Module System Module : 這個用法要之後研究, 還沒經驗.  Web Module Web module : 主要用來抓隨機圖片當筆記每日封面   tp.web.random_picture(size?: string, query?: string): Gets a random image from https://unsplash.com/   後記  到此翻譯思考了templator的官方文檔 . 看來還是需要實踐才會有更具體的感覺. 這個之後來補.\n  希望之後能讓紀錄資訊的效率更高！時時編輯寫作, 時時傳達思想！共勉之!\n  2022.01.11. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-11","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur013%E6%80%9D%E8%80%83obsidian%E5%A4%96%E6%8E%9Btemplater/","series":["每日文章"],"tags":["Obsidian"],"title":"MUR013 思考Obsidian外掛templater"},{"categories":["合成數據","可信任的AI"],"content":"合成數據於Chase銀行的金融應用  紫式晦澀每日一篇文章第12天\n 前言   今天是2022年第10天, 全年第2週, 一月的第二個週一. 今天來累積金融應用的各種術語知識.\n  今天的素材主要來自Chase銀行的技術部落格 Synthetic Data for Real Insights 中, 翻譯內文, 閱讀思考的紀錄.\n  合成數據: 作為大數據時代隱私保護解決方案  合成數據的背景: J.P. Morgan AI Research 生成合成數據集，以加速金融服務領域的研究和模型開發。要使 AI 模型有效地展示業務場景中的人類行為，它們需要接受大量代表現實的數據的訓練。 金融服務行業會產生大量可能非常有益的數據，但這些數據通常無法使用。 這對研究人員和開發人員提出了根本性挑戰。\n  真實數據的限制: 真實數據在許多方面可能難以訪問，包括隱私、法律許可以及與數量、表示和含義相關的技術方面。\n   如何實現依賴數據的新產品和服務的創新和構建?\n一個答案是使用合成數據!\n  合成數據優點一: 替代真實數據, 但共享格式與分佈: 可以與真實數據共享格式、分佈和標準化內容，同時不會產生使用真實數據的風險。\n  合成數據優點二: 增加AI算法穩健性, 因可探索歷史數據外的場景:合成數據可能具有額外的好處，即代表歷史數據之外的探索性場景，以準備 AI 算法並支持在新情況下的決策。 因此，合成數據使我們能夠更加穩健地應對具有挑戰性的情況。\n  合成數據優點三: 增加罕見示例, 更有效訓練ML算法:合成數據可以增加真實數據中可能很少見的示例，以便更有效地訓練機器學習算法。\n  合成數據優點四: 生成測試場景, 快速驗證新想法效果:最終，如果一個新想法在合成數據上顯示出前景，我們可以考慮將其推進實際部署並在真實數據上使用。\n  訓練方法: 真實數據的「生成模式建模」或「深度神經網路學習」  不同數據類型, 不同合成方法: 通過研究，摩根大通的人工智能研究團隊確定了幾種創建合成數據的方法，並了解到不同的方法可能適用於不同類型的數據。\n  方法一: 真實數據生成模式建模(Generative Modeling): 我們可以通過了解生成真實數據的過程來創建真實的合成數據，然後對過程本身進行建模以生成合成數據。該模型可以是聲明性(declarative)的或在模擬中捕獲(captured in simulations)。\n  方法二: 真實數據深度神經網路學習: 直接使用真實數據來訓練生成神經網絡（GNN），它已成功用於生成各種其他合成數據。\n  場景洞察: 詐欺檢測, 異常偵測, 反洗錢, 客戶旅程, 市場執行  合成數據提供新洞察: 合成的新樣本具有真實數據的屬性，但無法映射回真實數據。新樣本提供了對可能未被發現的數據的洞察。\n  關鍵場景一:欺詐檢測(fraud detection)模型訓練:\n   合成數據為 AI 模型提供了正常(normal transactions)和欺詐交易(fraudulent transactions)的示例，以了解可疑交易模式(suspicious transaction patterns)。 由於與非欺詐案件相比，欺詐案件的數量非常少，因此建模方法難以從可用數據中有效地訓練關於欺詐行為的模型。  關鍵場景二:異常行為(anomalous behavior)模型訓練:   成數據可用於訓練異常行為(anomalous behavior)模型。 該過程使不符合預期行為的交易比例更高，從而生成更多欺詐案例的合成樣本，以改進模型訓練。   關鍵場景三:反洗錢 (Anti0money laundering) 行為\n  關鍵場景四:客戶旅程(Customer journey)事件\n  關鍵場景五:市場執行(Markets execution)數據\n  關鍵場景六:用於欺詐檢測的支付數據(Payments data)\n  研究社群: 用戶公司互動模擬, 強監管產業的新研究範式  合成數據: 本質上模擬用戶與公司的互動: 該公司人工智能研究主管 Manuela Veloso 回顧了該團隊在零售銀行業務中啟用的合成數據功能。 “例如，合成數據生成讓我們能夠思考客戶開戶和申請貸款的整個生命週期。我們不只是檢查數據以了解人們在做什麼，而且我們還能夠分析他們與公司的互動，並從本質上模擬整個過程。”\n  合成數據的工作延伸: 該團隊的合成數據工作已經發展。自 2 月份提供其合成數據集以來，該團隊已經收到了許多對這些功能的請求。此外，該公司在斯坦福大學、康奈爾大學、CMU、布法羅大學、紐約大學和其他大學的教師研究獲獎者 正在利用數據集開發算法，以解決欺詐和洗錢、客戶旅程、市場執行和金融領域的其他領域。\n  合成數據: 強監管產業的新研究範式:AI Research 執行董事 Rob Tillman 總結了其合成數據所面臨的挑戰及其優勢。 “在金融等處理敏感數據的高度監管行業中，通常存在重大障礙，阻礙或延遲研究人員和開發人員使用數據開發人工智能解決方案以改善體驗或解決欺詐檢測和反欺詐等重要問題的能力。洗錢。該團隊的合成數據工作旨在解決這個問題，加速摩根大通人工智能解決方案的開發，並促進與學術界的合作。”\n  後記  到此翻譯思考了Synthetic Data for Real Insights 大部分的內容. 首先合成數據作為大數據時代隱私保護解決方案, 希望能替代真實數據的使用, 成為新時代的可再生能源. 訓練方法上, 可以執行「生成模式建模」或「深度神經網路學習」. 使用的場景主要是詐欺偵測與異常偵測. 對於金融業界而言, 合成數據能夠模擬用戶與公司的互動, 有望成為強監管產業的新研究範式.\n  閱讀翻譯過這篇文章以後, 感覺AI for Finance是一個很好研究AI知識如何落地的很好的場景. 聯想到今天聽到的元宇宙是AI科技落地的一大場景, 感覺五年十年會有許多新的商業模式可以出來. 非常有趣.\n  每天加強自己的coding能力, 將思維從科學轉為工程, 是解決自己時代問題的負責任的態度！主動實踐, 創造個人經驗, 推動時代, 共勉之！\n  2022.01.10. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-10","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur012%E5%90%88%E6%88%90%E6%95%B8%E6%93%9A%E6%96%BCchase%E7%9A%84%E9%87%91%E8%9E%8D%E6%87%89%E7%94%A8/","series":["每日文章"],"tags":["金融科技"],"title":"MUR012 合成數據於Chase銀行的金融應用"},{"categories":["educative 課程"],"content":"思考Bash中的Globbing與Quoting  紫式晦澀每日一篇文章第11天\n 前言   今天是2022年第9天, 全年第2週, 一月的第二個週日. 今天來組織educative課程裡面關於Bash相關的知識.\n  今天的素材主要來自educative 課程-Master the Bash Shell-Core Bash 中, 節選相關段落與紀錄個人理解與心得.\n  Bash是shell程序 Bash是shell程序: shell 程序通常是一個「可執行二進製文件」. 它接受鍵入的命令，並且（一旦按return）將這些命令轉換為（最終）對操作系統 API 的系統調用。 (二進制: A binary is a file that contains the instructions for a program, ie it is a ‘program’ file, rather than a ‘text’ file, or an ‘application’ file (such as a Word document).)   Shell程序: 告訴電腦做什麼. 把不同的程序glue在一起.\n  其他的shell程序: 市面上還有其他的shell程序, 包含sh, ash, dash, ksh, tcsh, zsh, tclsh. 只有看過sh.\n  Bash的歷史: Bash原名 ‘Bourne Again SHell’. 是‘Thompson Shell’ 的後代. 其兄弟有‘siblings’ (eg, ksh), ‘cousins’ (eg, tcsh), and ‘children’, (eg, zsh).\n  Globbing與Quoting  *指令的意思: shell轉換*轉換, 配對到「current working directory的所有檔案」.\n  正規表達式:正規表達式是用來matching字串的模式. 與globbing達到的matching檔案不一樣.\n  glob: 在電腦編程中, glob模式指定一組帶著通配符的檔案名字 In computer programming, glob patterns specify sets of filenames with wildcard characters.\n  例子: 看所有在工作目錄中的檔案:\n     製造三個檔案file1, file2, file3.    用ls看存在的檔案, match *    用echo印出檔案, match *.    1touch file1 file2 file3 2ls * 3echo *  Quoting:用單引號「*」與雙引號「*」會改變bash讀取內容的方式. 在變數環節會學到更多.\n  Globbing primitive:\n   * - matches files in current working directory 工作目錄的檔案 ? - matches any single character 單字元 [abd] - matches any character from a, b or d 特定字元 [a-d] - matches any character from a, b, c or d 範圍內字元  實例: 配對檔案 找有1的, 有file數字的  1root@educative:~/glob# ls *1 2file1 3root@educative:~/glob# ls file[a-z] 4ls: cannot access \u0026#39;file[a-z]\u0026#39;: No such file or directory 5root@educative:~/glob# ls file[0-9] 6file1 file2 file3 7root@educative:~/glob# ls file? 8file1 file2 file3 Dotfiles: 造了也在ls, ls*看不到的檔案們. 這樣這些檔案不會被簡單改動  1root@educative:~/glob# touch .adotfile 2root@educative:~/glob# mkdir .adotfolder 3root@educative:~/glob# touch .adotfolder/file1 .adotfolder/.adotfile 4root@educative:~/glob# ls 5file1 file2 file3  單點.表示目前檔案夾: single dot folder . 表達目前在的檔案夾. cd . 就會在原地不動\n  雙點..表示雙親檔案夾: double dot folder .. 表達雙親檔案夾. cd ..會到上一層.\n  後記  到此學習了educative 課程-Master the Bash Shell-Core Bash 中前兩節的內容. 首先了解Bash是一種Shell 程序, 主要讓我們能與把不同的program一起合作. 接下來看了globbing, 關於*表達「目前工作目錄的所有檔案」有了基礎認識; 接著quoting則提到單引號雙引號, 會有不同的效果. 可惜還沒精力讀到variable的章節, 之後再來思考quoting的深刻用法.\n  今天看了很不熟的主題-Bash編程, 知道的很少, 所以得慢慢前進. 接下來一年要主力投資工程技能, 增加工程能力去做更寬廣的研究. 實踐數學, 用最新科技, 創造新價值, 推動時代. 天天向上, 共勉之！\n  2022.01.09. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-09","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur011%E6%A0%B8%E5%BF%83bash%E7%9F%A5%E8%AD%98/","series":["每日文章"],"tags":["Bash"],"title":"MUR011 思考Bash中的Globbing與Quoting"},{"categories":[],"content":"全新思維裡的三感三力  紫式晦澀每日一篇文章第10天\n  前言   今天是2022年第8天, 全年第2週, 一月的第二個週六. 今天在得到聽書聽到有趣的「全新思維」, 其中強調左腦思維的任務很多在被人工智慧給自動化,而未來的人會有更多的時間使用「右腦思維」才能達到的境界. 十分有意思, 也關係到我們平時如何分配自己的精力, 因此寫文章記錄所想所感.\n  今天的素材主要來自得到聽書-全新思維:決勝未來的六大因素 中, 節選相關段落與紀錄個人理解與心得.\n  三感: 設計感, 娛樂感, 意義感  設計感: 設計是一種能力, 要求我們有很高的審美標準, 去不斷地追求新穎和美感. 人們不會為物品買單, 而是為設計買單. 審美就是靠直覺體曲優化信息的能力, 也是人類無法被機器取代的能力. 要對美有所感悟, 才能脫離價格戰, 往更高的領域跑.\n  找到設計感: 多看, 用相機紀錄生活中覺得醜的東西, 想想醜的原因; 紀錄很美的東西, 把美的原因寫下來, 哪個元素吸引到你. 多參觀創憶博物館, 多讀設計雜誌, 多觀察生活. 美, 需要不斷地接近與陶冶.\n  娛樂感: 把娛樂像基因浸入你的血液中. 浸入一個文化中, 產生娛樂感. 「好玩」在現在已經是一個非常重要的核心競爭力.\n  找到娛樂感: 我覺得學習各種文化, 思考存在的各種事物背後的文化背景, 就可以與該文化的人互動產生共鳴.\n  意義感: 也就是自我實現. 你做的事情被不被別人需要, 有沒有價值, 你能不能把你做的事和一個偉大的東西聯繫 到一塊? 在末來, 意義感會變得更為重要, 所有有才能的人都會追求自我實現, 要追求自我價值的最 大化.\n  找到意義感: 學會把自己做的事嫁接到一個很偉大的目標上. 例如. 現在做研究工作, 會帶來知識, 讓我能快速學習, 有很大的自由度, 做創造性的動作. \u0026gt; 意義是自己找的, 需要自己賦予.\n  三力: 故事力, 交想力, 共情力  故事力: 在敘事中加入情緒, 吸引讀者. 用故事調動人的情緒.\n  鍛鍊故事力: 寫迷你小說. 寫一個100字左右的超短小說. 在100字表達人物, 關係, 鋪墊, 轉折四點, 練習編故事的能力.\n  交想力: 系統思維, 把看似無關的因素組合的能力. 右腦思考的特點, 注重大局, 不糾纏細節, 成為跨領域打劫的人才.\n  鍛鍊交想力: 思考「跨界思維」. 練習比喻, 把理解事物的層次昇華. 了解新事物, 利用比喻來轉化為熟悉的東西.\n  共情力: 要與別人產生共鳴. 右腦思維是體會對方的情緒, 然後引導. 共感對方的情緒, 接著轉到你想要導向的方向過去.\n  鍛鍊共情力: 練習去體會別人的情緒, 站在對方的角度想問題, 去感知別人, 和對方達到一個情緒和思想頻率, 是在未來與理智思考相當重要的能力.\n  後記  到此紀錄了得到聽書-全新思維:決勝未來的六大因素 中提到在未來使用「右腦思維」所需要的三感三力.\n  從設計感娛樂感意義感, 來看vtuber產業, 需要「皮」設計得好, 「中之人」要懂聽眾文化帶來娛樂, 而「意義感」則是在為未來元宇宙的世界積累實力. 我想vtuber在未來會是一個很有競爭力的履歷.\n  從故事力交想力共情力, 來看教育產業, 我想未來我們在講知識點要帶出「故事脈絡」才能使資訊昇華為解決具體問題的知識; 而「串連新與舊」來讓學生的舊經驗得到新體會, 加深學習效果; 而「共情力」則是去了解學生發生的困難點具體在哪邊, 與其共感後提供教練式領導, 引學生到想要的方向.\n  很棒的週六早上! 今天是每日一篇文章的第10天! 今天是一個思維的文章, 短短的, 但對許多mindset有額外的角度, 來思考自己的價值與時間精力分佈的方向. 讓每一個精神上的邂逅留下紀錄, 積極前進, 共勉之！\n  2022.01.08. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-08","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur010%E5%85%A8%E6%96%B0%E6%80%9D%E7%B6%AD%E8%A3%A1%E7%9A%84%E4%B8%89%E6%84%9F%E4%B8%89%E5%8A%9B/","series":["每日文章"],"tags":["得到聽書"],"title":"MUR010 全新思維裡的三感三力"},{"categories":["物件導向編程"],"content":"思辨註釋與文檔  紫式晦澀每日一篇文章第9天\n  前言   今天是2022年第7天, 第1週的第1個週五. 今天來思考紀錄, 在寫程式時很重要的, 關於「註解與文檔」相關的知識. 長期而言, 這是「編程思維」系列下的第一篇.\n  今天的素材主要來自Documenting Python Code: A Complete Guide 中, 節選相關段落與紀錄個人理解與心得.\n  註解與文檔: 為讀者服務 讀者思維於程式: 寫程式本身為兩種主要讀者服務. 第一種是開發者(包含未來的自己), 另一種是使用者. 作為開發者, 把註解(Comment)寫好, 也會讓未來的自己省時間. 作為使用者, 把文檔(Documentation)寫好, 會讓未來可能的使用者學習成本下降.   “Code is more often read than written.”\n— Guido van Rossum\n 定義註解: 「程式註解」是向開發者描述你的程式. 預期的主要受眾是 Python 程式的維護者和開發者。 編寫良好的程式+註釋有助於引導讀者更好地理解你的程式的「目的」和「設計」.   “Code tells you how; Comments tell you why.”\n— Jeff Atwood (aka Coding Horror)\n 定義文檔: 「程式文檔」是向使用者描述你的程式的用途與功能. 其主要的受眾為用戶, 也會在開發過程中有幫助.  註解: 面向開發者 註解的兩種技法: 井字號, 類型提示 利用#給註解: 註解應該是「簡潔的幾個句子」. 例如:  1def hello_world(): 2 # A simple comment preceding a simple print statement 3 print(\u0026#34;Hello World\u0026#34;) 其中的「# A simple comment preceding a simple print statement」就是簡潔的註解. 根據PEP 8 , 一個註解不超過72個字元.\n利用類型提示: 類型提示(Type Hint)於 Python 3.5 開始支援.類型提示是一種幫助讀者閱讀程式的附加形式。它允許開發人員設計和解釋他們的部分代碼而不用註釋。 例如  1def hello_name(name: str) -\u0026gt; str: 2 return(f\u0026#34;Hello {name}\u0026#34;) 就提示了函數hellp_name是一個輸入為字串str資料類型, 輸出也是字串str資料類型的函數.\n通過檢查類型提示，可以立即看出該函數期望輸入名稱為 str 或字符串類型。 此外, 還可以判斷該函數的預期輸出也將是 str 或 string 類型。 雖然類型提示有助於減少註釋，但在創建或更新項目文檔時，這樣做也可能會增加額外的工作量。   省現在的時間, 或省未來的時間.\n 註解的四種用法: 計畫與回顧, 程式描述, 演算法描述, 標記待改良點 註解用法一:計畫與回顧: 當開發程式的新部分，可以首先使用註釋作為「規劃(Planning)」或概述該部分程式。 一旦實際程式已經實踐和審查/測試，就可以刪除這些註解.  1# First step 2# Second step 3# Third step  這個用法, 在寫文章也很常用. 尤其是寫科學研究論文的草稿時, 用註解的方式先安排文章的結構, 然後分個完成. 完成後的部分, 之後重新組合, 可能可以出現更好的組織, 昇華文章帶給讀者的意象.\n  註解用法二:程式描述: 註解可解釋「特定部分程式」的意圖.\n  1# Attempt a connection based on previous settings. If unsuccessful, 2# prompt user for new settings.  這個用法, 在寫理論文章時, 因為要證明很多lemma, 也會利用註解來描述特定lemma的意圖. 因此, 我們可以很合理的類比「lemma」就是「script」 , 而很多的lemma與很多的script構造出智慧的結晶.\n  註解用法三:演算法描述: 複雜的演算法, 可以利用註解向開發者解釋如何實踐你的程式. 也可描述為何選擇特定的算法. 這個可以寫在function裡面詳細每步的動機.\n  1# Using quick sort for performance gains 註解用法四: 標記: 對特定部分的程式, 標記已知問題, 來改良該部分的問題. 實際例子有: BUG, FIXME 與 TODO.  1# TODO: Add condition for when val is None Jeff Atwood的四條註解規則: 就近註釋, 避免複雜格式, 開門見山, 設計好的程式結構  就近註釋原則: 使註釋盡可能靠近所描述的代碼。 不在描述代碼附近的註釋會讓讀者感到沮喪，並且在進行更新時很容易錯過。\n  避免複雜格式: 不要使用複雜的格式（例如表格或 ASCII 數字）。 複雜的格式會導致內容分散注意力，並且隨著時間的推移可能難以維護。\n  開門見山原則: 不要包含多餘的信息。 假設代碼的讀者對編程原理和語言語法有基本的了解。\n  設計好的程式結構: 設計你的代碼來評論自己。 理解代碼最簡單的方法是閱讀它。 當您使用清晰、易於理解的概念設計代碼時，讀者將能夠快速概念化您的意圖。\n  文檔: 面向使用者 用字符串做文檔: 前面了解了註釋(Comment), 現在開始討論文檔(Documentation). 我們要學習使用「字符串 (Docstrings)」來做文檔. 若內置的字符串配置正確, 可顯著幫助用戶與自己的項目發展.  召喚字符串, 物件目錄, 文檔: help(), dir(), __doc__ help()召喚字符串: 利用Python的內置函數help()可召喚出「物件字符串」. 例如:  1\u0026gt;\u0026gt;\u0026gt; help(str) 2Help on class str in module builtins: 3 4class str(object) 5 | str(object=\u0026#39;\u0026#39;) -\u0026gt; str 6 | str(bytes_or_buffer[, encoding[, errors]]) -\u0026gt; str 7 | 8 | Create a new string object from the given object. If encoding or 9 | errors are specified, then the object must expose a data buffer 10 | that will be decoded using the given encoding and error handler. 11 | Otherwise, returns the result of object.__str__() (if defined) 12 | or repr(object). 13 | encoding defaults to sys.getdefaultencoding(). 14 | errors defaults to \u0026#39;strict\u0026#39;. 15 # Truncated for readability dir()召喚物件目錄: 在Python中, 什麼都是物件(Object). 利用函數dir()可召喚出物件的目錄(directory).  1\u0026gt;\u0026gt;\u0026gt; dir(str) 2[\u0026#39;__add__\u0026#39;, ..., \u0026#39;__doc__\u0026#39;, ..., \u0026#39;zfill\u0026#39;] # Truncated for readability 目錄的屬性:__doc__: 在召喚出來的物件目錄中, 有個特殊的屬性__doc__, 其內容如下:  1\u0026gt;\u0026gt;\u0026gt; print(str.__doc__) 2str(object=\u0026#39;\u0026#39;) -\u0026gt; str 3str(bytes_or_buffer[, encoding[, errors]]) -\u0026gt; str 4 5Create a new string object from the given object. If encoding or 6errors are specified, then the object must expose a data buffer 7that will be decoded using the given encoding and error handler. 8Otherwise, returns the result of object.__str__() (if defined) 9or repr(object). 10encoding defaults to sys.getdefaultencoding(). 11errors defaults to \u0026#39;strict\u0026#39;. 文檔客製物件:編輯__doc__, 三引號(triple-double quote)註解, 多行字符串 客製物件的文檔1: 編輯__doc__: 自定義函數, 可以透過編輯其__doc__屬性來提供文檔.  1def say_hello(name): 2 print(f\u0026#34;Hello {name}, is it me you\u0026#39;re looking for?\u0026#34;) 3 4say_hello.__doc__ = \u0026#34;A simple function that says hello... Richie style\u0026#34; 1\u0026gt;\u0026gt;\u0026gt; help(say_hello) 2Help on function say_hello in module __main__: 3 4say_hello(name) 5 A simple function that says hello... Richie style 客製物件的文檔2:三引號(triple-double quote)註解:另一種方法來寫函數的文檔, 是直接在def的下面加註解.  1def say_hello(name): 2 \u0026#34;\u0026#34;\u0026#34;A simple function that says hello... Richie style\u0026#34;\u0026#34;\u0026#34; 3 print(f\u0026#34;Hello {name}, is it me you\u0026#39;re looking for?\u0026#34;) 1\u0026gt;\u0026gt;\u0026gt; help(say_hello) 2Help on function say_hello in module __main__: 3 4say_hello(name) 5 A simple function that says hello... Richie style 多行字符串的格式: 當文檔較為精緻, 成為多行字符串, 則建議包含以下四個部分:   A one-line summary line 一行總結 A blank line proceeding the summary 一行空白 Any further elaboration for the docstring 更細緻的描述 Another blank line 另一行空白  三大文檔類型: 類字符串, 包與模組字符串, 腳本字符串 定義三大文檔類型:   Class Docstrings: 類(Class) and 類方法(class methods) Package and Module Docstrings: 包(Package), 模組(modules), and 函數(functions) Script Docstrings: 腳本(Script) and 函數(functions)  類字符串: __init__類方法上 類與類方法字符串: 定義類與類方法以後, 建議就加上文檔.  1class SimpleClass: 2 \u0026#34;\u0026#34;\u0026#34;Class docstrings go here.\u0026#34;\u0026#34;\u0026#34; 3 4 def say_hello(self, name: str): 5 \u0026#34;\u0026#34;\u0026#34;Class method docstrings go here.\u0026#34;\u0026#34;\u0026#34; 6 7 print(f\u0026#39;Hello {name}\u0026#39;) 其中有SimpleClass這個類的文檔, 也有say_hello這個類方法的文檔.\n類文檔應有資訊:類文檔應有以下四種資訊     A brief summary of its purpose and behavior 簡述類的「目的」與「行為」    Any public methods, along with a brief description 任何「公用方法」與其簡單描述    Any class properties (attributes) 任何「類屬性」    Anything related to the interface for subclassers, if the class is intended to be subclassed 是否是作為「介面(interface)」而建立的類     __init__類方法文檔: 在__init__類方法的字符串, 應該要聞黨類創造子的參數(class constructor parameters). 類創造子是一個物件導向中的基礎概念, 其限定了類創造物件時的細節.\n  類方法字符串應有資訊: 類方法字符串應有資訊包含以下六點:\n     A brief description of what the method is and what it’s used for 方法的簡單描述與其目的    Any arguments (both required and optional) that are passed including keyword arguments 任何會在「關鍵字參數 (Keyword arguments)」中「必要(required)」或「可選(optional)」的「實際參數(argument)」    Label any arguments that are considered optional or have a default value 標記可選參數或其預設值    Any side effects that occur when executing the method 方法的副作用    Any exceptions that are raised 會出現的例外    Any restrictions on when the method can be called 呼叫方法後會引發的限制    實例: Animal類: 這個類有簡單的「類屬性(class properties)」, 「實例屬性(instance properties)」, 「__init__類方法」, 「實例方法(instance method )」  1class Animal: 2 \u0026#34;\u0026#34;\u0026#34; 3A class used to represent an Animal 45... 67Attributes 8---------- 9says_str : str 10a formatted string to print out what the animal says 11name : str 12the name of the animal 13sound : str 14the sound that the animal makes 15num_legs : int 16the number of legs the animal has (default 4) 1718Methods 19------- 20says(sound=None) 21Prints the animals name and what sound it makes 22\u0026#34;\u0026#34;\u0026#34; 23 24 says_str = \u0026#34;A {name}says {sound}\u0026#34; 25 26 def __init__(self, name, sound, num_legs=4): 27 \u0026#34;\u0026#34;\u0026#34; 28Parameters 29---------- 30name : str 31The name of the animal 32sound : str 33The sound the animal makes 34num_legs : int, optional 35The number of legs the animal (default is 4) 36\u0026#34;\u0026#34;\u0026#34; 37 38 self.name = name 39 self.sound = sound 40 self.num_legs = num_legs 41 42 def says(self, sound=None): 43 \u0026#34;\u0026#34;\u0026#34;Prints what the animals name is and what sound it makes. 4445If the argument `sound` isn\u0026#39;t passed in, the default Animal 46sound is used. 4748Parameters 49---------- 50sound : str, optional 51The sound the animal makes (default is None) 5253Raises 54------ 55NotImplementedError 56If no sound is set for the animal or passed in as a 57parameter. 58\u0026#34;\u0026#34;\u0026#34; 59 60 if self.sound is None and sound is None: 61 raise NotImplementedError(\u0026#34;Silent Animals are not supported!\u0026#34;) 62 63 out_sound = self.sound if sound is None else sound 64 print(self.says_str.format(name=self.name, sound=out_sound)) 包與模組字符串: __init__.py檔案中  包字符串: 包字符串要在包頂層的__init__.py檔案中. 這個字符串要包含「模組(modules)」與「子包(subpackages)」的列表.\n  模組字符串: 模組字符串要列表出任何其包含的函數. 模組字符串要在整的文件的最頂端. 須包含以下兩點:\n     A brief description of the module and its purpose 簡述模組與其目的    A list of any classes, exception, functions, and any other objects exported by the module 列表類, 例外, 函數, 其他任何模組會輸出的物件.    模組函數文檔: 其規則與「類方法」類似, 要包含六點:     A brief description of what the method is and what it’s used for 方法的簡單描述與其目的    Any arguments (both required and optional) that are passed including keyword arguments 任何會在「關鍵字參數 (Keyword arguments)」中「必要(required)」或「可選(optional)」的「實際參數(argument)」    Label any arguments that are considered optional or have a default value 標記可選參數或其預設值    Any side effects that occur when executing the method 方法的副作用    Any exceptions that are raised 會出現的例外    Any restrictions on when the method can be called 呼叫方法後會引發的限制    腳本字符串: argparse活用  定義腳本: 腳本(Scripts)是主機(Console)可執行的單檔案. Scripts are considered to be single file executables run from the console.\n  腳本字符串: 要在檔案最上方, 要提供足夠資訊讓用戶能使用腳本.\n  使用argparse:使用argparser.parser.add_argument函數, 利用argpase的__doc__文檔來對參數的description. (詳細請見Command-Line Parsing Libraries )\n  列出腳本前置包: 文檔要包含讓用戶知道要先安裝哪些包. 以下是列印表格欄的腳本. 有說明要安裝pandas還有要輸入模組get_spreadsheet_cols，並包含函數main.\n  1\u0026#34;\u0026#34;\u0026#34;Spreadsheet Column Printer 23This script allows the user to print to the console all columns in the 4spreadsheet. It is assumed that the first row of the spreadsheet is the 5location of the columns. 67This tool accepts comma separated value files (.csv) as well as excel 8(.xls, .xlsx) files. 910This script requires that `pandas` be installed within the Python 11environment you are running this script in. 1213This file can also be imported as a module and contains the following 14functions: 1516* get_spreadsheet_cols - returns the column headers of the file 17* main - the main function of the script 18\u0026#34;\u0026#34;\u0026#34; 19 20import argparse 21 22import pandas as pd 23 24 25def get_spreadsheet_cols(file_loc, print_cols=False): 26 \u0026#34;\u0026#34;\u0026#34;Gets and prints the spreadsheet\u0026#39;s header columns 2728Parameters 29---------- 30file_loc : str 31The file location of the spreadsheet 32print_cols : bool, optional 33A flag used to print the columns to the console (default is 34False) 3536Returns 37------- 38list 39a list of strings used that are the header columns 40\u0026#34;\u0026#34;\u0026#34; 41 42 file_data = pd.read_excel(file_loc) 43 col_headers = list(file_data.columns.values) 44 45 if print_cols: 46 print(\u0026#34;\\n\u0026#34;.join(col_headers)) 47 48 return col_headers 49 50 51def main(): 52 parser = argparse.ArgumentParser(description=__doc__) 53 parser.add_argument( 54 \u0026#39;input_file\u0026#39;, 55 type=str, 56 help=\u0026#34;The spreadsheet file to pring the columns of\u0026#34; 57 ) 58 args = parser.parse_args() 59 get_spreadsheet_cols(args.input_file, print_cols=True) 60 61 62if __name__ == \u0026#34;__main__\u0026#34;: 63 main() 後記 發展文檔: 原文後面有個文檔進展的五個階段:     No Documentation    Some Documentation    Complete Documentation    Good Documentation    Great Documentation 感覺這也很適合講正在執筆寫的論文. 從No, some, complete, good, great. 先搞出完整的, 再到好, 再到很棒. 用這樣子來分類項目的成熟度, 十分不錯.     到此紀錄了由Documenting Python Code: A Complete Guide 中, 節選的相關段落. 首先理解註解是為開發者而服務, 而文檔是為用戶而服務. 接著看了類與類方法, 包與模組, 腳本三種Python文件的文檔字符串寫法.\n  非常好！今天是每日一篇文章的第九天. 今天這個技術文章很長, 一段一段讀, 改寫, 也刺激自己對這些技術概念的融會貫通, 感覺心中更踏實. 持續輸出! 共勉之！\n  2022.01.07. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-07","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur009%E8%A9%95%E8%AB%96%E8%88%87%E6%96%87%E6%AA%94/","series":["每日文章","編程思維"],"tags":["Python"],"title":"MUR009 思辨註釋與文檔"},{"categories":["寫作","得到課程"],"content":"讀蔡鈺商業參考發刊詞-理解商業, 與世界保持同步  紫式晦澀每日一篇文章第8天\n  前言   今天是2022年第6天, 第1週的第1個週四. 今天早上聽到蔡鈺老師的商業參考發刊詞-理解商業, 與世界保持同步, 對思維影響巨大！決定趁著興致把所思所想組織成文章, 以更深入看見這段邂逅的價值.\n  今天的素材主要是從蔡鈺老師的商業參考得到課程:发刊词: 理解商业, 与世界保持同步 的閱讀中, 紀錄組織下來的個人所思所想.\n  智慧: 深思熟慮, 抑或見多識廣? 見多識廣, 或者深思熟慮?: 這句話真是打中我心. 兩者的偏好, 我也慢慢從後者轉移到前者. 但這不代表不再深思熟慮, 而是用已經習慣的深度, 敏捷地去見更多的世面, 認識更廣的現象.   在开始为期一年的唠嗢之前, 我想先问你一个问题: 同样是形容有智慧, 有两个词, 见多识广和深思熟虑, 如果要你来选, 你更向往哪种状态? 当然这俩不是冲突的, 我问的是偏好。\n  爱德华. 德博诺, 《六顶思考帽》的作者, 说过一段话。他说: 思考的目的不是要做到正确,而是要做到有效。正确的意思是你从头到尾都要正确, 而有效呢, 只要求最终正确就可以。\n 思考的目的不是要做到正确, 而是要做到有效。 一爱德华 - 德博诺\n   深思熟慮求穩定: 如果想要时刻保持正确, 就很容易求稳, 会放弃寻找新思路\n  求新求變, 而不是在一個地方持續replay, 我的天性似乎更喜歡變來變去. 畢竟, 李李玲蘭, Gura, Laplus都是搞怪的代表, 而也很符合我認為的「有趣」.\n  見多識廣求有效: 但如果想要做到的是有效, 那么我们途中就应该让想法足够多, 即使中间有些想法是错的也没关系, 这总比满足于已有的正确答案, 放弃新想法要强。有一句话叫做「朝一个方向看得再远, 你也末必能看到新方向」, 也是这个意思。所以, 见多识广 当然不是反对正确, 只不过它追求的不是正确, 而是丰富。\n  或許, 年紀大的時候寫書, 會講求的是「系統化的豐富」, 來提供下一代人進入這些人類智慧結晶的知識地圖. 有意思.\n  解釋: 不一定是唯一正確的答案, 但提供豐富觀察維度, 與視野的開闊程度  解釋不一定是唯一正確的答案, 但提供豐富觀察維度, 與視野的開闊程度: 这些解释末必是唯一正确答案, 甚至可能在时间长河里会被证明不正确, 但是他们给我们提供的不 是正确, 而是丰富的观察维度和开阔的视野。\n  這個部分, 也是我們做科學研究寫文章的基本需求. 對同樣的問題, 提供另一個觀察維度, 讓視野開闊, 連結不同的知識背景研究社群.\n  發人深省! 我們追求深度與廣度. 要有深潛的能力, 也要有環遊世界的能力. 反思直到現在30歲的人生, 16-25歲這幾年在台北與台灣大學累積了「深潛」的能力儲備; 25-30歲這幾年在美國與普渡大學養成了「環遊世界」的技術儲備. 做研究寫論文來把現象理解做深, 聽課程寫文章把事件見聞做廣.\n  變化: 商業改變生活, 找機會, 借智慧, 成年人的世界真奇妙  商業改變生活: 新的服務, 基於新的科技, 正在改造我們身處的社會, 城市; 影響著我們的工作, 社交, 娛樂. 當代人類社會中, 生活需求由多少的產業來滿足呢? 一日之所需, 百工斯為備; 上升到治理城市國家的層級, 便開始能關心天下事.\n  找機會, 借智慧: 了解不同的商業模型, 來得到自己的「aha! moment」, 於現在的自己是如醍醐灌頂般的享受, 於未來的自己是洞察新時代優雅生活的知識儲備.\n  成年人的世界真奇妙: 人類的群體智慧, 互動產生許多「元宇宙」型態的精神活動. 很多神奇功能的背後, 都有很有趣的原理與巧思, 讓我們可以發現新的做法, 原來還能這麼做! 窺探內裡的秩序與邏輯, 發現屬於自己的真相.\n  後記  到此紀錄了由蔡鈺老師的商業參考得到課程:发刊词: 理解商业, 与世界保持同步 所引發的思考與自我認識. 從智慧是深思熟慮或是見多識廣開始, 體會了每一種解釋不一定是正確的答案, 但會提供豐富觀察維度, 與視野的開闊程度. 商業改變生活, 從商業變化中洞察商業機會, 學習商業智慧, 享受成年人版本的大自然真奇妙.\n  十分過癮!今天是每日一篇文章的第八天. 有了每天寫文章的習慣後, 看到的資訊都會先前處理, 在寫作的過程中仔細處理, 後來交付成一篇基本的文章. 寫成的文章, 就可以讓未來的自己檢視, 也能有效地將資訊個人化, 培養更深刻的見聞. 持續向上, 共勉之！\n  2022.01.06. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-06","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur008%E8%AE%80%E8%94%A1%E9%88%BA%E5%95%86%E6%A5%AD%E5%8F%83%E8%80%83%E7%99%BC%E5%88%8A%E8%A9%9E-%E7%90%86%E8%A7%A3%E5%95%86%E6%A5%AD-%E8%88%87%E4%B8%96%E7%95%8C%E4%BF%9D%E6%8C%81%E5%90%8C%E6%AD%A5/","series":["每日文章"],"tags":["商業參考"],"title":"MUR008 讀蔡鈺商業參考發刊詞-理解商業, 與世界保持同步"},{"categories":["寫作"],"content":"清單體的四個角度  紫式晦澀每日一篇文章第7天\n  前言   今天是2022年第5天, 第1週的第1個週三. 今天來整理之前對「清單體」話題的一些筆記.\n  今天的素材主要是從得到APP與網路上收集而來. 從五個訊息源頭做了15個點的筆記. 以下整理為關於清單體的五個角度.\n  結構寫作:快速起承轉合, 體裁骨架, 題材血肉, 提高閱讀便利性 結構化寫作文體:   要想學會快速寫文章，需要掌握一些「易於複製、條理清晰的結構化寫作文體」。 比如日記體、清單體、語錄體、點評體、問答體等。 這些文體最大的優點是幫助寫作者快速搞定文章結構，快速起承轉合，把思想的「血肉」迅速填充到文章的「骨架」當中。  體裁骨架, 題材血肉:   寫文章，首先需要搞定「體裁（文體）」和「題材」的區別。 體裁是文章的種類和樣式，側重形式特徵；題材是文章涉及的領域、話題或素材，側重內容特徵。 文章的體裁，也稱文體，用於解決「用什麼文章形式」「怎麼寫」的問題；題材用於解決「關注什麼話題」「寫什麼」的問題。 掌握結構化的快捷文體，能夠幫我們快速整理思路、鋪陳素材、流暢表達，寫作效率可以明顯提高，也更容易養成持續寫作的習慣。  前結構化寫作:   常見的結構化寫作文體：日記體、清單體、語錄體、資訊體、點評體、圖片體、問答體、倒金字塔體等等。 前結構化寫作最大的功用，是可以幫助寫作者快速解決「文章如何鋪陳」「先寫什麼後寫什麼」「怎麼快速下筆」的問題，鼓勵作者先把文章寫出來，而不必過多糾結於文章怎麼起承轉合。 此外，結構化寫作還有這些優勢：降低理解成本、提高閱讀便利性、易於分享和傳播。  知識分享: 個人喜好排序, 品味清單體, 高級整理者 清單依喜好排序萬事萬物:   一切有邏輯關係的事物都可以用列表的方式來呈現。 我們列清單，是因為萬事萬物開始圍繞著我們的需求喜好來重新排序，而不再受傳統的層級類型的限制。 清單可以做什麼？既可以是個人邏輯梳理的工具，也可以作為結構化社會知識的載體。  清單體（listicle）高度可分享:   內容的清單化：無清單，不傳播 內容的清單化早在產品的清單化之前就已經開始了。 《紐約時報》內部去年5月發佈一份《數字時代革新報告》，其中將BuzzFeed的成名原因總結為3點：積極進取的社交網絡推廣、高度可分享的內容、試驗性的新聞模式。 清單體（listicle）與小測試、短視頻一起，正好構成其高度可分享的、試驗性等3大特色  先做一個高級的整理者:   清單體沒有作者，而只有編輯。或者說，作者的角色降級為「整理者」，很多微信公眾賬號的運營者扮演的正是這樣的角色。 早在2013年，鈦媒體作者魏武揮老師就指出：一個平台活躍度的核心，生產者並不是第一位重要的，整理者才是第一位重要的。 先是高級的整理者，才有辦法同中有異，創新。  知識輸出: 快速輸出精華, 一段三行強操作性, 以編輯鍛鍊邏輯 清單體快速輸出:   在這個注意力短缺，新知識層出不窮的互聯網時代，清單體寫作具有條理清晰、簡單易看、乾貨十足的特點，可以極大地減少讀者的閱讀壓力。 運用清單體寫作，可以快速吸收一本書的精華內容並且快速輸出，即使是面對一本不太對胃口的書，也能運用清單體寫作榨出它極少的好處。 掌握清單體的寫法，對我們好處極大，下面闡述一下清單體寫作的一些技巧。  一段三行強操作性:   文字簡潔，分行羅列清單體講究把複雜的道理簡單說，能用兩個字表達的意思，就不要用5個字。 所以清單體寫作，每句話不要太長，寫完後要多讀幾遍，反復修改，調整或刪除累贅的話。還有要善用空行，把文字分段。 讀者對冗長的文字很容易失卻耐心，把每段話限制在三行以內。標準的清單體格式是，用500~800字，輸出10條清單。  編輯鍛鍊邏輯:   注重邏輯，講究實用。清單體起到提醒知識點作用，一定要清晰明瞭，操作性強。 每條清單體不是簡單的羅列，各條清單體間都有其內在的聯繫。或是層層遞進、或是不同視角、或是按照現象原因方法劃分等。 選好要闡述的清單後，要思考，把他們按照什麼樣的方式去闡述，更容易讓人記住?可以運用思維導圖，對清單進行梳理，從而更有條理地闡述。 還有寫清單體的閱讀對象除了自己，還有別人。要考慮別人能不能看懂，對他們有沒有幫助？自己日後復習又能不能看懂？  每日文章: 以清單體草稿寫作, 以文章解答讀者問題 作為寫作草稿的清單體:   我們可以在自己擅長的領域多花時間做積累，積累能夠寫清單的話題，有時間就蒐集整理素材，存入素材庫。 等到需要寫的時候，就把這些清單拿出來，也就不愁斷更了。 清單體比較適合分享知識類內容的文章，先確定大概的主題，然後廣泛搜索類似的網絡素材，並做好排序，就可以完成一篇清單體文章。 清單體相對好寫，關鍵是清單蒐集的質量要高。清單體一般最少湊足10條，但如果內容太多，就要捨得刪減，留下最精彩的。 最好的方式是找到專屬寫作主題，透過寫作主題反覆練習，戰勝自己的懶惰。  為讀者提供解答:   讀者愛看清單體，重點在於能一次看到多項介紹、比較，省去他花時間爬文，而且數量多，容易激起人性中，害怕錯過的特性，通常會有不錯的點閱。 寫出專屬你的清單體 清單體因為是以資料整理為主，導致相同主題能寫的資料有限，而在你之前，可能已經有很多人寫過，要和其他筆者做出差異，建議做足競爭者分析的功夫。 瞭解對方哪塊的資訊不足，而你能補充哪些資訊，替讀者解決問題。 重點就是瞭解讀者需要哪些資訊，幫助他們解決問題。  後記  到此我們提煉了與清單體相關的四個角度: 結構寫作, 知識分享, 知識輸出, 每日文章. 道與術皆有, 寫文章就能刺激自己具體化累積的知識, 留下數位足跡. 累積夠多的輸出後, 會產生湧現現象, 發展出更新的角度與觀點, 醍醐灌頂, 找到更好的知識模型.\n  自我反思, 今天已經是每日一篇文章的第七天. 的確, 現在看到各種資訊都會有意識地先整理成清單體, 累積夠多的知識點以後, 再以文章的形式輸出. 在輸出的過程, 提煉自己的邏輯組織, 總結小標題的能力. 這類的能力稱為「編輯能力」, 而世界上那麼多存在的知識, 要靠自己精選組織各種資訊, 成為文本, 才可以讓未來的自己回顧, 往更深層的理解前進. 持續向上, 共勉之！\n  2022.01.05. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-05","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur007%E6%B8%85%E5%96%AE%E9%AB%94/","series":["每日文章"],"tags":[],"title":"MUR007 清單體的四個角度"},{"categories":["可信任的AI"],"content":"差分隱私尋根  紫式晦澀每日一篇文章第6天\n  前言   今天是2022年第4天, 第一週的第一個週二. 今天來思考差分隱私最基礎的定義.\n  今天的素材是Cynthia Dwork與Aaron Roth的The Algorithmic Foundations of Differential Privacy 的第二章所節選的材料.\n  差分隱私故事: 背景, 期望, 隱私洩露  差分隱私背景: 保護隱私的數據分析問題由來已久，橫跨多個學科。隨著關於個人的電子數據變得越來越詳細，以及技術使這些數據的收集和整理變得越來越強大，對一個強大的、有意義的、數學上嚴格的隱私定義的需求也在增加，同時也需要一個滿足這個定義的計算豐富的算法(a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms)類別。差分隱私就是滿足這樣定義的一系列演算法。\n  對隱私保護的期望: ”差分隱私 \u0026ldquo;描述了數據持有者對數據主體做出的承諾。\u0026ldquo;如果允許你的數據被用於任何研究或分析，無論其他研究、數據集或信息來源如何，你都不會受到不利或其他的影響。\u0026rdquo; 在最好的情況下，不同的私有數據庫機制可以使機密數據廣泛用於準確的數據分析，而不需要依賴於數據清洗、數據使用協議、數據保護計劃或限制訪問。\n  回答太清楚, 隱私就洩漏: 儘管如此，數據的效用最終還是會被消耗掉：信息恢復的基本法則(Fundamental Law of Information Recovery )指出，對太多問題的過度準確的回答會以驚人的方式破壞隱私。差別隱私的算法研究的目標是盡可能推遲這種不可避免性。\n  差分隱私是一個定義，不是一種算法: 對於一個給定的計算任務$T$和一個給定的$\\varepsilon$值，將有許多差分隱私算法以$\\varepsilon$-差分隱私的方式實現$T$。有些算法會比其他算法有更好的準確性。當$\\varepsilon$較小時，為$T$找到一個高度精確的$\\varepsilon$-差分隱私算法可能是困難的，就像為一個特定的計算任務找到一個數值穩定的算法一樣困難。\n  形式化基礎元素-輸出值機率向量, 隨機演算法, 其差分隱私性質. 機率單純型: 裡面的每個$x$都代表著一個「機率向量」, 對應每個可能輸出值的機率.   Definition 2.1 (Probability Simplex). Given a discrete set $B$, the probability simplex over $B$, denoted $\\Delta(B)$ is defined to be: $$\\Delta(B)={x \\in \\mathbb{R}^{|B|}: x_{i} \\geq 0 \\text { for all } i \\text { and } \\sum_{i=1}^{|B|} x_{i}=1}$$\n 隨機演算法: 看作一個隨機映射, 對一個固定的輸入a, 有一定的概率輸出b. 這個機率向量被前面的機率單純型給描述.一般來說，一個具有域$A$和（離散）範圍$B$的隨機算法將與一個從$A$到$B$上的概率單線的映射有關，表示為$\\Delta(B)$ 。   Definition 2.2 (Randomized Algorithm). A randomized algorithm $\\mathcal{M}$ with domain $A$ and discrete range $B$ is associated with a mapping $M: A \\rightarrow \\Delta(B) .$ On input $a \\in A$, the algorithm $\\mathcal{M}$ outputs $\\mathcal{M}(a)=b$ with probability $(M(a))_{b}$ for each $b \\in B$. The probability space is over the coin flips of the algorithm $\\mathcal{M}$.\n 數據庫的長度: 我們將認為「數據庫$x$」是來自宇宙$\\mathcal{X}$的記錄集合。用直方圖來表示數據庫往往很方便：$x\\in\\mathbb{N}^{|\\mathcal{X}|}$，其中每個條目$x_{i}$代表數據庫$x$中$i\\in\\mathcal{X}$類型的元素數量。在這種表述中，兩個數據庫$x$和$y$之間距離的自然度量將是它們的$\\ell_{1}$距離。   $|x|{1}$是衡量數據庫$x$的大小（即它所包含的記錄數. $|x-y|{1}$是衡量$x$和$y$之間有多少記錄不同。\n 隨機算法的差分隱私性質: 我們現在準備正式定義差異化隱私。直觀上, 保證了隨機化算法, 在類似的輸入數據庫上, 輸出是相似的。   Definition 2.4 (Differential Privacy). A randomized algorithm $\\mathcal{M}$ with domain $\\mathbb{N}^{|\\mathcal{X}|}$ is $(\\varepsilon, \\delta)$-differentially private if for all $\\mathcal{S} \\subseteq \\operatorname{Range}(\\mathcal{M})$ and for all $x, y \\in \\mathbb{N}^{|\\mathcal{X}|}$ such that $|x-y|_{1} \\leq 1$ : $$ \\operatorname{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}] \\leq \\exp (\\varepsilon) \\operatorname{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}]+\\delta $$where the probability space is over the coin flips of the mechanism $\\mathcal{M}$. If $\\delta=0$, we say that $\\mathcal{M}$ is $\\varepsilon$-differentially private.\n  保護隱私失敗的機率$\\delta$:通常情況下，我們對$\\delta$的值感興趣，它小於數據庫規模的任何多項式的倒數。特別是，$\\delta$的值在$1 /|x|_{1}$的數量級上是非常危險的：它們允許通過公佈少數數據庫參與者的完整記錄來 \u0026ldquo;保護隱私\u0026rdquo;.\n  有沒有失敗機率, 解釋起來很不同:然而，即使$\\delta$可以忽略不計，在$(\\varepsilon, 0)$和$(\\varepsilon, \\delta)$差異性隱私之間也有理論上的區別。其中最主要的是相當於量化順序的轉換。前者的輸出會差不多, 但後者的輸出可以差很多\n   $(\\varepsilon, 0)$差分隱私確保，對於機制$\\mathcal{M}(x)$的每一次運行，觀察到的輸出（幾乎）同樣可能在每個相鄰的數據庫中同時被觀察到。 相反，$(\\varepsilon, \\delta)$差分隱私說，對於每一對相鄰的數據庫$x, y$，事後觀察到的值$\\mathcal{M}(x)$在數據庫為$x$時比在數據庫為$y$時產生的可能性要大得多或小得多，這是極其不可能的。 然而，給定一個輸出$x_i\\sim\\mathcal{M}(x)$，有可能找到一個數據庫$y$，使$x_i$在$y$上產生的可能性遠遠大於數據庫為$x$時的可能性。也就是說，$x_i$ 在$\\mathcal{M}(y)$分布中的質量可能大大高於它在$\\mathcal{M}(x)$分布中的質量。   這樣來說，$(\\varepsilon, \\delta)$差分隱私雖然保護隱私, 但也讓之後相關的機器學習任務很難做.\n 隱私損失: 控制隱私預算, 免疫後處理轉換 定義隱私損失: 關鍵量是 $$\\mathcal{L}_{\\mathcal{M}(x) | \\mathcal{M}(y)}^{(\\xi)}=\\ln \\left(\\frac{\\operatorname{Pr}[\\mathcal{M}(x)=\\xi]}{\\operatorname{Pr}[\\mathcal{M}(y)=\\xi]}\\right).$$ 我們把它稱為觀察$\\xi$所產生的隱私損失。   這種損失可能是正的（當一個事件在$x$下比在$y$下更可能發生），也可能是負的（當一個事件在$y$下比在$x$下更可能發生）。 正如我們在Lemma 3.17中看到的，$(\\varepsilon, \\delta)$差分隱私確保對於所有相鄰的$x, y$，隱私損失的絕對值將被$\\varepsilon$所約束，概率至少為$1-\\delta$。   這個定義是把$e^{\\epsilon}$的部分做了個轉換; 可否把這個想成policy的decision變化的程度? 但感覺與隱私沒什麼關係. 這個寫法的好處可以跟似然比接起來, 也難怪可以用假設檢定的框架來做.\n  差分隱私對後處理是免疫的: 數據分析員在沒有關於私人數據庫的額外知識的情況下，不能計算私人算法$\\mathcal{M}$的輸出的函數並使其減少差異性隱私。\n   也就是說，如果一個算法保護了個人的隱私，那麼數據分析員不能增加隱私損失, 無論是在正式定義下還是在任何直觀意義上。 從形式上看，與數據無關的「後處理映射$f$」與$(\\varepsilon, \\delta)-$差分隱私算法$\\mathcal{M}$的組合也是$(\\varepsilon, \\delta)$差分隱私。   也是這個特點, 讓隱私處理後的資料可以被公開做研究.\n 後記  到此我們看過了Cynthia Dwork與Aaron Roth的The Algorithmic Foundations of Differential Privacy 的第二章的節選內容. 在「說太清楚就隱私洩露」的事實下, 我們需要給隨機演算法增加「差分隱私」的性質, 以免惡意人士透過觀察隨機演算法的輸出值, 來還原特定的數據. 形式化框架下, 經過差分隱私處理後的數據, 在後處理下不會造成額外的隱私洩露. 對形式化的學術研究而言, 「隱私損失」會是重點分析目標.\n  至此, 對差分隱私又多瞭解了一點, 也可以理解其形式化細節會與似然比, 假設檢定等等技術有關係. 很多統計的結果都可以套上這層外衣. 之後可以多想想這些基礎的關聯.\n  2022.01.04. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-04","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur006%E5%B7%AE%E5%88%86%E9%9A%B1%E7%A7%81%E5%B0%8B%E6%A0%B9/","series":["每日文章"],"tags":["差分隱私"],"title":"MUR006 差分隱私尋根"},{"categories":["合成數據","可信任的AI"],"content":"合成數據如何幫助基於機器學習的偵測詐欺?  紫式晦澀每日一篇文章第5天\n  前言   今天是2022年第3天, 第一週的第一個週一!今天來思考「合成數據(Synthetic Data)」是如何在金融科技領域中，幫助「詐欺偵測(Fraud Detection)」的任務.\n  今天的素材主要是從文章Follow the Trail: Machine Learning for Fraud Detection in Fintech Applications 節選提到合成數據的相關段落 .\n  合成數據用於詐欺偵測-檢測率, 訓練數據集來源  合成數據對詐欺偵測的效果: 本文章實驗了ML方法對欺詐檢測的貢獻, 分別利用真實數據集與「合成數據集」來訓練. 此文章討論了各種方法在「檢測率」的有效性. 此外, 分析了所選特徵對其性能的影響.\n  詐欺偵測的訓練數據集-Kaggle, ML Repository, Simulator:\n   A. 金融科技的詐欺檢測, 缺乏公開可用的測試數據. B. 來源一: Kaggle數據集 (括信用卡數據集[9]、銀行交易數據[10]和區塊鏈歷史數據[11]). C. 來源二: 已知的、稍舊的合成數據集可以在UC Irvine ML Repository中找到（例如，UC Irvine[12]） D. 來源三:模擬器(Simulator) BankSim[13]和PaySim[14]等模擬器被應用於解決這個問題。前者代表了一個基於代理的銀行支付模擬器，而後者則通過生成客戶和執行交易來模擬移動交易。  合成數據集範例 PaySim: 基於私人數據, 注入惡意行為, 產生合成數據 PaySim的模擬器:   A. 在4.2.2節中，將介紹一個使用名為PaySim的模擬器生成的合成數據集（PaySim數據集在下文中）。 B. PaySim使用私人數據集的匯總數據，生成一個類似於交易正常運行的合成數據集，並注入惡意行為，以便日後評估欺詐檢測方法的性能。 C. 這是通過在一個非洲國家的真實交易樣本基礎上模擬移動支付交易來實現的。原始數據是由一家跨國移動金融服務提供商提供的[104]。 D. 在這個特定的數據集中，代理商的欺詐行為旨在通過控制或客戶的賬戶來獲利，並試圖通過轉移到另一個賬戶來清空資金，然後從系統中套現。 E. 數據集詳情見表5。 Table 5. Synthetic Financial Datasets for Fraud Detection dataset overview.    Dataset name Synthetic Financial Datasets for Fraud Detection     Domain Financial Transactions   Url https://www.kaggle.com/ntnu-testimon/paysim1    Year (accessed on 30 November 2020)   Type 2015   Subset Synthetic data   Annotated PS_20174392719_1491204439457_log.csv   Unbalanced Yes   No. of entries Yes   Contamination rate 6,362,620   Time duration 0.129%   No. of features 1 month   List of features 11    step, type, amount, name0rig, oldbalance0rg,      定義PaySim: PaySim.   A. 一個移動貨幣支付模擬器 移動貨幣支付模擬案例研究是基於一個真實的公司，該公司開發了一個移動貨幣實施方案，為手機用戶提供了使用手機作為一種電子錢包在他們之間轉移資金的能力。 B.「任務」是開發一種能夠檢測出表明欺詐的可疑活動的方法。 C. 不幸的是，在我們研究的最初階段，這項服務只在演示模式下運行。這使我們無法收集任何可用於分析可能的檢測方法的數據。 D. PaySim的開發包括兩個階段。  D.a 在第一階段，我們模擬並實現了一個MABS，它使用了真實的移動支付服務的模式，並根據對真實系統開始運行時可能出現的情況的預測，生成了合成數據。 D.b在第二階段，我們獲得了該系統的財務交易日誌，並開發了一個新版本的模擬器，該模擬器使用匯總的交易數據來生成更類似於原始來源的財務信息。   E. Keywords [en]: multi-agent based simulation, fraud detection, retail fraud, synthetic data http://bth.diva-portal.org/smash/get/diva2:1085629/FULLTEXT03.pdf .  BankSim: 用於詐欺檢測研究的合成數據 BankSim:   A. 小節4.2.3 介紹的數據集是使用BankSim創建的，這是一個基於代理的銀行支付模擬器，基於西班牙一家銀行提供的匯總交易數據樣本。 B. 目標: 生成可用於欺詐檢測研究的合成數據。 C. 這個數據集結合了正常的支付和已知的欺詐特徵，不包含任何個人信息或任何其他交易的披露。 D. 數據集的詳情見表6。    Dataset name Synthetic data from a financial payment system     Domain Financial Transactions   Url https://www.kaggle.com/ntnu-testimon/banksim1    Year (accessed on 30 November 2020)   Type 2014   Subset Synthetic data   Annotated bs140513_032310.csv   Unbalanced Yes   No. of entries Yes   Contamination rate 594,643   Time duration 1.21%   No. of features 6 months   List of features 10   Subset step, customer, age, gender, zipcode0ri,   Annotated merchant, zipMerchant, category, amount, fraud   Unbalanced bsNET140513_032310.csv   No. of entries Yes   Contamination rate Yes   Time duration 594,643   No. of features 1.21%   List of features 6 months      Bank Transaction Data: 檢測欺詐交易和洗錢的合成數據 Bank Transaction Data:   A.Bank Transaction Data是一種分析工具，旨在檢測欺詐交易和洗錢。 B. 開發人員希望建立一個工具，可以使用IFSC代碼提取銀行名稱；通過系統獲取兩個不同賬戶在同一日期的借方和貸方的相同數量的交易以及匹配的敘述；並在敘述的基礎上對類似交易進行分類。 C. 數據集的細節見表7。    Dataset name Bank Transaction Data     Domain Financial Transactions   Url https://www.kaggle.com/apoorvwatsky/bank-    Year transaction-data (accessed on 30 November 2020)   Type 2017   Subset Synthetic data   Annotated bank.xlsx   Unbalanced No   No. of entries n//a   Contamination rate 116,201   Time duration n//a   No. of features 7 months    8   List of features Account No., Date, Transaction Details, Cheque    No. , Value Date, Withdrawal Amount, Deposit      結果與洞察: 合成數據集中, 特徵的可變性不夠高  Variability of features in a synthetically created dataset might not be on a high enough level.\n 合成數據集中, 特徵的可變性不夠高:   圖15中給出了被測試的集合方法的ROC曲線比較。 基於tpr和tnr的比較分析表明，對於一個給定的數據集，AdaBoost在測試方法中表現最好。 同時，可以注意到這三種測試方法的靈敏度和特異性都很高，幾乎為1，這表明合成的數據集中特徵的可變性可能還不夠高(variability of features in a synthetically created dataset might not be on a high enough level)。 還應該注意的是，在這個特定的數據集上，集合方法的表現優於離群點檢測方法。   結論與洞察: 利用集群方法, 執行基於合成數據的訓練, 會有更好的效果.  ensemble approaches significantly outperformed outlier detection methods on the two tested synthetic datasets\n 集成方法在合成數據集上表現較好   所進行的實驗結果證實了ML的好處。 首先，現有的ML算法成功地在複雜的數據集中檢測到了異常情況。 此外，實驗結果證實，ML方法可以通過支持增強欺詐檢測能力的方式，成功地為金融技術系統的安全做出貢獻。 此外，研究還發現，特徵工程和選擇會嚴重影響某些算法的性能，仔細選擇特徵可以提高整體性能並限制某些特徵的負面影響。 還應注意的是，集合方法對可變的特徵選擇情況保持了更穩健的性能，總體表現非常好，在大多數情況下比離群值檢測方法更好 集合方法在兩個測試的合成數據集（PaySim和BankSim）上的表現明顯優於離群值檢測方法 (ensemble approaches significantly outperformed outlier detection methods on the two tested synthetic datasets) 而在包含真實數據的測試數據集（CreditCard）上，這兩種方法的結果是相當的。  後記  到此我們看過了Follow the Trail: Machine Learning for Fraud Detection in Fintech Applications 文章中關於「合成數據」的段落與洞察。本文章指出(1) 公開用於訓練機器學習模型的合成數據集(PaySim 與 BankSim), 特徵的可變性可能不夠高, (2) 利用集群方法(Ensemble Method), 執行基於合成數據的訓練, 會有更好的效果.\n  這邊所謂的「合成數據」的邏輯, 似乎是加入惡意行為, 然後看使用的算法是否能夠成功抓到惡意行為. 惡意行為的「產生機制」可否用deep learning來做呢? 這樣的generative model, 生成帶有隱藏惡意行為的驗證資料(Validation dataset), 以此來審計(audit)系統中使用的機器學習算法. 這類似學校對TA做種族平等的培訓, 以避免TA在課堂上做出違反符合美國大學系統價值下認為的平等.\n  十分有趣, 之後需要多學習一些詐欺偵測的資料科學工程技術, 來進一步思考此文章給出對合成數據集的洞察. 持續精進, 共勉之！\n  2022.01.03. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-03","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur005%E5%90%88%E6%88%90%E6%95%B8%E6%93%9A%E5%A6%82%E4%BD%95%E9%97%9C%E8%81%AF%E5%81%B5%E6%B8%AC%E8%A9%90%E6%AC%BA/","series":["每日文章"],"tags":["金融科技","詐欺偵測"],"title":"MUR005 合成數據如何幫助基於機器學習的偵測詐欺?"},{"categories":["寫作"],"content":"知識管理的五個角度  紫式晦澀每日一篇文章第4天\n  前言   今天是2022年第2天! 今天來整理之前對「知識管理」話題的一些筆記.\n  今天的素材主要是從得到APP上收集而來. 從五個訊息源頭做了15個點的筆記. 以下整理為關於知識管理的五個角度.\n  組織知識管理: 重要性, 難題與巧勁  組織知識管理的重要性: 管理學家彼得·德魯克反復強調，知識型組織中，知識是最重要的資源。「知識管理」關係著組織能不能持續、高質量地發展。\n  組織知識管理的難題:組織的智慧分散在不同部門、員工身上。想找文件只能做伸手要，想要學習過去的經驗只能當面問，沒有一個統一的入口能讓人高效地找到有用的知識.\n  組織知識管理的巧勁: 人和知識能不能有效匹配，要看知識是不是正好對應你要解決的問題，是不是足夠及時；還要看量，「少瞭解決不了問題，多了增加識別難度。」\n  個人知識管理：數據，資訊，底層規律  數據管理: 第一個是數據管理的維度。在這個維度上，我們所談論的知識管理更多是具體的「數據層面技巧」。比如，下載的文件怎麼保存？學到的知識點怎麼歸類？如何快速搜索文件？如何給文件貼標籤？怎樣整理文件夾？在哪裡找到合適的書單？等等。\n  這個部分年輕時我也犯過很多錯, 想要收集很大量的數據, 包括書本, 課程, 雜誌等等. 但現在一切都變得很方便以後, 反而是應該讓數據就留在他的出處, 而我們僅需要根據當下的需求去取回相對應的部分, 組織成能解決問題的知識即可.\n  資訊管理: 第二個是信息管理的維度。在這個維度上，我們關注的是怎樣更好地理解、消化和應用獲得的各個知識點。有很多非常有用的方法可以組織起來強化這一過程，比如：如何做讀書筆記？如何用思維導圖增強理解？學習中精讀和泛讀的區別、行動學習法、刻意練習等等。而能夠有效利用這些方法，也是一個學習者進階的標誌——能夠有效地把學到的知識用於解決問題。\n  這個部分有了Obsidian以後, 逐漸從note-taking轉化為note-making. 的確, 要將「數據, 文本」轉化成「資訊, 知識」需要倚靠各種框架來蒸餾出價值. 我們在學習的過程需要借助「語言的重複性」來增進理解, 但記憶的過程需要「知識的結構化」材能有效建立資訊之間的連結. 這個部分小小Sha的知識卡片書籍幫助很大, 持續master裡面的技藝.\n  底層邏輯管理: 第三個是底層規律的維度。在這個維度上，我們關心的不僅僅是具體的方法和技巧，更關心自己的認知深度：我們必須在大量具體知識積澱的基礎上，形成更宏觀和抽象的理解，在深層次上掌握普遍規律，從而將之前學到的繁雜的知識用一根線串起來，在具體知識之外找到新的答案，將有形化為無形，又將無形用於有形。\n  這個部分是30歲要更加要求自己的. 建立自己的知識體系已經簡單, 但如何「刻意驗證」來不斷升級自己知識體系的能力, 就要靠持續做各種不同的應用來累積與這個事件具體的經驗。做項目可以幫助我們累積這方面的能力！終身學習, 持續精進, 做時間的朋友！\n  知識管理三學派: 技術, 行為, 綜合  技術學派-知識本身: 技術學派認為知識管理就是對信息的管理，強調運用信息技術手段管理顯性知識。他們強調運用電子郵件、群件及其他工具從人、知識庫以及計算機網絡獲取「顯性知識」。許多學者，包括計算機專業人員，目前仍然在深化這方面的研究，諸如數據挖掘技術、人工智能技術、知識存儲與更新技術等。在知識管理未來的發展中，信息技術將會提供更多、更強有力的支持。在知識管理的實踐中人們已逐步達成如下共識：信息技術雖然重要，但只是知識管理成功實施的必要而非充分條件。\n  行為學派-知識擁有人: 該學派認為知識管理是對擁有知識的人（即知識工作者）的管理，他們重視對表現為人力資本和結構資本的「智力（知識）資本」的管理。比較關注知識管理與企業戰略、企業競爭優勢關係的研究，還關注組織間知識管理的研究。\n  綜合學派-人與知識並重: 該學派認為知識管理不僅要對信息和人進行管理，還要將信息和人連接起來進行管理；知識管理就是要將信息處理能力和人的創新能力相互結合，增強組織對環境的適應能力。該學派融合了信息技術及經濟學、管理學的相關知識，推動了技術學派和行為學派的相互交流、學習與融合。由於綜合學派能用較為系統、全面的觀點看待和實施知識管理，所以能很快被實踐者所接受。\n  知識管理方法: 產生, 分享, 管理  資訊化風潮: 受到 1990 年代的資訊化（Informatization），知識管理的觀念出現，成為企業或組織累積知識財富、創造更多競爭力的人文與技術具備的系統。如何深化「知識產生」的內涵、針對不同族群去做「知識分享」的設計，是「知識管理」當前與未來更重要的目標。\n  五步驟-產生與分享: 將知識透過「獲得、記錄、組織、存取與更新」，不斷「去由外而內的累積」以及「由內而外的優化」，助於企業及個人做出決策，因應環境的變遷。在這五個步驟的循環中，前面三步驟為「知識的產生」－強調如何使隱性的知識（最後一段會說明）方法化（methodology），後面兩步驟為「知識的分享」－強調讓受眾（員工、網路社群、大眾等等）容易瞭解、容易接受，並且容易感受到「成果」。如何深化「知識產生」的內涵、針對不同族群去做「知識分享」的設計，這兩大部分是「知識管理」當前與未來更重要的目標。\n   獲得\u0026ndash;\u0026gt;紀錄\u0026ndash;\u0026gt;組織\u0026ndash;\u0026gt;存取\u0026ndash;\u0026gt;更新\n 方法（methodology): 一種帶有約束性甚至強制性的模型規範，它會明確地告訴人們應該做什麼，不應該做什麼，什麼先做，什麼後做，怎樣才能事半功倍，取得最大的效益等。因此，方法往往是以規範、章程，條例，使得接受者能有共同的畫面去瞭解這個概念。  知識管理心法: 多元資訊渠道, 秉持心中之軸, 實踐輸出效應 多元資訊渠道: 積累關注不同信息渠道，去試一切錯。增加信息入口，是使人突破成長的最好辦法。常見的信息入口比如，閱讀經典書籍，互聯網信息資源等等，更難獲得的信息入口其實在人群中，也就是人際關係和貴人指路。 ​​​​不用擔心錯過什麼，有一天你會明白，關閉可能性 與 開拓可能性，同樣重要。   關閉可能性 與 開拓可能性，同樣重要。\n  秉持心中之軸: 你的個人憲法就是你一直秉持的信念、價值觀、思維方式，要經過深思熟慮，幾次修改，才能定案。從你最深刻的經歷、最難忘的故事中，回想你最刻骨銘心的事情，那其中包含你的人生哲學。原則是恆久不變的，你對原則的理解是會因為經歷而改變的，所以原則是歷久彌新的，是可以信賴的。\n  實踐輸出效應:去新的方法、思維，首先通過實踐輸出。比如說手帳方法具體體現了生活方式和工作方法，記錄反思在手帳上。第二層輸出是我實踐之後的復盤反思，總結成文章，寫成公眾號文章。我的公眾號是以個人成長、生活方式為核心。公眾號文章是電子生活+工作手帳。第三層輸出是跟進自己的行動、以及觀察生活中其他人的行為，調動我知識體系中的觀點、思維方式，去再一次更新我的認知框架和輸出效應。\n   工作方法\u0026ndash;\u0026gt; 復盤反思\u0026ndash;\u0026gt; 更新認知框架與輸出效應.\n 後記  到此我們看過了知識管理的五個面向; 對於個人或者組織, 都是需要想「數據, 資訊, 底層規律」三者是如何結合進工作流. 好的工具, 好的團隊, 促進知識的產生, 管理與分享. 長期而言, 知識是在實踐的過程中, 將自己的知識體系與外部的世界做主動驗證, 藉由試錯來高速迭代.\n  自我反思, 現在已經能藉由主動輸出, 來進行高強度的知識應用. 讓平時的瀏覽成為積累素材, 讓平時的休憩成為編輯文章的好時機. 材料累積久, 產生湧現效應, 便能有意想不到的體悟. 另外, 獲得\u0026ndash;\u0026gt;紀錄\u0026ndash;\u0026gt;組織\u0026ndash;\u0026gt;存取\u0026ndash;\u0026gt;更新這個知識管理的五步驟, 也不謀而合與每日文章輸出相同. 持之以恆, 共勉之！\n  2022.01.02. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-02","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur004%E7%9F%A5%E8%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E4%BA%94%E5%80%8B%E8%A7%92%E5%BA%A6/","series":["每日文章"],"tags":[],"title":"MUR004 知識管理的五個角度"},{"categories":["可信任的AI"],"content":"拉普拉斯差分隱私機制的四個面向  紫式晦澀每日一篇文章第3天\n  前言   今天是2022年的第一天! 今天思考了許多關於差分隱私相關的問題. 今天針對最經典的差分隱私機制,「拉普拉斯差分隱私機制」, 來做文章思考.\n  今天的素材是Cynthia Dwork與Aaron Roth的The Algorithmic Foundations of Differential Privacy 的章節3.3, 以及維基百科關於Additive noise mechanism 的條目.\n  場景: 數據庫查詢  數據庫查詢: 問題起源於「數據庫查詢(Database queries)」: 當我們查詢數據庫, 獲得數值資料時, 算法會將數據庫映射為k個實數回傳. 這個映射寫作$$f:\\text{數據庫}\\mapsto \\text{k個實數}.$$\n  敏感性: 我們能多準確地完成這樣的查詢呢? 利用「敏感性」來妙術最壞情況下, 一個人的數據可以改變「查詢映射f」的程度。   不確定性之必要: 萬一上述的敏感性很劇烈, 那麼惡意攻擊者就可以透過比較查詢結果, 來回推特定用戶的特定數據. 為了避免特定用戶的特定數據被還原出來, 增加不確定性是必要的.\n  直觀: 一個「查詢映射f」的靈敏度, 給出了我們必須對其輸出進行多大的擾動, 才能保護隱私, 的上限。 (The sensitivity of a function gives an upper bound on how much we must perturb its output to preserve privacy. )\n  方案: 增加拉普拉斯分佈為不確定性  拉普拉斯分佈: 拉普拉斯分布是指數分布的一個對稱版本。   拉普拉斯機制: 拉普拉斯機制將簡單地計算「查詢映射f」，並用來自拉普拉斯分布的噪聲來擾動每個坐標。 微調噪聲的規模將被校准為「查詢映射f的敏感性」（除以ε）。   拉普拉斯機制的隱私性質: 拉普拉斯機制保留了(ε,0)微分隱私。 處理前與處理後, 兩個查詢結果出現的機率, 不會差太多.\n  實例: 具體數據庫查詢任務  與機器學習的關係？\n  計算查詢: (Counting queries) 在數據庫中，有多少元素滿足特定\n  直方圖查詢: (Histogram queries) 將資料可能分佈的空間分割為單元格，查詢每個單元格中有多少數據庫元素\n  姓氏查詢: 從10,000個潛在名字的列表中計算出哪些名字在2010年人口普查的參與者中最常見. 是一種直方圖查詢.\n  差分隱私選取: (Differential private selection) 結果的空間是離散的，任務是產生一個 \u0026ldquo;最佳 \u0026ldquo;答案，在這種情況下，就是人口最多的直方圖單元。\n  最常見的病症: 在一組受訪者的醫療史中，哪種病症（大約）是最常見的，所以這組問題是，對於所考慮的每種病症，個人是否曾經接受過這種病症的診斷。\n  推廣: 加性噪音機制  推廣定義: 從預先確定的分布中, 添加受控的噪聲, 是設定「差分隱私機制」的一種方式。這種技術對於設計敏感數據上的實值函數的隱私機制很有用。常用於添加噪聲的分布包括拉普拉斯和高斯分布。\n  推廣敏感性: 令$\\mathcal{D}$為資料集; $f:\\mathcal{D} \\mapsto \\mathbb{R}$ 為查詢映射。則查詢映射的敏感性$\\Delta f$定義為 $$ \\Delta f = \\max |f(x)-f(y)|. $$ 這裡的最大值是在$\\mathcal{D}$中, 只差一個元素的一對資料集$x$和$y$。對於維度高的查詢映射函數, 通常在$\\ell_{1}$或$\\ell_{2}$下測量敏感性。\n  論證差分隱私性: 要論證該機制滿足$\\epsilon$差分隱私，需證明「輸出分布」在乘法意義上是封閉的。技巧是利用分佈本身的似然比.\n  後記  到此我們走完了一趟拉普拉斯機制相關的的基礎細節. 基本上, 問題的場景是對數據庫的各種查詢. 如果給準確的數值, 那惡意攻擊者有可能透過查詢結果來回推特定個體的某些數值. 因此, 根據查詢的特性本身針對資料變動的「敏感度」, 設計回傳的數值額外加上的不確定性, 就可以防止這類的惡意攻擊. 其中要量化隱私保護的程度, 就會用到拉普拉斯分佈的各種性質; 也不難想像「充分統計量」對應著「查詢映射」, 而「隱私保護查詢結果」對應著「不確定性部分的集中不等式」.\n  十分有趣, 而這類的工作在2021已經被延伸到各種機器學習模型的訓練上, 造成很多機器學習任務表現被影響甚巨. 因此, 藉由做「模型審計」相關的研究, 來評估「基於隱私保護資料所訓練的機器學習模型」, 是一個基礎必須做的工作. 合成數據, 在這個分野上, 是有什麼樣的直覺可以解決這個問題呢?\n  2022.01.01. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2022-01-01","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur003%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B7%AE%E5%88%86%E9%9A%B1%E7%A7%81%E6%A9%9F%E5%88%B6%E7%9A%84%E5%9B%9B%E5%80%8B%E9%9D%A2%E5%90%91/","series":["每日文章"],"tags":["差分隱私"],"title":"MUR003 拉普拉斯差分隱私機制的四個面向"},{"categories":["日本"],"content":"關於大晦日的五個知識點  紫式晦澀每日一篇文章第2天\n    今天是2021年的最後一天! 今天在Hololive六期生的出道後看了許多的vtuber. 其中日本文化裡一年的最後一天為「大晦日」. 一開始以為是悔改的意思, 但搜集資料後發現故事還不少, 很有意思.\n  今天的素材是在google上面搜尋「大晦日」, 將閱讀文章摘要三點累積而成. 共讀了5個文章, 總結出十五條細節. 下面總結出關於大晦日的五個知識點.\n  年末說法: 大晦日(おおみそか)，指的是每年新曆的12月31日。另外, 表達最後一天的日語還有「年末」和「年の暮れ(としのくれ)」。其中「末(matsu)」和「暮れ(kure)」表示「～的最後」的意思，所以兩者都表示「年末」、「1年的結束」。   神社文化: 神社在大晦日將舉行「大祓い」儀式，以淨化積累了一年的穢物。佛教則為了清除迷惑身心的108個煩惱，將敲響除夜（除夕夜）之鐘。人類有108個煩惱，每敲一次鐘就能消除一個煩惱，所以才敲響108次鐘。   晦日緣由: 在舊曆的晦日代表「每月的最後一天」，而一年的最後一天，也就是一年最後一個月的最後一天稱之為「大晦日」。「晦日」可追溯回中國，《公羊傳》中就有出現過「何以不日？晦日也。」的記載. 其中晦日指的是農曆中每月最後一日，當天幾乎看不到月亮. 晉代杜預對《左傳》的注釋就提到，「晦，月終，陰之盡。」在日本，晦字也有「月隠り（つきごもり）」之意，而「つごもり」正是日文「晦日」的其中一個讀法。 日文「晦日」的另一個讀法「みそか」，就是古文中「30日」的讀法〈如20日的讀音是はつか〉，而由於農曆月終不是29就是30，所以兩者稱呼都是「みそか」。  年神文化: 12月31日是日本人為了在「正月」迎接神靈, 而做各種準備的日子。在「正月」，日本人要迎接一年到頭守護家庭的神靈－「年神様」（年神，歲神），因此正月也是歡迎神靈之日。其中, 會於家門使用神靈休憩的靈木－「松」製作的松飾「門松」，立在家門左右。門松，還擔當著神靈的引路標誌。另外, 在玄關的門上面。還會裝飾「注連繩」，以迎接神靈。在神棚（かみだな：室內供奉神靈的架子），要上供「鏡餅」。   初夢文化: 初夢（はつゆめ）是「新年最初所作的夢」. 一般指在1月1日或1月2日夜晚所作的夢. 傳統日本人認為初夢可以用來占卜新的一年運勢. 特別的, 如果夢到了1.富士山（ふじさん）2.鷹（たか）3.茄子（なす）, 是非常吉利的象徵. ① 這三樣都是德川家康喜歡的東西 ② 富士山是日本第一的山、老鷹是最威猛的鳥類、茄子則是重貴的食物（容易腐壞不易保存 ) ③ 富士山是「無事（ぶじ）」的諧音、老鷹是「高（たかい）」的諧音、茄子是「事事順遂（事をなす）」的諧音.  今天是2021年的最後一天. 回顧過去一年成就許多, 也意識到自己更多的不足. 持續思考, 精進自己, 用輸出指導輸入, 用文章精煉思考. 新的一年, 持續學習語言, 程式, 做更落地的研究, 思考商業, 組織執行項目, 承擔更大的責任, 做一個大寫的人.\n  2021.12.31. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2021-12-31","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur002%E9%97%9C%E6%96%BC%E5%A4%A7%E6%99%A6%E6%97%A5%E7%9A%84%E4%BA%94%E5%80%8B%E7%9F%A5%E8%AD%98%E9%BB%9E/","series":["每日文章"],"tags":[],"title":"MUR002 關於大晦日的五個知識點"},{"categories":["寫作"],"content":"每日寫文章的五個心法  紫式晦澀每日一篇文章第1天\n    新年新希望! 每天寫文章來增強自己的表達能力. 現在使用Obsidian與ipad可以很輕鬆收集素材; 接著使用github page來發表每日的文章.\n  今天素材收集是在google上面搜尋「每日寫文章」以後, 照讀到的文章總結出三個面相. 共讀了5個文章, 總結出十五條細節. 這個流程蠻不錯, 之後可以參考, 做淺淺的研究. (跟仔細讀paper的方法不同)\n  固定習慣: 目前的設定是每天晚上十點, 在床邊的書桌寫作. 在十一點左右將文章Po到網站上. 有想過可以將素材收集到臉書的粉絲專頁, 用mac系統的quick note等等. 晚上整理, 輸出, 成為以後寫書的材料.\n  格式魔法: 在開頭寫「紫式晦澀每日一篇文章第X天」, 蒐集要寫的素材所屬於的領域, 以及維持書寫輸出的自律習慣. 這個點很不錯, 讓努力能夠累積, 也有足夠的素材能夠當作更高級的文件的底料.\n  刻意輸入: 不要完美主義, 先完成30分的作品. 之後根據需求, 再把文件完善, 讓分數提高. 在消費媒體時, 也持續打字寫下想法, 這樣就有底稿, 可以執行模塊輸出. 利用「溝通黃金圈」或者「清單體」等等的寫作架構, 就可以文思泉湧, 讓思想具現化.\n  碎片分享: 文件都是改出來的. 當我們消費各種知識, 寫作能夠提升我們總結的能力. 這個過程稱為「知識生產」，而生產出來的產物則是「知識卡片」等等碎片化的知識(知識=資訊+脈絡+流程). 利用總結技巧, 模板化輸出, 是什麼為什麼怎麼做. 將成果天天分享於社群帳號; 累積夠多素材後還能整理成大的文章, 標準化寫作. 用粉絲專業寫平時靈感. 用專業帳號發個人品牌文章.\n  定性心流: 把寫文章步驟化, 例如: 關鍵字，架構，內文，插圖，制式內容，校正，內容說明，摘要，發文. 共九個步驟. 寫的時候不要回頭檢查文章, 只在校正的步驟做檢查重排; 當真的寫不出來, 就抽離, 做其他事情, 一但有了靈感就再繼續, 不要免強.\n  即將邁入三十歲, 感覺要從知識的消費者轉為生產者! 分享其實也是在為未來的自己做知識服務. 寫下來的東西對自己最有用. 文章的再修改也能讓未來的自己看見知識如何逐漸形成. 共勉之!\n  2021.12.30. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"2021-12-30","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur001%E6%AF%8F%E6%97%A5%E5%AF%AB%E6%96%87%E7%AB%A0%E7%9A%84%E4%BA%94%E5%80%8B%E5%BF%83%E6%B3%95/","series":["每日文章"],"tags":[],"title":"MUR001 每日寫文章的五個心法"},{"categories":["寫作"],"content":"","date":"2021-12-28","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/rdd001-%E7%A0%94%E7%A9%B6%E8%A8%AD%E8%A8%88%E7%B3%BB%E5%88%97%E5%B0%8E%E6%96%87/","series":["研究設計"],"tags":[],"title":"RDD001 研究設計系列導文"},{"categories":["寫作"],"content":"211223 思考豆腐塊文章的執行面  目的: 藉由寫豆腐塊文章來強化輸出. 讀者: 放在自己的blog上面. 原則: 一篇文章只講一個觀點. 這個觀點, 快的話應該是要從讀的東西輸出 執行: 理想上, 在製作[[AGM331 知識卡片 (Knowledge Card)]]的同時, 就可以加工成[[豆腐塊作文]], 發到網站上, 整理思想.\n目的: 藉由寫豆腐塊文章來強化輸出. 讀者: 放在自己的blog上面. 原則: 一篇文章只講一個觀點. 這個觀點, 快的話應該是要從讀的東西輸出 執行: 理想上, 在製作[[AGM331 知識卡片 (Knowledge Card)]]的同時, 就可以加工成[[豆腐塊作文]], 發到網站上, 整理思想.\n把部落格當作平常練手的豆腐塊區域 練技術, 要實際寫程式練手 練寫作, 用各種豆腐塊練手\n豆腐塊: 簡述一篇學術文章的每個第一段; 給個Profile 這個Profile應該是各種觀點, 與各種術語.\n 可以練手, 用Fintech 與 Synthetic Data的交集來看.\n  Example : Fintech 與 Synthetic Data的交集.  MDPI   關於「備忘」與「筆記」 我現在意識到, 要有意地去改變寫「備忘」與「筆記」的比例; 「備忘」是讀了一些東西的觀後感, 小總結; 「筆記」則是什麼都不看, 環繞自己某個問題寫. 前者要重視轉化「外在文本」為「自身知識儲備」, 後者要重視轉化「自身知識儲備」為「具體問題的解決過程」. 所以在網站上, 只能放「筆記」 在個人知識庫裡, 主要放「備忘」\n","date":"2021-12-23","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/art478-%E8%B1%86%E8%85%90%E5%A1%8A%E6%96%87%E7%AB%A0%E6%80%9D%E8%80%83/","series":["豆腐塊"],"tags":[],"title":"ART478 思考豆腐塊文章的執行面"},{"categories":[],"content":"這是第一篇文章\n思考一: 實踐寫作套路, 學習寫作工作總結   想要研究寫作的套路，早上看了一些關於文案寫作的書。但他們的性質跟我們平常知識生產的模式不太一樣。 在網路上寫部落格的原因是什麼? 我想是經營個人品牌。因為我自己都還沒有個人網站。 那為什麼要建立個人品牌呢因為在現在這個時代我們找工作的履歷不再只是一張紙, 而是一個網站可以表現我們過去幾年的工作。 這個有點像在寫工作總結，那怎麼寫好工作總結呢？我覺得這就是可以看得到App上面的課程。  流程圖原始碼  1graph LR 2\tT(實踐寫作套路); A(發表網路部落格); R(建立個人品牌); Act(強化工作總結技能) 3\tT--\u0026gt;A--\u0026gt;R--\u0026gt;Act 思考二: 邏輯流 邏輯流治天下\n","date":"2021-12-21","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/art001-guidance/","series":[],"tags":["邏輯流"],"title":"ART001 文章概覽"},{"categories":null,"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.com/yuin/goldmark  https://github.com/alecthomas/chroma  https://github.com/muesli/smartcrop  https://github.com/spf13/cobra  https://github.com/spf13/viper   Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub .\n","date":"2019-02-28","img":"","permalink":"https://laplus3667.github.io/zh-tw/about/","series":null,"tags":null,"title":"About"},{"categories":["機器學習"],"content":"學習理論工具箱  紫式晦澀每日一篇文章第37天\n 前言   今天是2022年第35天, 全年第5週, 二月的第1個週五. 今天對「機器學習理論(Machine Learning Theory))」做學習思考.\n  今天的素材主要來自文章:\n   2021: Learning Theory from First Principles 的第八單元  簡介: 限制最小平方 基本的方法在高維度讓學習變困難, 試著用正規化克服高維度學習的複雜度 為何要考慮高維度?:\n 目標Lipschitz-continuous: 「過風險(Excess Risk)」隨著維度增加, 學習變困難 目標linear: 過風險也隨維度增加, 學習變困難.    解法: 使用「正規化(Regularization)」:\n Ridge: 得到「維度獨立」的bound, 最優已無法改良 $l_0$: 選變數, 找到預測子, 只與小數量的變數有關.   選變數(Variable Selection):\n 原特徵集合很大 高維度可能可以幫助預測, 但僅預期小數目是相關的. 如果實際上不是小數目相關, 那這些方法不會比較好.   兩種選變數技巧:$l_0$與$l_1$ 懲罰項:\n 最小化損失函數, 可以是廣義的loss. (需要到什麼條件?🤔)    最小平方法:\n 平方損失, 變異固定噪音. 目標: 讓「自標準化平方損失」愈小愈好. OLS的過風險$\\sigma^2 d / n$ (最好的結果, 在對真實參數沒有結構假設的狀況). 結構資訊$|\\theta_{*}|_0=k$: 僅有小部分的變數與實際訊號有關.     ****: ****: ****:\n ****: ****: ****:\n變數選擇由l0懲罰  ****: ****: ****:\n ****: ****: ****:\n高維度估計由l1正規  ****: ****: ****:\n ****: ****: ****:\n後記 大概花了90分鐘, 把合成數據方法章節寫過了一遍. 仔細比較一下, 這個章節寫得蠻好的! 從最「心智」的模型到最「計算」的模型, 展現人與機器的結合程度由淺至深.\n   方法編號 研究範式 實踐技巧     方法一 數理機率範式 多變量高斯, 共變異,耦合   方法二 應用機率範式 多變量高斯, 共變耦合, 決策樹   方法三 統計科學範式 混血合成=真實資訊+假說資訊   方法四 機器學習範式 決策樹, 序貫合成   方法五 深度學習範式 變分自動編碼器, 對抗生成網路   方法六 強化學習範式 服務是事件的序列, 合成轉移矩陣    過去的經驗都還能支撐這六種做法, 非常棒! 我想所謂的工程師, 就是要能很快速實踐各種研究範式裡面的方法, 來認識複雜的世界. 非常有趣！期待之後經驗累積! 天天向上, 共勉之!\n2022.02.03. 紫蕊 於 西拉法葉, 印第安納, 美國.\n","date":"1995-02-04","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/mur-%E5%AD%B8%E7%BF%92%E7%90%86%E8%AB%96%E5%B7%A5%E5%85%B7%E7%AE%B1/","series":["每日文章"],"tags":[],"title":"MUR037 學習理論工具箱"},{"categories":[],"content":"Question Q. 樹籬算法(Hedge algorithm)的證明技巧?  Q. 樹籬算法(Hedge algorithm)的證明技巧?\n Evidence: 2021:Relaxing the I.I.D. Assumption: Adaptively Minimax Optimal Regret via Root-Entropic Regularization  2021:Relaxing the I.I.D. Assumption: Adaptively Minimax Optimal Regret via Root-Entropic Regularization   測度集中: 凸包, 證明D.Hedge, FTRL-CARE, Meta-Care. 測度集中(Concentration of Measure):\n 非i.i.d.的「測度集中(Concentration of Measure) 可用來upper bounds本文章的三種算法: D.Hedge, FTRL-CARE, Meta-Care. 要求「凸限制級(Convex constraint)」; 非凸的就使用到「凸包(Convex hull)」. 凸集: 原始可用分佈的混合(Mixtures). 專家與環境都可以任選分佈來產生資料.    凸限制五實例: 隨機帶檻(Stochastic-with-a-gap):\n 只有一個真值$\\mu_{0}$ 存在一個最好的專家, 其與第二好的專家差$\\Delta$.   對抗(Adversarial):\n 限制: 所有的機率測度 \u0026mdash;\u0026gt; 對抗設定   對抗帶檻(Adversarial-with-a-gap):\n 混合最好的專家, 滿足gap條件    與Lasso bandit很多很像.\n 對抗帶期望檻(Adversarial-with-an-E-gap):\n 期望版本   球環IID(Ball-around-I.I.D.):\n 靠近的機率測度.   測度集中定理 定理一:\n 每一輪的「最好專家」可能不同, 可以接著用Azuma-Hoeffding inequality. 就算每一輪最好專家不同, 「最有效專家」與「任何無效專家」的gap至少上漲得如「均勻次高斯隨機變數」, 其均值為$-\\Delta_0$. 證明依賴於von Neumann\u0026rsquo;s minimax theorem.    整個很抽象！需要補很多脈絡！\n 適應性 = (有效專家個數, 有效隨機檻) 時間一致凸限制 (Time-homogeneous convex constraints):\n N個專家 損失函數: $l:\\hat{Y} \\times Y \\mapsto [0,1]$. 機率測度: $M(\\hat{Y} \\times Y )$ 資料產生機制: $\\pi$. 專家預測的條件分佈, 是time-homogeneously constrined. 有效專家(Effective Experts): $I_0(D)$收集可能在某些round成為最好專家的那些專家們. $N_0(D)$: 有效專家的個數 有效隨機檻(Effective Stochastic Gap)$\\Delta_0$: 「無效專家(ineffective expert)」相比較於「有效專家(Effective expert)」的最小的額外損失 利用「(有效專家個數, 有效隨機檻)」來描述問題的「適應性(Adaptivity)」 接著收集相對應的資料生成機制.    怎麼感覺Lasso bandit也與expert很像, 可以分有效專家, 無效專家.\n Answer A. 隨即群體公平的落差分析  🤔 更中間的adaptivity是否能達到group fairness? 或者, 達到adaptively minimax optimal的預測子, 其group fairness完成的如何?\n 這次的50分鐘體會了, 這真的是一篇很技術性的paper! 竟然將適應性討論到了有幾個「有效專家」的地步. 感覺這是要先很了解「樹籬算法」的人, 才能appreciate他的工作. 可以下週再來挑戰.\n","date":"1990-01-05","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/gfh005-hedge%E9%81%A9%E6%87%89%E6%80%A7%E5%88%9D%E6%8E%A2/","series":["樹籬公平性算法"],"tags":[],"title":"GFH005 Hedge適應性初探"},{"categories":[],"content":"Question Q. 樹籬算法(Hedge algorithm)的公平性基礎?  Q. 樹籬算法(Hedge algorithm)的公平性基礎?\n Evidence: 2021:Best-Case Lower Bounds in Online Learning  2021:Best-Case Lower Bounds in Online Learning   此作能達到anytime group fairness DTOL(Decision-Theoretic Online Learning)裡面的group fairness: 每一組的平均損失一樣:\n 每一輪, Learner會面對一個group $g_{t}$, 接著要play action $w_{t}$ 每個專家, 會在群體層面平衡失誤. 專家j, 在特定組別的時間, 犯的錯加起來, 成為$L_{T(g),j}$. 任兩組的「平均損失」是均衡的, 成為「孤獨公平(Fair in isolation)」.  達到公平的做法:Interleaved Hedge:\n 常數學習率, 已知時間界線 在每一群體自己跑「樹籬算法(Hedge)」\u0026ndash;\u0026gt; 每個子序列自己跑. Interleaved Hedge的後悔為$O(\\sqrt{T})$.    Interleaved Hedge的群體公平保證:\n 另外有「群體公平保證」, 任兩組的公平差異大概是$O(\\sqrt{\\frac{\\log d}{T_0}})$ 這個文章可以給更好的regret bound, 不只是rate, 而是真實的upper bound. 原作法需要知道「每個子序列的長度」, 但這個文章用anytime best-case lower bound來做, 可以給很大的改善. 他們的貢獻可以得到「anytime群體公平」. 很酷👍. 此文章講了一些adaptivity下, 可以重新檢視group-fair adversarial online learning.    我們可以想: adversarial online learning的group fairness可能達不太到; 此文章的adaptivity的狀況可以達到group fairness; 那麼更中間的adaptivity是否能達到group fairness?\n 隨時公平, 計算公平性落差的框架 公平性落差(Fairness Gap)的證明策略:\n 改證明的策略: 每一群體, 找出損失最少的那個專家 每群體自己跑樹籬算法, 找出損失最少那個群體 用此文章的bound, 可以算fairness optimality gap. 用Decreasing Hedge, 可以證明另外一個bound. 藉此達到fairness gap的證明   隨時公平(Anytime Fairness):\n 固定horizon看fairness不是太有用 定義anytime fairness的概念 會讓這個定義只是「大概approximately」滿足 藉此證明一個anytime fairness的結果.    在應用上, 怎樣的是group呢? 可否用Multi-agent的框架來? distributed learning? 如何描述?\n 四種Adaptivity: 決策邊界免疫, 最優專家後悔, 資料幾何適應, 最好專家序列(負後悔)    Anytime 算法: 不用知道決策邊界    Timeless算法: 後悔決定於最優的專家    AdaGrad: 適應資料的幾何 (Duchi et al., 2011)    Shifting regret, tracking regret: 與最好專家序列對戰(產生負後悔) (Herbster and Warmuth, 1998)    Answer A. 隨即群體公平的落差分析  🤔 更中間的adaptivity是否能達到group fairness?\n 這次的50分鐘體會了, 原來適應性有四種, 還可以得到「負的regret」！這樣玩一玩感覺都有新框架. 而這個文章將這個第四框架設定為best-case lower bounds, 在這裡面研究regret的表現. 不知道這種算法, 跟bandit搭配起來是如何, 感覺是理論上很有趣的問題. 裡面還有提到湯姆森取樣在adversarial bit prediction的比較, 好好玩!\n","date":"1990-01-04","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/gfh004-%E9%9A%A8%E6%99%82%E7%BE%A4%E9%AB%94%E5%85%AC%E5%B9%B3%E7%9A%84%E8%90%BD%E5%B7%AE%E5%88%86%E6%9E%90/","series":["樹籬公平性算法"],"tags":[],"title":"GFH004 隨時群體公平的落差分析"},{"categories":[],"content":"Question Q. 樹籬算法(Hedge algorithm)的公平性基礎?  Q. 樹籬算法(Hedge algorithm)的公平性基礎?\n Formulation 問題設定: 「行動$w_t$」是我們的「估計$\\beta_t$」; 「損失$\\langle l_{t}, w_{t}\\rangle$」是我們的「期望平均 $\\langle x_{t}, \\beta_{t}\\rangle$. 那他們對於「loss vector distribution」的要求是? 問題設定:\n 學習者選「行動$w_{t}$」是一個d維度向量.(不像bandit是選一個行動) 學習者觀察到損失: $f_{t}(w_{t})$. 具體的損失-線性損失:$f_{t}(w_{t})=\\langle l_{t}, w_{t}\\rangle$. (轉成reward就可以符合我們要的設定, 只是會變成contextual bandit?) 專家j的累積損失: $L_{t,j}=\\sum_{s=1}^{t}l_{s,j}$ 學習者的累積損失:     解法:FTRL 跟著正規領導者: 「data-dependent sequence of regularizer」平行「online regularization scheme」; 他們的目標是regret好; 我們的目標是控制收斂速度, 以「統計偏差(Statistical error)」之名. 解法:FTRL 跟著正規領導者\n data-dependent sequence of regularizer (與我們的online regularization也很類似) 這樣找到的「action」就像是在$l_1$ regularizer下找到的Lasso. (這樣串起來, 好像其實我們之前做的是一種Follow the regularizer leader. 但我們的context是什麼? 好像還是不一樣, 我們那邊是mean reward, 然後最優的那個的mean reward也是一個算法; 所以我們的$\\beta_{t}$是這邊的$w_{t}$) (所以, 只要regret返回, 變成$|w-w_{t}|$函數, 然後我們會算$w_{t}$的收斂速度, 就可以得到regret的收斂速度.) (與optimization比較, 他們這裡關心的是regret; 我們統計關心的是$w_{t}$是否也有收斂到true parameter $w$, 因為true parameter代表了population, 也就是我們想要驗的data generative model.) 這樣感覺整個比較懂了！很酷, 這個真的蠻神奇的\u0026hellip;🥳   Evidence: 2021:Best-Case Lower Bounds in Online Learning  2021:Best-Case Lower Bounds in Online Learning   Any-time Hedge: 有好的worst-case regret Any-time Hedge有好的worst-case regret.:\n 調整learning rate的Hedge算法, 有好的worst-case regret.   For example, in the setting of decision-theoretic online learning (DTOL) with d experts (Freund and Schapire, 1997), the well-known anytime version of Hedge (which uses the time-varying learning rate $\\eta_{t} \\asymp \\sqrt{\\log (d) / t}$ ) enjoys $O(\\sqrt{T \\log d})$ worst-case regret and, as we show, has $-O(\\sqrt{T \\log d})$ best-case regret.\n 常數學習率Hedge的缺點: 非anytime的群體公平 常數學習率Hedge的缺點: 非anytime的群體公平:\n constant learning Hedge: 其best-case後悔是$-O(\\sqrt{T})$. 有限多個滿足「群體公平」的專家, 在巧妙使用Hedge的狀況下也能滿足同樣的「群體公平」, 且可以得到$O(\\sqrt{T})$後悔. 缺點: 不是anytime算法. 導致最後的群體公平, 看起來會很不公平. The fixed time horizon assumption also implies that their notion of group fairness also is inherently tied to a fixed time horizon (see Section 4 for a detailed discussion), and this latter implication can lead to experts that seem very unfair but which, based on a fixed horizon view of group fairness, are technically considered to be fair. (👻 可以大寫特寫的點, 想想看怎麼變成work.) 本文章的方法給出了一個「anytime version of group fairness」, 感覺這才是我們要的！   A key motivation for our work is a recent result of Blum et al. (2018) which shows, in the setting of DTOL, that Hedge with constant learning rate has best-case regret lower bounded by $-O(\\sqrt{T})$. This result, taken together with worst-case upper bounds of order $O(\\sqrt{T})$, is then used to show that if each of finitely many experts approximately satisfies a certain notion of group fairness, then a clever use of the Hedge algorithm (running it separately on each group) also approximately satisfies the same notion of group fairness while still enjoying $O(\\sqrt{T})$ regret. However, we stress that their result is very limited in that it applies only to Hedge when run with a known time-horizon. The fixed time horizon assumption also implies that their notion of group fairness also is inherently tied to a fixed time horizon (see Section 4 for a detailed discussion), and this latter implication can lead to experts that seem very unfair but which, based on a fixed horizon view of group fairness, are technically considered to be fair. Our best-case lower bounds enable the results of Blum et al. (2018) to hold in much greater generality; in particular, our results enable the use of an anytime version of group fairness, which we feel is trulv needed.\n Decreasing Hedge (Mourtada and Gaïffas, 2019):有好的anytime-best-case lower bound與worst-case upper bound Decreasing Hedge (Mourtada and Gaïffas, 2019):有好的anytime-best-case lower bound與worst-case upper bound:\n 使用「負夏儂熵(Negative Shannon entropy)」來做regularizer; 跟著配合的learning rate. 使得最後做的「決策」是以 的模式採用專j的意見. (這裡感覺可以與j arm的故事產生關係.)   Answer A. 行動就是參數; 損失就是脈絡  我們可否建立minimax optimal相關的group fairness呢？\n 這次的50分鐘領悟了, 其實行動就是參數; 損失就是脈絡. 在online learning的各種技術,主要是為了回答regret層面的考量; 而Statistics對估計的考量會了以後, 又可以討論統計誤差, 再更具體執行統計的任務. 研究真有意思. Martin Wainright相關的作品一定就是走這條學術密碼吧！\n","date":"1990-01-03","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/gfh003-%E8%A1%8C%E5%8B%95%E5%B0%B1%E6%98%AF%E5%8F%83%E6%95%B8-%E6%90%8D%E5%A4%B1%E5%B0%B1%E6%98%AF%E8%84%88%E7%B5%A1/","series":["樹籬公平性算法"],"tags":[],"title":"GFH003 行動就是參數; 損失就是脈絡"},{"categories":[],"content":"Question Q. 樹籬算法(Hedge algorithm)的公平性基礎?  Q. 樹籬算法(Hedge algorithm)的公平性基礎?\n 群體脈絡線上學習\n a.group-outcome sequence tuple: 時間, 群脈絡, 結果 b. 先看到群脈絡, 算法給專家們機率, 來決定要聽哪個專家的. c. 算法會看到整個損失向量, 然後計算期望損失  Evidence: blum2018preserving  blum2018preserving   同等錯誤率-每個族群都應享有同樣的服務品質 服務品質均等:\n 同比例不可能: 有個\u0026quot;equalized odds\u0026quot;條件的strong impossibility results, 所以才搜尋替代的「公平性標準(Fairness notion)」. 公平性標準: 非歧視預測器, 達成有效性, 維持非歧視性. (given access to non-discriminatory predictors, achieves efficiency while preserving non-discrimination.) 替代概念: 「同等錯誤率(Equalized error rates)」\u0026ndash;每個族群的平均期望損失應該要是相同的 同等錯誤率在「效率與公平性以同樣目標函數測量」的狀況下, 有道理. 例子一: 醫療應用, 不同的子母群, 所承擔的醫療風險應該一樣, 才是公平. (🥲真的有道理嗎? 對特定的藥可能無法, 但對醫療服務的確有可能, 要因材施藥) 所有子母群都應享受同樣的服務品質(all subpopulations receive same quality of service.): 不應該用一個種族做實驗, 來增強另一種族得到的服務品質.   「同等錯誤率(Equalized error rates)」\u0026ndash;每個族群的平均期望損失應該要是相同的\n  所有子母群都應享受同樣的服務品質\nAll subpopulations receive same quality of service.\n  Our results for equalized error rates. The strong impossibility results with respect to equal- ized odds invite the natural question of whether there exists some alternative fairness notion that, given access to non-discriminatory predictors, achieves efficiency while preserving non-discrimination. We answer the above positively by suggesting the notion of equalized error rates, which requires that the average expected loss (regardless whether it stems from false positives or false negatives) encoun- tered by each group should be the same. This notion makes sense in settings where performance and fairness are measured with respect to the same objective. Consider a medical application where people from different subpopulations wish to receive appropriate treatment and any error in treat- ment costs equally both towards performance and towards fairness.1 It is morally objectionable to discriminate against one group, e.g. based on race, using it as experimentation to enhance the quality of service of the other, and it is reasonable to require that all subpopulations receive same quality of service.\n 實為非歧視的公平: 損失計算在各個族群裡都要一樣 實為非歧視的公平: 損失計算在各個族群裡都要一樣:\n 所有專家是「孤獨公平(Fair in Isolation)」 群體脈絡(Group contexts)從固定分佈隨機而來; 也可以從對抗而來. 分類問題: 特定族群裡面, 被標記為+的那些時間點 在特定子群裡面, 計算一個專家的平均損失表現. 不是算每個時間點, 是算特定的時間點.   非歧視理想: 算法的期望表現在各群體相同 非歧視理想: 算法的期望表現在各群體相同:\n 非歧視理想(Non-discrimination desiderata): 利用「孤獨公平預測子」來做算法, 得到的「期望表現」要在每個群體相同. 期望是對realization取的, 表現差異任兩群不要大過$\\alpha$. 非歧視理想會被「同時」用到多重度量上 例子一: 分類問題中, 「偽陽率(False Positive Rate)」與「偽陰率(False Negative Rate)」要在各子群中相當. 實務上會focus在一個metric上面.   「群體公平(Group fairness)」實踐\u0026ndash;「機會均等(Equality of Opportunity)」\u0026ndash;單一度量上的非歧視條件, 才有實現的可能. 「群體公平(Group fairness)」理想與現實\u0026ndash;「優勢均等(Equalized odds)」與「機會均等(Equality of Opportunity)」:\n 受歡迎的「群體公平(Group fairness)」\u0026ndash;「優勢均等(Equalized odds)」. 分類問題例子: 各個族群的偽陽率(False Positive Rate)」與「偽陰率(False Negative Rate)」在各族群要要相當 弱一點的「群體公平(Group fairness)」\u0026ndash;「機會均等(Equality of Opportunity)」: 非歧視條件僅在「偽陰率(False Negative Rate)」上滿足. 在i.i.d.的狀況下, 是可以達成 在non i.i.d. online learning會無法達到 有兩個impossibility result  Answer A. 實踐群體公平性以機會均等 這次的50分鐘讀到了, 理想上的群體公平, 應該是「優勢均等(Equalized Odds)」. 然而這篇文章證明優勢均等是不可能的. 因此, 需要放弱「群體公平」的條件, 成為「機會均等(Equality of Opportunity)」, 才能夠有理論結果.\n這個對應回Bandit裡面的Equal exposure的故事非常match, 也把expert算法個故事包裝了一番.\n","date":"1990-01-02","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/gfh002-%E5%AF%A6%E8%B8%90%E7%BE%A4%E9%AB%94%E5%85%AC%E5%B9%B3%E6%80%A7%E4%BB%A5%E6%A9%9F%E6%9C%83%E5%9D%87%E7%AD%89/","series":["樹籬公平性算法"],"tags":[],"title":"GFH002 實踐群體公平性以機會均等"},{"categories":[],"content":"Group Fairness + Hedge Group Fairness + Hedge:\n 2021:Best-Case Lower Bounds in Online Learning If each of finitely many experts approximately satisfies a certain notion of group fairness, then a clever use of the Hedge algorithm (running it separately on each group) also approximately satisfies the same notion of group fairness while still enjoying $O(\\sqrt{T})$ regret. 每個experts都滿足一點group fairness, 那用樹籬算法也可以滿足group fairness. blum2018preserving 講了很多Fairness的故事 bechavod2019equal 講了Equal Opportunity相關文獻. 與Hedge無關  1## Group Fairness + Hedge 2 3### 1. 4@article{guzman2021best, 5 title={Best-case lower bounds in online learning}, 6 author={Guzm{\\\u0026#39;a}n, Crist{\\\u0026#39;o}bal and Mehta, Nishant and Mortazavi, Ali}, 7 journal={Advances in Neural Information Processing Systems}, 8 volume={34}, 9 year={2021} 10} 11 12### 1.1 13@article{blum2018preserving, 14 title={On preserving non-discrimination when combining expert advice}, 15 author={Blum, Avrim and Gunasekar, Suriya and Lykouris, Thodoris and Srebro, Nathan}, 16 journal={arXiv preprint arXiv:1810.11829}, 17 year={2018} 18} 19 20### 1.1.1. 21@article{bechavod2019equal, 22 title={Equal opportunity in online classification with partial feedback}, 23 author={Bechavod, Yahav and Ligett, Katrina and Roth, Aaron and Waggoner, Bo and Wu, Zhiwei Steven}, 24 journal={arXiv preprint arXiv:1902.02242}, 25 year={2019} 26} 27 Fairness of Exposure in Stochastic Bandits  2021: Fairness of Exposure in Stochastic Bandits : 有Hedge法 2021: ONLINE FAIR REVENUE MAXIMIZING CAKE DIVISION WITH NON-CONTIGUOUS PIECES IN ADVERSARIAL BANDITS : 有Hedge法 2021: A Unified Approach to Fair Online Learning via Blackwell Approachability : Blackwell Approachability 給各種Fair online bandit.  1## Fair Exposure in Stochastic Bandits 2 3## 2. 4@article{wang2021fairness, 5 title={Fairness of Exposure in Stochastic Bandits}, 6 author={Wang, Lequn and Bai, Yiwei and Sun, Wen and Joachims, Thorsten}, 7 journal={arXiv preprint arXiv:2103.02735}, 8 year={2021} 9} 10 11## 2.1 12@article{ghodsi2021online, 13 title={Online Fair Revenue Maximizing Cake Division with Non-Contiguous Pieces in Adversarial Bandits}, 14 author={Ghodsi, Mohammad and Mirfakhar, Amirmahdi}, 15 journal={arXiv preprint arXiv:2111.14387}, 16 year={2021} 17} 18 19## 2.2 20@article{chzhen2021unified, 21 title={A Unified Approach to Fair Online Learning via Blackwell Approachability}, 22 author={Chzhen, Evgenii and Giraud, Christophe and Stoltz, Gilles}, 23 journal={Advances in Neural Information Processing Systems}, 24 volume={34}, 25 year={2021} 26} 27 樹籬算法  2022: Relaxing the I.I.D. Assumption: Adaptively Minimax Optimal Regret via Root-Entropic Regularization. (NIPS2022)  2019: On the optimality of the Hedge algorithm in the stochastic regime   鏡面下降  Lecture note 19: Mirror descent ; Course  2019: Connections Between Mirror Descent, Thompson Sampling and the Information Ratio   ","date":"1990-01-01","img":"","permalink":"https://laplus3667.github.io/zh-tw/posts/gfh001-%E6%A8%B9%E7%B1%AC%E5%85%AC%E5%B9%B3%E7%AE%97%E6%B3%95/","series":["樹籬公平性算法"],"tags":[],"title":"GFH001 樹籬公平算法"},{"categories":null,"content":"","date":"0001-01-01","img":"","permalink":"https://laplus3667.github.io/zh-tw/contact/","series":null,"tags":null,"title":"Contact Us"},{"categories":null,"content":"","date":"0001-01-01","img":"","permalink":"https://laplus3667.github.io/zh-tw/faq/","series":null,"tags":null,"title":"FAQs"},{"categories":null,"content":"","date":"0001-01-01","img":"","permalink":"https://laplus3667.github.io/zh-tw/offline/","series":null,"tags":null,"title":"Offline"}]